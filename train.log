Compiling user custom op, it will cost a few seconds.....
W0810 10:04:37.093199 45710 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.5, Driver API Version: 10.1, Runtime API Version: 10.1
W0810 10:04:37.096711 45710 gpu_resources.cc:91] device: 0, cuDNN Version: 7.6.
PAConv(
  (scorenet1): ScoreNet(
    (mlp_convs_hidden): LayerList(
      (0): Conv2D(6, 16, kernel_size=[1, 1], data_format=NCHW)
      (1): Conv2D(16, 8, kernel_size=[1, 1], data_format=NCHW)
    )
    (mlp_bns_hidden): LayerList(
      (0): BatchNorm2D(num_features=16, momentum=0.1, epsilon=1e-05)
      (1): BatchNorm2D(num_features=8, momentum=0.1, epsilon=1e-05)
    )
  )
  (scorenet2): ScoreNet(
    (mlp_convs_hidden): LayerList(
      (0): Conv2D(6, 16, kernel_size=[1, 1], data_format=NCHW)
      (1): Conv2D(16, 8, kernel_size=[1, 1], data_format=NCHW)
    )
    (mlp_bns_hidden): LayerList(
      (0): BatchNorm2D(num_features=16, momentum=0.1, epsilon=1e-05)
      (1): BatchNorm2D(num_features=8, momentum=0.1, epsilon=1e-05)
    )
  )
  (scorenet3): ScoreNet(
    (mlp_convs_hidden): LayerList(
      (0): Conv2D(6, 16, kernel_size=[1, 1], data_format=NCHW)
      (1): Conv2D(16, 8, kernel_size=[1, 1], data_format=NCHW)
    )
    (mlp_bns_hidden): LayerList(
      (0): BatchNorm2D(num_features=16, momentum=0.1, epsilon=1e-05)
      (1): BatchNorm2D(num_features=8, momentum=0.1, epsilon=1e-05)
    )
  )
  (scorenet4): ScoreNet(
    (mlp_convs_hidden): LayerList(
      (0): Conv2D(6, 16, kernel_size=[1, 1], data_format=NCHW)
      (1): Conv2D(16, 8, kernel_size=[1, 1], data_format=NCHW)
    )
    (mlp_bns_hidden): LayerList(
      (0): BatchNorm2D(num_features=16, momentum=0.1, epsilon=1e-05)
      (1): BatchNorm2D(num_features=8, momentum=0.1, epsilon=1e-05)
    )
  )
  (bn1): BatchNorm1D(num_features=64, momentum=0.1, epsilon=1e-05, data_format=NCL)
  (bn2): BatchNorm1D(num_features=64, momentum=0.1, epsilon=1e-05, data_format=NCL)
  (bn3): BatchNorm1D(num_features=128, momentum=0.1, epsilon=1e-05, data_format=NCL)
  (bn4): BatchNorm1D(num_features=256, momentum=0.1, epsilon=1e-05, data_format=NCL)
  (bn5): BatchNorm1D(num_features=1024, momentum=0.1, epsilon=1e-05, data_format=NCL)
  (conv5): Sequential(
    (0): Conv1D(512, 1024, kernel_size=[1], data_format=NCL)
    (1): BatchNorm1D(num_features=1024, momentum=0.1, epsilon=1e-05, data_format=NCL)
  )
  (linear1): Linear(in_features=2048, out_features=512, dtype=float32)
  (bn11): BatchNorm1D(num_features=512, momentum=0.1, epsilon=1e-05, data_format=NCL)
  (dp1): Dropout(p=0.5, axis=None, mode=upscale_in_train)
  (linear2): Linear(in_features=512, out_features=256, dtype=float32)
  (bn22): BatchNorm1D(num_features=256, momentum=0.1, epsilon=1e-05, data_format=NCL)
  (dp2): Dropout(p=0.5, axis=None, mode=upscale_in_train)
  (linear3): Linear(in_features=256, out_features=40, dtype=float32)
)
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:0	batch id:0	 lr:0.099996 loss:4.681467
[Train] epoch:0	batch id:10	 lr:0.099996 loss:3.851323
[Train] epoch:0	batch id:20	 lr:0.099996 loss:4.015349
[Train] epoch:0	batch id:30	 lr:0.099996 loss:3.655223
[Train] epoch:0	batch id:40	 lr:0.099996 loss:3.714255
[Train] epoch:0	batch id:50	 lr:0.099996 loss:3.613210
[Train] epoch:0	batch id:60	 lr:0.099996 loss:3.367568
[Train] epoch:0	batch id:70	 lr:0.099996 loss:3.378956
[Train] epoch:0	batch id:80	 lr:0.099996 loss:3.388882
[Train] epoch:0	batch id:90	 lr:0.099996 loss:3.175711
[Train] epoch:0	batch id:100	 lr:0.099996 loss:3.215294
[Train] epoch:0	batch id:110	 lr:0.099996 loss:3.161312
[Train] epoch:0	batch id:120	 lr:0.099996 loss:3.190067
[Train] epoch:0	batch id:130	 lr:0.099996 loss:3.082705
[Train] epoch:0	batch id:140	 lr:0.099996 loss:3.060782
[Train] epoch:0	batch id:150	 lr:0.099996 loss:3.048669
[Train] epoch:0	batch id:160	 lr:0.099996 loss:3.140438
[Train] epoch:0	batch id:170	 lr:0.099996 loss:3.030969
[Train] epoch:0	batch id:180	 lr:0.099996 loss:2.974733
[Train] epoch:0	batch id:190	 lr:0.099996 loss:2.826028
[Train] epoch:0	batch id:200	 lr:0.099996 loss:2.940431
[Train] epoch:0	batch id:210	 lr:0.099996 loss:2.670253
[Train] epoch:0	batch id:220	 lr:0.099996 loss:3.221779
[Train] epoch:0	batch id:230	 lr:0.099996 loss:2.878720
[Train] epoch:0	batch id:240	 lr:0.099996 loss:2.874891
[Train] epoch:0	batch id:250	 lr:0.099996 loss:2.844756
[Train] epoch:0	batch id:260	 lr:0.099996 loss:2.890497
[Train] epoch:0	batch id:270	 lr:0.099996 loss:2.720817
[Train] epoch:0	batch id:280	 lr:0.099996 loss:2.733163
[Train] epoch:0	batch id:290	 lr:0.099996 loss:2.707378
[Train] epoch:0	batch id:300	 lr:0.099996 loss:2.838789
[Train] 0, loss: 3.190578, train acc: 0.270562, 
[Test] epoch:0	batch id:0	 loss:2.103574
[Test] epoch:0	batch id:10	 loss:2.602399
[Test] epoch:0	batch id:20	 loss:2.423138
[Test] epoch:0	batch id:30	 loss:2.698239
[Test] epoch:0	batch id:40	 loss:2.416044
[Test] epoch:0	batch id:50	 loss:2.536357
[Test] epoch:0	batch id:60	 loss:2.045604
[Test] epoch:0	batch id:70	 loss:2.544502
[Test] epoch:0	batch id:80	 loss:2.417145
[Test] epoch:0	batch id:90	 loss:2.353309
[Test] epoch:0	batch id:100	 loss:2.979567
[Test] epoch:0	batch id:110	 loss:2.282628
[Test] epoch:0	batch id:120	 loss:2.577813
[Test] epoch:0	batch id:130	 loss:2.697823
[Test] epoch:0	batch id:140	 loss:2.091484
[Test] epoch:0	batch id:150	 loss:2.713538
[Test] 0, loss: 2.547117, test acc: 0.460292,
Max Acc:0.460292
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:1	batch id:0	 lr:0.099986 loss:2.784483
[Train] epoch:1	batch id:10	 lr:0.099986 loss:2.682193
[Train] epoch:1	batch id:20	 lr:0.099986 loss:2.917061
[Train] epoch:1	batch id:30	 lr:0.099986 loss:2.743320
[Train] epoch:1	batch id:40	 lr:0.099986 loss:2.615853
[Train] epoch:1	batch id:50	 lr:0.099986 loss:2.662030
[Train] epoch:1	batch id:60	 lr:0.099986 loss:2.660961
[Train] epoch:1	batch id:70	 lr:0.099986 loss:2.902342
[Train] epoch:1	batch id:80	 lr:0.099986 loss:2.753107
[Train] epoch:1	batch id:90	 lr:0.099986 loss:2.618095
[Train] epoch:1	batch id:100	 lr:0.099986 loss:2.689899
[Train] epoch:1	batch id:110	 lr:0.099986 loss:2.608737
[Train] epoch:1	batch id:120	 lr:0.099986 loss:2.728873
[Train] epoch:1	batch id:130	 lr:0.099986 loss:2.510689
[Train] epoch:1	batch id:140	 lr:0.099986 loss:2.520118
[Train] epoch:1	batch id:150	 lr:0.099986 loss:2.576153
[Train] epoch:1	batch id:160	 lr:0.099986 loss:2.772976
[Train] epoch:1	batch id:170	 lr:0.099986 loss:2.610610
[Train] epoch:1	batch id:180	 lr:0.099986 loss:2.487125
[Train] epoch:1	batch id:190	 lr:0.099986 loss:2.623649
[Train] epoch:1	batch id:200	 lr:0.099986 loss:2.178514
[Train] epoch:1	batch id:210	 lr:0.099986 loss:2.617895
[Train] epoch:1	batch id:220	 lr:0.099986 loss:2.319086
[Train] epoch:1	batch id:230	 lr:0.099986 loss:2.565532
[Train] epoch:1	batch id:240	 lr:0.099986 loss:2.404648
[Train] epoch:1	batch id:250	 lr:0.099986 loss:2.662358
[Train] epoch:1	batch id:260	 lr:0.099986 loss:2.769116
[Train] epoch:1	batch id:270	 lr:0.099986 loss:2.330490
[Train] epoch:1	batch id:280	 lr:0.099986 loss:2.261694
[Train] epoch:1	batch id:290	 lr:0.099986 loss:2.492064
[Train] epoch:1	batch id:300	 lr:0.099986 loss:2.447538
[Train] 1, loss: 2.603532, train acc: 0.465187, 
[Test] epoch:1	batch id:0	 loss:1.849560
[Test] epoch:1	batch id:10	 loss:2.352784
[Test] epoch:1	batch id:20	 loss:2.124089
[Test] epoch:1	batch id:30	 loss:2.466272
[Test] epoch:1	batch id:40	 loss:2.170689
[Test] epoch:1	batch id:50	 loss:2.137875
[Test] epoch:1	batch id:60	 loss:1.859951
[Test] epoch:1	batch id:70	 loss:2.272884
[Test] epoch:1	batch id:80	 loss:2.285295
[Test] epoch:1	batch id:90	 loss:2.081986
[Test] epoch:1	batch id:100	 loss:2.589770
[Test] epoch:1	batch id:110	 loss:2.049130
[Test] epoch:1	batch id:120	 loss:2.364623
[Test] epoch:1	batch id:130	 loss:2.490247
[Test] epoch:1	batch id:140	 loss:1.929669
[Test] epoch:1	batch id:150	 loss:2.460366
[Test] 1, loss: 2.287289, test acc: 0.557131,
Max Acc:0.557131
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:2	batch id:0	 lr:0.099972 loss:2.511897
[Train] epoch:2	batch id:10	 lr:0.099972 loss:2.325787
[Train] epoch:2	batch id:20	 lr:0.099972 loss:2.490423
[Train] epoch:2	batch id:30	 lr:0.099972 loss:2.580444
[Train] epoch:2	batch id:40	 lr:0.099972 loss:2.508535
[Train] epoch:2	batch id:50	 lr:0.099972 loss:2.700779
[Train] epoch:2	batch id:60	 lr:0.099972 loss:2.515684
[Train] epoch:2	batch id:70	 lr:0.099972 loss:2.449121
[Train] epoch:2	batch id:80	 lr:0.099972 loss:2.515093
[Train] epoch:2	batch id:90	 lr:0.099972 loss:2.390657
[Train] epoch:2	batch id:100	 lr:0.099972 loss:2.493186
[Train] epoch:2	batch id:110	 lr:0.099972 loss:2.610943
[Train] epoch:2	batch id:120	 lr:0.099972 loss:2.612504
[Train] epoch:2	batch id:130	 lr:0.099972 loss:2.375526
[Train] epoch:2	batch id:140	 lr:0.099972 loss:2.400802
[Train] epoch:2	batch id:150	 lr:0.099972 loss:2.543370
[Train] epoch:2	batch id:160	 lr:0.099972 loss:2.402322
[Train] epoch:2	batch id:170	 lr:0.099972 loss:2.457816
[Train] epoch:2	batch id:180	 lr:0.099972 loss:2.378053
[Train] epoch:2	batch id:190	 lr:0.099972 loss:2.479494
[Train] epoch:2	batch id:200	 lr:0.099972 loss:2.345123
[Train] epoch:2	batch id:210	 lr:0.099972 loss:2.477396
[Train] epoch:2	batch id:220	 lr:0.099972 loss:2.220345
[Train] epoch:2	batch id:230	 lr:0.099972 loss:2.508598
[Train] epoch:2	batch id:240	 lr:0.099972 loss:2.446976
[Train] epoch:2	batch id:250	 lr:0.099972 loss:2.338743
[Train] epoch:2	batch id:260	 lr:0.099972 loss:2.235337
[Train] epoch:2	batch id:270	 lr:0.099972 loss:2.527561
[Train] epoch:2	batch id:280	 lr:0.099972 loss:2.365111
[Train] epoch:2	batch id:290	 lr:0.099972 loss:2.321459
[Train] epoch:2	batch id:300	 lr:0.099972 loss:2.146377
[Train] 2, loss: 2.415252, train acc: 0.534406, 
[Test] epoch:2	batch id:0	 loss:1.745832
[Test] epoch:2	batch id:10	 loss:2.128886
[Test] epoch:2	batch id:20	 loss:1.824755
[Test] epoch:2	batch id:30	 loss:2.235980
[Test] epoch:2	batch id:40	 loss:1.941014
[Test] epoch:2	batch id:50	 loss:1.810821
[Test] epoch:2	batch id:60	 loss:1.683158
[Test] epoch:2	batch id:70	 loss:2.274110
[Test] epoch:2	batch id:80	 loss:2.157404
[Test] epoch:2	batch id:90	 loss:1.830094
[Test] epoch:2	batch id:100	 loss:2.348464
[Test] epoch:2	batch id:110	 loss:1.780724
[Test] epoch:2	batch id:120	 loss:2.265912
[Test] epoch:2	batch id:130	 loss:2.297152
[Test] epoch:2	batch id:140	 loss:1.848852
[Test] epoch:2	batch id:150	 loss:2.532115
[Test] 2, loss: 2.120202, test acc: 0.661669,
Max Acc:0.661669
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:3	batch id:0	 lr:0.099954 loss:2.519019
[Train] epoch:3	batch id:10	 lr:0.099954 loss:2.284985
[Train] epoch:3	batch id:20	 lr:0.099954 loss:2.483208
[Train] epoch:3	batch id:30	 lr:0.099954 loss:2.424817
[Train] epoch:3	batch id:40	 lr:0.099954 loss:2.144244
[Train] epoch:3	batch id:50	 lr:0.099954 loss:2.297249
[Train] epoch:3	batch id:60	 lr:0.099954 loss:2.083758
[Train] epoch:3	batch id:70	 lr:0.099954 loss:2.522872
[Train] epoch:3	batch id:80	 lr:0.099954 loss:2.463131
[Train] epoch:3	batch id:90	 lr:0.099954 loss:2.801998
[Train] epoch:3	batch id:100	 lr:0.099954 loss:2.173683
[Train] epoch:3	batch id:110	 lr:0.099954 loss:2.267868
[Train] epoch:3	batch id:120	 lr:0.099954 loss:2.276708
[Train] epoch:3	batch id:130	 lr:0.099954 loss:2.116460
[Train] epoch:3	batch id:140	 lr:0.099954 loss:2.144598
[Train] epoch:3	batch id:150	 lr:0.099954 loss:2.460707
[Train] epoch:3	batch id:160	 lr:0.099954 loss:2.117415
[Train] epoch:3	batch id:170	 lr:0.099954 loss:2.217056
[Train] epoch:3	batch id:180	 lr:0.099954 loss:2.424208
[Train] epoch:3	batch id:190	 lr:0.099954 loss:2.453256
[Train] epoch:3	batch id:200	 lr:0.099954 loss:2.184113
[Train] epoch:3	batch id:210	 lr:0.099954 loss:2.210396
[Train] epoch:3	batch id:220	 lr:0.099954 loss:2.377608
[Train] epoch:3	batch id:230	 lr:0.099954 loss:2.214763
[Train] epoch:3	batch id:240	 lr:0.099954 loss:2.433187
[Train] epoch:3	batch id:250	 lr:0.099954 loss:2.578773
[Train] epoch:3	batch id:260	 lr:0.099954 loss:2.415138
[Train] epoch:3	batch id:270	 lr:0.099954 loss:2.363350
[Train] epoch:3	batch id:280	 lr:0.099954 loss:2.300933
[Train] epoch:3	batch id:290	 lr:0.099954 loss:2.288587
[Train] epoch:3	batch id:300	 lr:0.099954 loss:2.498057
[Train] 3, loss: 2.327058, train acc: 0.575122, 
[Test] epoch:3	batch id:0	 loss:1.666313
[Test] epoch:3	batch id:10	 loss:2.055425
[Test] epoch:3	batch id:20	 loss:1.867103
[Test] epoch:3	batch id:30	 loss:2.166750
[Test] epoch:3	batch id:40	 loss:1.946219
[Test] epoch:3	batch id:50	 loss:1.826825
[Test] epoch:3	batch id:60	 loss:1.574427
[Test] epoch:3	batch id:70	 loss:2.046381
[Test] epoch:3	batch id:80	 loss:2.049061
[Test] epoch:3	batch id:90	 loss:1.918841
[Test] epoch:3	batch id:100	 loss:2.278918
[Test] epoch:3	batch id:110	 loss:1.739140
[Test] epoch:3	batch id:120	 loss:2.119691
[Test] epoch:3	batch id:130	 loss:1.997665
[Test] epoch:3	batch id:140	 loss:1.778538
[Test] epoch:3	batch id:150	 loss:2.190285
[Test] 3, loss: 2.017536, test acc: 0.696921,
Max Acc:0.696921
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:4	batch id:0	 lr:0.099932 loss:2.128622
[Train] epoch:4	batch id:10	 lr:0.099932 loss:2.298533
[Train] epoch:4	batch id:20	 lr:0.099932 loss:2.434687
[Train] epoch:4	batch id:30	 lr:0.099932 loss:2.195848
[Train] epoch:4	batch id:40	 lr:0.099932 loss:2.214598
[Train] epoch:4	batch id:50	 lr:0.099932 loss:2.115045
[Train] epoch:4	batch id:60	 lr:0.099932 loss:2.156080
[Train] epoch:4	batch id:70	 lr:0.099932 loss:2.164269
[Train] epoch:4	batch id:80	 lr:0.099932 loss:2.338104
[Train] epoch:4	batch id:90	 lr:0.099932 loss:2.068960
[Train] epoch:4	batch id:100	 lr:0.099932 loss:2.141599
[Train] epoch:4	batch id:110	 lr:0.099932 loss:2.363141
[Train] epoch:4	batch id:120	 lr:0.099932 loss:2.393125
[Train] epoch:4	batch id:130	 lr:0.099932 loss:2.053275
[Train] epoch:4	batch id:140	 lr:0.099932 loss:2.070882
[Train] epoch:4	batch id:150	 lr:0.099932 loss:2.264608
[Train] epoch:4	batch id:160	 lr:0.099932 loss:2.273480
[Train] epoch:4	batch id:170	 lr:0.099932 loss:1.992514
[Train] epoch:4	batch id:180	 lr:0.099932 loss:2.283379
[Train] epoch:4	batch id:190	 lr:0.099932 loss:2.435947
[Train] epoch:4	batch id:200	 lr:0.099932 loss:2.211144
[Train] epoch:4	batch id:210	 lr:0.099932 loss:2.336096
[Train] epoch:4	batch id:220	 lr:0.099932 loss:2.257400
[Train] epoch:4	batch id:230	 lr:0.099932 loss:2.449364
[Train] epoch:4	batch id:240	 lr:0.099932 loss:2.373113
[Train] epoch:4	batch id:250	 lr:0.099932 loss:2.184926
[Train] epoch:4	batch id:260	 lr:0.099932 loss:2.241063
[Train] epoch:4	batch id:270	 lr:0.099932 loss:2.191162
[Train] epoch:4	batch id:280	 lr:0.099932 loss:2.168803
[Train] epoch:4	batch id:290	 lr:0.099932 loss:2.497210
[Train] epoch:4	batch id:300	 lr:0.099932 loss:2.191233
[Train] 4, loss: 2.250744, train acc: 0.601283, 
[Test] epoch:4	batch id:0	 loss:1.639668
[Test] epoch:4	batch id:10	 loss:1.994346
[Test] epoch:4	batch id:20	 loss:1.790133
[Test] epoch:4	batch id:30	 loss:2.187847
[Test] epoch:4	batch id:40	 loss:1.964454
[Test] epoch:4	batch id:50	 loss:1.816406
[Test] epoch:4	batch id:60	 loss:1.601163
[Test] epoch:4	batch id:70	 loss:2.014614
[Test] epoch:4	batch id:80	 loss:2.022153
[Test] epoch:4	batch id:90	 loss:1.897650
[Test] epoch:4	batch id:100	 loss:2.308080
[Test] epoch:4	batch id:110	 loss:1.741989
[Test] epoch:4	batch id:120	 loss:2.131035
[Test] epoch:4	batch id:130	 loss:2.152913
[Test] epoch:4	batch id:140	 loss:1.922577
[Test] epoch:4	batch id:150	 loss:2.213835
[Test] 4, loss: 2.004196, test acc: 0.704214,
Max Acc:0.704214
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:5	batch id:0	 lr:0.099906 loss:2.514085
[Train] epoch:5	batch id:10	 lr:0.099906 loss:2.466177
[Train] epoch:5	batch id:20	 lr:0.099906 loss:2.019001
[Train] epoch:5	batch id:30	 lr:0.099906 loss:2.087706
[Train] epoch:5	batch id:40	 lr:0.099906 loss:2.357558
[Train] epoch:5	batch id:50	 lr:0.099906 loss:2.109713
[Train] epoch:5	batch id:60	 lr:0.099906 loss:2.295426
[Train] epoch:5	batch id:70	 lr:0.099906 loss:2.123355
[Train] epoch:5	batch id:80	 lr:0.099906 loss:1.992734
[Train] epoch:5	batch id:90	 lr:0.099906 loss:2.113846
[Train] epoch:5	batch id:100	 lr:0.099906 loss:1.985699
[Train] epoch:5	batch id:110	 lr:0.099906 loss:2.249348
[Train] epoch:5	batch id:120	 lr:0.099906 loss:2.060718
[Train] epoch:5	batch id:130	 lr:0.099906 loss:2.111492
[Train] epoch:5	batch id:140	 lr:0.099906 loss:2.229463
[Train] epoch:5	batch id:150	 lr:0.099906 loss:2.155270
[Train] epoch:5	batch id:160	 lr:0.099906 loss:1.962339
[Train] epoch:5	batch id:170	 lr:0.099906 loss:2.186227
[Train] epoch:5	batch id:180	 lr:0.099906 loss:2.285763
[Train] epoch:5	batch id:190	 lr:0.099906 loss:2.124889
[Train] epoch:5	batch id:200	 lr:0.099906 loss:2.274333
[Train] epoch:5	batch id:210	 lr:0.099906 loss:2.482446
[Train] epoch:5	batch id:220	 lr:0.099906 loss:2.181466
[Train] epoch:5	batch id:230	 lr:0.099906 loss:2.068455
[Train] epoch:5	batch id:240	 lr:0.099906 loss:1.902890
[Train] epoch:5	batch id:250	 lr:0.099906 loss:1.887090
[Train] epoch:5	batch id:260	 lr:0.099906 loss:2.300764
[Train] epoch:5	batch id:270	 lr:0.099906 loss:2.206838
[Train] epoch:5	batch id:280	 lr:0.099906 loss:2.369345
[Train] epoch:5	batch id:290	 lr:0.099906 loss:2.059154
[Train] epoch:5	batch id:300	 lr:0.099906 loss:2.357992
[Train] 5, loss: 2.191172, train acc: 0.625509, 
[Test] epoch:5	batch id:0	 loss:1.679082
[Test] epoch:5	batch id:10	 loss:1.891870
[Test] epoch:5	batch id:20	 loss:1.680902
[Test] epoch:5	batch id:30	 loss:2.000010
[Test] epoch:5	batch id:40	 loss:1.807685
[Test] epoch:5	batch id:50	 loss:1.897867
[Test] epoch:5	batch id:60	 loss:1.557445
[Test] epoch:5	batch id:70	 loss:1.886800
[Test] epoch:5	batch id:80	 loss:2.014590
[Test] epoch:5	batch id:90	 loss:1.845196
[Test] epoch:5	batch id:100	 loss:2.328451
[Test] epoch:5	batch id:110	 loss:1.662410
[Test] epoch:5	batch id:120	 loss:1.912301
[Test] epoch:5	batch id:130	 loss:1.915982
[Test] epoch:5	batch id:140	 loss:1.854282
[Test] epoch:5	batch id:150	 loss:2.109114
[Test] 5, loss: 1.919957, test acc: 0.730956,
Max Acc:0.730956
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:6	batch id:0	 lr:0.099876 loss:2.159108
[Train] epoch:6	batch id:10	 lr:0.099876 loss:2.403592
[Train] epoch:6	batch id:20	 lr:0.099876 loss:2.073407
[Train] epoch:6	batch id:30	 lr:0.099876 loss:2.137016
[Train] epoch:6	batch id:40	 lr:0.099876 loss:2.257547
[Train] epoch:6	batch id:50	 lr:0.099876 loss:2.208952
[Train] epoch:6	batch id:60	 lr:0.099876 loss:2.226712
[Train] epoch:6	batch id:70	 lr:0.099876 loss:2.049055
[Train] epoch:6	batch id:80	 lr:0.099876 loss:2.272575
[Train] epoch:6	batch id:90	 lr:0.099876 loss:1.892264
[Train] epoch:6	batch id:100	 lr:0.099876 loss:2.113483
[Train] epoch:6	batch id:110	 lr:0.099876 loss:2.283322
[Train] epoch:6	batch id:120	 lr:0.099876 loss:1.873690
[Train] epoch:6	batch id:130	 lr:0.099876 loss:2.059971
[Train] epoch:6	batch id:140	 lr:0.099876 loss:1.965269
[Train] epoch:6	batch id:150	 lr:0.099876 loss:2.149104
[Train] epoch:6	batch id:160	 lr:0.099876 loss:1.955284
[Train] epoch:6	batch id:170	 lr:0.099876 loss:2.112871
[Train] epoch:6	batch id:180	 lr:0.099876 loss:2.206738
[Train] epoch:6	batch id:190	 lr:0.099876 loss:2.187860
[Train] epoch:6	batch id:200	 lr:0.099876 loss:2.028599
[Train] epoch:6	batch id:210	 lr:0.099876 loss:2.012300
[Train] epoch:6	batch id:220	 lr:0.099876 loss:2.229434
[Train] epoch:6	batch id:230	 lr:0.099876 loss:2.091275
[Train] epoch:6	batch id:240	 lr:0.099876 loss:2.153336
[Train] epoch:6	batch id:250	 lr:0.099876 loss:2.199971
[Train] epoch:6	batch id:260	 lr:0.099876 loss:2.101214
[Train] epoch:6	batch id:270	 lr:0.099876 loss:2.026126
[Train] epoch:6	batch id:280	 lr:0.099876 loss:1.953152
[Train] epoch:6	batch id:290	 lr:0.099876 loss:1.993580
[Train] epoch:6	batch id:300	 lr:0.099876 loss:1.906638
[Train] 6, loss: 2.147324, train acc: 0.644951, 
[Test] epoch:6	batch id:0	 loss:1.613047
[Test] epoch:6	batch id:10	 loss:1.928856
[Test] epoch:6	batch id:20	 loss:1.699471
[Test] epoch:6	batch id:30	 loss:1.992448
[Test] epoch:6	batch id:40	 loss:1.852967
[Test] epoch:6	batch id:50	 loss:1.801996
[Test] epoch:6	batch id:60	 loss:1.534287
[Test] epoch:6	batch id:70	 loss:1.864033
[Test] epoch:6	batch id:80	 loss:1.907892
[Test] epoch:6	batch id:90	 loss:1.751998
[Test] epoch:6	batch id:100	 loss:2.372790
[Test] epoch:6	batch id:110	 loss:1.650975
[Test] epoch:6	batch id:120	 loss:1.977708
[Test] epoch:6	batch id:130	 loss:1.790405
[Test] epoch:6	batch id:140	 loss:1.718616
[Test] epoch:6	batch id:150	 loss:2.223155
[Test] 6, loss: 1.901850, test acc: 0.730551,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:7	batch id:0	 lr:0.099843 loss:1.913000
[Train] epoch:7	batch id:10	 lr:0.099843 loss:1.933538
[Train] epoch:7	batch id:20	 lr:0.099843 loss:1.768672
[Train] epoch:7	batch id:30	 lr:0.099843 loss:2.122910
[Train] epoch:7	batch id:40	 lr:0.099843 loss:2.215473
[Train] epoch:7	batch id:50	 lr:0.099843 loss:2.132587
[Train] epoch:7	batch id:60	 lr:0.099843 loss:1.885041
[Train] epoch:7	batch id:70	 lr:0.099843 loss:1.946243
[Train] epoch:7	batch id:80	 lr:0.099843 loss:2.162669
[Train] epoch:7	batch id:90	 lr:0.099843 loss:1.926406
[Train] epoch:7	batch id:100	 lr:0.099843 loss:2.281020
[Train] epoch:7	batch id:110	 lr:0.099843 loss:2.150924
[Train] epoch:7	batch id:120	 lr:0.099843 loss:2.050715
[Train] epoch:7	batch id:130	 lr:0.099843 loss:1.939173
[Train] epoch:7	batch id:140	 lr:0.099843 loss:2.102709
[Train] epoch:7	batch id:150	 lr:0.099843 loss:2.100699
[Train] epoch:7	batch id:160	 lr:0.099843 loss:2.144184
[Train] epoch:7	batch id:170	 lr:0.099843 loss:2.069200
[Train] epoch:7	batch id:180	 lr:0.099843 loss:2.245007
[Train] epoch:7	batch id:190	 lr:0.099843 loss:2.006603
[Train] epoch:7	batch id:200	 lr:0.099843 loss:1.934010
[Train] epoch:7	batch id:210	 lr:0.099843 loss:1.977199
[Train] epoch:7	batch id:220	 lr:0.099843 loss:2.259340
[Train] epoch:7	batch id:230	 lr:0.099843 loss:2.230884
[Train] epoch:7	batch id:240	 lr:0.099843 loss:2.049560
[Train] epoch:7	batch id:250	 lr:0.099843 loss:2.045927
[Train] epoch:7	batch id:260	 lr:0.099843 loss:2.103313
[Train] epoch:7	batch id:270	 lr:0.099843 loss:1.985728
[Train] epoch:7	batch id:280	 lr:0.099843 loss:2.026166
[Train] epoch:7	batch id:290	 lr:0.099843 loss:2.097861
[Train] epoch:7	batch id:300	 lr:0.099843 loss:2.274746
[Train] 7, loss: 2.113257, train acc: 0.665513, 
[Test] epoch:7	batch id:0	 loss:1.686559
[Test] epoch:7	batch id:10	 loss:1.821953
[Test] epoch:7	batch id:20	 loss:1.759881
[Test] epoch:7	batch id:30	 loss:2.068413
[Test] epoch:7	batch id:40	 loss:1.818083
[Test] epoch:7	batch id:50	 loss:1.850844
[Test] epoch:7	batch id:60	 loss:1.559687
[Test] epoch:7	batch id:70	 loss:1.820035
[Test] epoch:7	batch id:80	 loss:1.882534
[Test] epoch:7	batch id:90	 loss:1.827148
[Test] epoch:7	batch id:100	 loss:2.325571
[Test] epoch:7	batch id:110	 loss:1.744260
[Test] epoch:7	batch id:120	 loss:2.003562
[Test] epoch:7	batch id:130	 loss:1.962650
[Test] epoch:7	batch id:140	 loss:1.909349
[Test] epoch:7	batch id:150	 loss:2.188040
[Test] 7, loss: 1.928722, test acc: 0.739465,
Max Acc:0.739465
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:8	batch id:0	 lr:0.099805 loss:2.075295
[Train] epoch:8	batch id:10	 lr:0.099805 loss:2.194367
[Train] epoch:8	batch id:20	 lr:0.099805 loss:2.017952
[Train] epoch:8	batch id:30	 lr:0.099805 loss:2.107065
[Train] epoch:8	batch id:40	 lr:0.099805 loss:2.457709
[Train] epoch:8	batch id:50	 lr:0.099805 loss:1.831617
[Train] epoch:8	batch id:60	 lr:0.099805 loss:1.903425
[Train] epoch:8	batch id:70	 lr:0.099805 loss:1.900335
[Train] epoch:8	batch id:80	 lr:0.099805 loss:2.389031
[Train] epoch:8	batch id:90	 lr:0.099805 loss:1.866402
[Train] epoch:8	batch id:100	 lr:0.099805 loss:1.998352
[Train] epoch:8	batch id:110	 lr:0.099805 loss:2.369030
[Train] epoch:8	batch id:120	 lr:0.099805 loss:2.559271
[Train] epoch:8	batch id:130	 lr:0.099805 loss:2.138433
[Train] epoch:8	batch id:140	 lr:0.099805 loss:1.975753
[Train] epoch:8	batch id:150	 lr:0.099805 loss:2.111444
[Train] epoch:8	batch id:160	 lr:0.099805 loss:1.848937
[Train] epoch:8	batch id:170	 lr:0.099805 loss:1.978414
[Train] epoch:8	batch id:180	 lr:0.099805 loss:2.041678
[Train] epoch:8	batch id:190	 lr:0.099805 loss:1.929975
[Train] epoch:8	batch id:200	 lr:0.099805 loss:1.924125
[Train] epoch:8	batch id:210	 lr:0.099805 loss:1.947044
[Train] epoch:8	batch id:220	 lr:0.099805 loss:2.245261
[Train] epoch:8	batch id:230	 lr:0.099805 loss:2.267867
[Train] epoch:8	batch id:240	 lr:0.099805 loss:2.232788
[Train] epoch:8	batch id:250	 lr:0.099805 loss:2.124598
[Train] epoch:8	batch id:260	 lr:0.099805 loss:2.345923
[Train] epoch:8	batch id:270	 lr:0.099805 loss:2.163939
[Train] epoch:8	batch id:280	 lr:0.099805 loss:2.042023
[Train] epoch:8	batch id:290	 lr:0.099805 loss:1.989373
[Train] epoch:8	batch id:300	 lr:0.099805 loss:2.060795
[Train] 8, loss: 2.086196, train acc: 0.675590, 
[Test] epoch:8	batch id:0	 loss:1.742475
[Test] epoch:8	batch id:10	 loss:1.917097
[Test] epoch:8	batch id:20	 loss:1.687998
[Test] epoch:8	batch id:30	 loss:1.902291
[Test] epoch:8	batch id:40	 loss:1.789320
[Test] epoch:8	batch id:50	 loss:1.830668
[Test] epoch:8	batch id:60	 loss:1.474352
[Test] epoch:8	batch id:70	 loss:1.862262
[Test] epoch:8	batch id:80	 loss:2.063941
[Test] epoch:8	batch id:90	 loss:1.754207
[Test] epoch:8	batch id:100	 loss:2.417856
[Test] epoch:8	batch id:110	 loss:1.663211
[Test] epoch:8	batch id:120	 loss:1.885575
[Test] epoch:8	batch id:130	 loss:1.870141
[Test] epoch:8	batch id:140	 loss:1.730828
[Test] epoch:8	batch id:150	 loss:2.182969
[Test] 8, loss: 1.894659, test acc: 0.731767,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:9	batch id:0	 lr:0.099763 loss:1.846388
[Train] epoch:9	batch id:10	 lr:0.099763 loss:2.154016
[Train] epoch:9	batch id:20	 lr:0.099763 loss:2.058282
[Train] epoch:9	batch id:30	 lr:0.099763 loss:2.116499
[Train] epoch:9	batch id:40	 lr:0.099763 loss:1.950770
[Train] epoch:9	batch id:50	 lr:0.099763 loss:2.146416
[Train] epoch:9	batch id:60	 lr:0.099763 loss:2.116360
[Train] epoch:9	batch id:70	 lr:0.099763 loss:2.097437
[Train] epoch:9	batch id:80	 lr:0.099763 loss:1.921807
[Train] epoch:9	batch id:90	 lr:0.099763 loss:1.886656
[Train] epoch:9	batch id:100	 lr:0.099763 loss:2.060550
[Train] epoch:9	batch id:110	 lr:0.099763 loss:1.962152
[Train] epoch:9	batch id:120	 lr:0.099763 loss:1.935101
[Train] epoch:9	batch id:130	 lr:0.099763 loss:1.932337
[Train] epoch:9	batch id:140	 lr:0.099763 loss:2.304845
[Train] epoch:9	batch id:150	 lr:0.099763 loss:2.010783
[Train] epoch:9	batch id:160	 lr:0.099763 loss:2.052007
[Train] epoch:9	batch id:170	 lr:0.099763 loss:2.210559
[Train] epoch:9	batch id:180	 lr:0.099763 loss:1.943758
[Train] epoch:9	batch id:190	 lr:0.099763 loss:2.179729
[Train] epoch:9	batch id:200	 lr:0.099763 loss:1.954051
[Train] epoch:9	batch id:210	 lr:0.099763 loss:2.071790
[Train] epoch:9	batch id:220	 lr:0.099763 loss:1.906039
[Train] epoch:9	batch id:230	 lr:0.099763 loss:2.183603
[Train] epoch:9	batch id:240	 lr:0.099763 loss:2.111687
[Train] epoch:9	batch id:250	 lr:0.099763 loss:2.120182
[Train] epoch:9	batch id:260	 lr:0.099763 loss:2.084332
[Train] epoch:9	batch id:270	 lr:0.099763 loss:1.841014
[Train] epoch:9	batch id:280	 lr:0.099763 loss:1.809199
[Train] epoch:9	batch id:290	 lr:0.099763 loss:2.009538
[Train] epoch:9	batch id:300	 lr:0.099763 loss:1.894142
[Train] 9, loss: 2.055027, train acc: 0.688518, 
[Test] epoch:9	batch id:0	 loss:1.617055
[Test] epoch:9	batch id:10	 loss:1.824055
[Test] epoch:9	batch id:20	 loss:1.674681
[Test] epoch:9	batch id:30	 loss:1.751266
[Test] epoch:9	batch id:40	 loss:1.774587
[Test] epoch:9	batch id:50	 loss:1.771245
[Test] epoch:9	batch id:60	 loss:1.491248
[Test] epoch:9	batch id:70	 loss:1.818967
[Test] epoch:9	batch id:80	 loss:1.869980
[Test] epoch:9	batch id:90	 loss:1.584389
[Test] epoch:9	batch id:100	 loss:2.263896
[Test] epoch:9	batch id:110	 loss:1.600922
[Test] epoch:9	batch id:120	 loss:1.773548
[Test] epoch:9	batch id:130	 loss:1.767173
[Test] epoch:9	batch id:140	 loss:1.679004
[Test] epoch:9	batch id:150	 loss:1.976417
[Test] 9, loss: 1.806901, test acc: 0.780794,
Max Acc:0.780794
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:10	batch id:0	 lr:0.099717 loss:2.244545
[Train] epoch:10	batch id:10	 lr:0.099717 loss:2.089045
[Train] epoch:10	batch id:20	 lr:0.099717 loss:2.009763
[Train] epoch:10	batch id:30	 lr:0.099717 loss:1.996542
[Train] epoch:10	batch id:40	 lr:0.099717 loss:1.911793
[Train] epoch:10	batch id:50	 lr:0.099717 loss:2.379770
[Train] epoch:10	batch id:60	 lr:0.099717 loss:2.111899
[Train] epoch:10	batch id:70	 lr:0.099717 loss:1.886990
[Train] epoch:10	batch id:80	 lr:0.099717 loss:1.951207
[Train] epoch:10	batch id:90	 lr:0.099717 loss:2.303596
[Train] epoch:10	batch id:100	 lr:0.099717 loss:2.009706
[Train] epoch:10	batch id:110	 lr:0.099717 loss:2.138766
[Train] epoch:10	batch id:120	 lr:0.099717 loss:2.285543
[Train] epoch:10	batch id:130	 lr:0.099717 loss:2.355867
[Train] epoch:10	batch id:140	 lr:0.099717 loss:1.978766
[Train] epoch:10	batch id:150	 lr:0.099717 loss:1.784685
[Train] epoch:10	batch id:160	 lr:0.099717 loss:2.006604
[Train] epoch:10	batch id:170	 lr:0.099717 loss:2.067208
[Train] epoch:10	batch id:180	 lr:0.099717 loss:1.843339
[Train] epoch:10	batch id:190	 lr:0.099717 loss:1.887530
[Train] epoch:10	batch id:200	 lr:0.099717 loss:1.962399
[Train] epoch:10	batch id:210	 lr:0.099717 loss:2.126167
[Train] epoch:10	batch id:220	 lr:0.099717 loss:1.828389
[Train] epoch:10	batch id:230	 lr:0.099717 loss:2.232631
[Train] epoch:10	batch id:240	 lr:0.099717 loss:2.054592
[Train] epoch:10	batch id:250	 lr:0.099717 loss:1.853591
[Train] epoch:10	batch id:260	 lr:0.099717 loss:2.088223
[Train] epoch:10	batch id:270	 lr:0.099717 loss:1.770803
[Train] epoch:10	batch id:280	 lr:0.099717 loss:2.100255
[Train] epoch:10	batch id:290	 lr:0.099717 loss:2.091155
[Train] epoch:10	batch id:300	 lr:0.099717 loss:2.198570
[Train] 10, loss: 2.033537, train acc: 0.697272, 
[Test] epoch:10	batch id:0	 loss:1.606953
[Test] epoch:10	batch id:10	 loss:1.836761
[Test] epoch:10	batch id:20	 loss:1.631147
[Test] epoch:10	batch id:30	 loss:1.889094
[Test] epoch:10	batch id:40	 loss:1.710068
[Test] epoch:10	batch id:50	 loss:1.738155
[Test] epoch:10	batch id:60	 loss:1.459710
[Test] epoch:10	batch id:70	 loss:1.790369
[Test] epoch:10	batch id:80	 loss:1.904679
[Test] epoch:10	batch id:90	 loss:1.603361
[Test] epoch:10	batch id:100	 loss:2.112718
[Test] epoch:10	batch id:110	 loss:1.560743
[Test] epoch:10	batch id:120	 loss:1.882321
[Test] epoch:10	batch id:130	 loss:1.671030
[Test] epoch:10	batch id:140	 loss:1.596929
[Test] epoch:10	batch id:150	 loss:2.134560
[Test] 10, loss: 1.815782, test acc: 0.779579,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:11	batch id:0	 lr:0.099667 loss:2.140567
[Train] epoch:11	batch id:10	 lr:0.099667 loss:1.926981
[Train] epoch:11	batch id:20	 lr:0.099667 loss:2.042725
[Train] epoch:11	batch id:30	 lr:0.099667 loss:1.991678
[Train] epoch:11	batch id:40	 lr:0.099667 loss:1.694530
[Train] epoch:11	batch id:50	 lr:0.099667 loss:2.153382
[Train] epoch:11	batch id:60	 lr:0.099667 loss:1.909937
[Train] epoch:11	batch id:70	 lr:0.099667 loss:1.754801
[Train] epoch:11	batch id:80	 lr:0.099667 loss:2.128794
[Train] epoch:11	batch id:90	 lr:0.099667 loss:1.805184
[Train] epoch:11	batch id:100	 lr:0.099667 loss:1.867581
[Train] epoch:11	batch id:110	 lr:0.099667 loss:2.086570
[Train] epoch:11	batch id:120	 lr:0.099667 loss:2.112092
[Train] epoch:11	batch id:130	 lr:0.099667 loss:1.885760
[Train] epoch:11	batch id:140	 lr:0.099667 loss:2.184897
[Train] epoch:11	batch id:150	 lr:0.099667 loss:1.977654
[Train] epoch:11	batch id:160	 lr:0.099667 loss:1.973681
[Train] epoch:11	batch id:170	 lr:0.099667 loss:1.924292
[Train] epoch:11	batch id:180	 lr:0.099667 loss:1.857165
[Train] epoch:11	batch id:190	 lr:0.099667 loss:2.009137
[Train] epoch:11	batch id:200	 lr:0.099667 loss:2.091175
[Train] epoch:11	batch id:210	 lr:0.099667 loss:2.198258
[Train] epoch:11	batch id:220	 lr:0.099667 loss:1.906801
[Train] epoch:11	batch id:230	 lr:0.099667 loss:1.903207
[Train] epoch:11	batch id:240	 lr:0.099667 loss:1.996824
[Train] epoch:11	batch id:250	 lr:0.099667 loss:2.038256
[Train] epoch:11	batch id:260	 lr:0.099667 loss:2.188866
[Train] epoch:11	batch id:270	 lr:0.099667 loss:1.836553
[Train] epoch:11	batch id:280	 lr:0.099667 loss:2.024112
[Train] epoch:11	batch id:290	 lr:0.099667 loss:1.975782
[Train] epoch:11	batch id:300	 lr:0.099667 loss:2.282976
[Train] 11, loss: 2.004661, train acc: 0.712134, 
[Test] epoch:11	batch id:0	 loss:1.747670
[Test] epoch:11	batch id:10	 loss:1.955657
[Test] epoch:11	batch id:20	 loss:1.707395
[Test] epoch:11	batch id:30	 loss:1.820370
[Test] epoch:11	batch id:40	 loss:1.893265
[Test] epoch:11	batch id:50	 loss:1.891807
[Test] epoch:11	batch id:60	 loss:1.542567
[Test] epoch:11	batch id:70	 loss:1.846128
[Test] epoch:11	batch id:80	 loss:1.926134
[Test] epoch:11	batch id:90	 loss:1.707685
[Test] epoch:11	batch id:100	 loss:2.549150
[Test] epoch:11	batch id:110	 loss:1.702527
[Test] epoch:11	batch id:120	 loss:1.950517
[Test] epoch:11	batch id:130	 loss:1.847803
[Test] epoch:11	batch id:140	 loss:1.739345
[Test] epoch:11	batch id:150	 loss:2.118078
[Test] 11, loss: 1.853388, test acc: 0.767828,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:12	batch id:0	 lr:0.099614 loss:1.695235
[Train] epoch:12	batch id:10	 lr:0.099614 loss:2.213729
[Train] epoch:12	batch id:20	 lr:0.099614 loss:1.815077
[Train] epoch:12	batch id:30	 lr:0.099614 loss:2.017506
[Train] epoch:12	batch id:40	 lr:0.099614 loss:2.088192
[Train] epoch:12	batch id:50	 lr:0.099614 loss:1.919603
[Train] epoch:12	batch id:60	 lr:0.099614 loss:2.214654
[Train] epoch:12	batch id:70	 lr:0.099614 loss:2.190444
[Train] epoch:12	batch id:80	 lr:0.099614 loss:1.825732
[Train] epoch:12	batch id:90	 lr:0.099614 loss:1.780401
[Train] epoch:12	batch id:100	 lr:0.099614 loss:1.964873
[Train] epoch:12	batch id:110	 lr:0.099614 loss:2.159350
[Train] epoch:12	batch id:120	 lr:0.099614 loss:1.896089
[Train] epoch:12	batch id:130	 lr:0.099614 loss:2.187450
[Train] epoch:12	batch id:140	 lr:0.099614 loss:2.048913
[Train] epoch:12	batch id:150	 lr:0.099614 loss:1.775490
[Train] epoch:12	batch id:160	 lr:0.099614 loss:1.690098
[Train] epoch:12	batch id:170	 lr:0.099614 loss:2.006043
[Train] epoch:12	batch id:180	 lr:0.099614 loss:2.225420
[Train] epoch:12	batch id:190	 lr:0.099614 loss:1.979833
[Train] epoch:12	batch id:200	 lr:0.099614 loss:2.110160
[Train] epoch:12	batch id:210	 lr:0.099614 loss:2.022500
[Train] epoch:12	batch id:220	 lr:0.099614 loss:2.142917
[Train] epoch:12	batch id:230	 lr:0.099614 loss:1.986528
[Train] epoch:12	batch id:240	 lr:0.099614 loss:2.134031
[Train] epoch:12	batch id:250	 lr:0.099614 loss:1.973913
[Train] epoch:12	batch id:260	 lr:0.099614 loss:1.879060
[Train] epoch:12	batch id:270	 lr:0.099614 loss:2.125957
[Train] epoch:12	batch id:280	 lr:0.099614 loss:2.097911
[Train] epoch:12	batch id:290	 lr:0.099614 loss:1.855127
[Train] epoch:12	batch id:300	 lr:0.099614 loss:1.973078
[Train] 12, loss: 1.992949, train acc: 0.717020, 
[Test] epoch:12	batch id:0	 loss:1.535313
[Test] epoch:12	batch id:10	 loss:1.749235
[Test] epoch:12	batch id:20	 loss:1.622707
[Test] epoch:12	batch id:30	 loss:1.777889
[Test] epoch:12	batch id:40	 loss:1.655750
[Test] epoch:12	batch id:50	 loss:1.650559
[Test] epoch:12	batch id:60	 loss:1.393447
[Test] epoch:12	batch id:70	 loss:1.652576
[Test] epoch:12	batch id:80	 loss:1.797058
[Test] epoch:12	batch id:90	 loss:1.629943
[Test] epoch:12	batch id:100	 loss:2.120261
[Test] epoch:12	batch id:110	 loss:1.515383
[Test] epoch:12	batch id:120	 loss:1.754474
[Test] epoch:12	batch id:130	 loss:1.691209
[Test] epoch:12	batch id:140	 loss:1.604737
[Test] epoch:12	batch id:150	 loss:2.094799
[Test] 12, loss: 1.762030, test acc: 0.808347,
Max Acc:0.808347
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:13	batch id:0	 lr:0.099556 loss:1.786213
[Train] epoch:13	batch id:10	 lr:0.099556 loss:2.040261
[Train] epoch:13	batch id:20	 lr:0.099556 loss:2.042921
[Train] epoch:13	batch id:30	 lr:0.099556 loss:2.269367
[Train] epoch:13	batch id:40	 lr:0.099556 loss:1.911111
[Train] epoch:13	batch id:50	 lr:0.099556 loss:1.824843
[Train] epoch:13	batch id:60	 lr:0.099556 loss:2.070593
[Train] epoch:13	batch id:70	 lr:0.099556 loss:2.006730
[Train] epoch:13	batch id:80	 lr:0.099556 loss:1.907265
[Train] epoch:13	batch id:90	 lr:0.099556 loss:1.885880
[Train] epoch:13	batch id:100	 lr:0.099556 loss:1.984515
[Train] epoch:13	batch id:110	 lr:0.099556 loss:2.016263
[Train] epoch:13	batch id:120	 lr:0.099556 loss:2.303524
[Train] epoch:13	batch id:130	 lr:0.099556 loss:1.890288
[Train] epoch:13	batch id:140	 lr:0.099556 loss:2.114284
[Train] epoch:13	batch id:150	 lr:0.099556 loss:1.910995
[Train] epoch:13	batch id:160	 lr:0.099556 loss:1.845511
[Train] epoch:13	batch id:170	 lr:0.099556 loss:1.851226
[Train] epoch:13	batch id:180	 lr:0.099556 loss:1.879479
[Train] epoch:13	batch id:190	 lr:0.099556 loss:1.948624
[Train] epoch:13	batch id:200	 lr:0.099556 loss:1.911409
[Train] epoch:13	batch id:210	 lr:0.099556 loss:2.044125
[Train] epoch:13	batch id:220	 lr:0.099556 loss:2.174251
[Train] epoch:13	batch id:230	 lr:0.099556 loss:1.716464
[Train] epoch:13	batch id:240	 lr:0.099556 loss:2.156465
[Train] epoch:13	batch id:250	 lr:0.099556 loss:2.011643
[Train] epoch:13	batch id:260	 lr:0.099556 loss:2.228812
[Train] epoch:13	batch id:270	 lr:0.099556 loss:2.196432
[Train] epoch:13	batch id:280	 lr:0.099556 loss:1.767844
[Train] epoch:13	batch id:290	 lr:0.099556 loss:2.062636
[Train] epoch:13	batch id:300	 lr:0.099556 loss:1.824113
[Train] 13, loss: 1.978199, train acc: 0.725366, 
[Test] epoch:13	batch id:0	 loss:1.536009
[Test] epoch:13	batch id:10	 loss:1.649343
[Test] epoch:13	batch id:20	 loss:1.514970
[Test] epoch:13	batch id:30	 loss:1.717293
[Test] epoch:13	batch id:40	 loss:1.737570
[Test] epoch:13	batch id:50	 loss:1.608421
[Test] epoch:13	batch id:60	 loss:1.422713
[Test] epoch:13	batch id:70	 loss:1.683497
[Test] epoch:13	batch id:80	 loss:1.798350
[Test] epoch:13	batch id:90	 loss:1.549328
[Test] epoch:13	batch id:100	 loss:2.129412
[Test] epoch:13	batch id:110	 loss:1.467477
[Test] epoch:13	batch id:120	 loss:1.587397
[Test] epoch:13	batch id:130	 loss:1.675781
[Test] epoch:13	batch id:140	 loss:1.598140
[Test] epoch:13	batch id:150	 loss:1.947587
[Test] 13, loss: 1.726501, test acc: 0.811183,
Max Acc:0.811183
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:14	batch id:0	 lr:0.099494 loss:1.935730
[Train] epoch:14	batch id:10	 lr:0.099494 loss:2.095147
[Train] epoch:14	batch id:20	 lr:0.099494 loss:2.157006
[Train] epoch:14	batch id:30	 lr:0.099494 loss:2.221661
[Train] epoch:14	batch id:40	 lr:0.099494 loss:2.097793
[Train] epoch:14	batch id:50	 lr:0.099494 loss:1.923947
[Train] epoch:14	batch id:60	 lr:0.099494 loss:1.938670
[Train] epoch:14	batch id:70	 lr:0.099494 loss:2.203911
[Train] epoch:14	batch id:80	 lr:0.099494 loss:1.798589
[Train] epoch:14	batch id:90	 lr:0.099494 loss:2.065273
[Train] epoch:14	batch id:100	 lr:0.099494 loss:2.003220
[Train] epoch:14	batch id:110	 lr:0.099494 loss:2.332709
[Train] epoch:14	batch id:120	 lr:0.099494 loss:1.859532
[Train] epoch:14	batch id:130	 lr:0.099494 loss:1.872583
[Train] epoch:14	batch id:140	 lr:0.099494 loss:1.850750
[Train] epoch:14	batch id:150	 lr:0.099494 loss:1.932859
[Train] epoch:14	batch id:160	 lr:0.099494 loss:1.699310
[Train] epoch:14	batch id:170	 lr:0.099494 loss:1.680207
[Train] epoch:14	batch id:180	 lr:0.099494 loss:1.962351
[Train] epoch:14	batch id:190	 lr:0.099494 loss:1.940059
[Train] epoch:14	batch id:200	 lr:0.099494 loss:1.888160
[Train] epoch:14	batch id:210	 lr:0.099494 loss:2.045986
[Train] epoch:14	batch id:220	 lr:0.099494 loss:1.973112
[Train] epoch:14	batch id:230	 lr:0.099494 loss:1.909126
[Train] epoch:14	batch id:240	 lr:0.099494 loss:2.189516
[Train] epoch:14	batch id:250	 lr:0.099494 loss:1.812374
[Train] epoch:14	batch id:260	 lr:0.099494 loss:1.815509
[Train] epoch:14	batch id:270	 lr:0.099494 loss:2.118149
[Train] epoch:14	batch id:280	 lr:0.099494 loss:2.083988
[Train] epoch:14	batch id:290	 lr:0.099494 loss:1.700163
[Train] epoch:14	batch id:300	 lr:0.099494 loss:1.874383
[Train] 14, loss: 1.964409, train acc: 0.729947, 
[Test] epoch:14	batch id:0	 loss:1.491849
[Test] epoch:14	batch id:10	 loss:1.732644
[Test] epoch:14	batch id:20	 loss:1.593069
[Test] epoch:14	batch id:30	 loss:1.619731
[Test] epoch:14	batch id:40	 loss:1.674298
[Test] epoch:14	batch id:50	 loss:1.728588
[Test] epoch:14	batch id:60	 loss:1.452445
[Test] epoch:14	batch id:70	 loss:1.565606
[Test] epoch:14	batch id:80	 loss:1.733928
[Test] epoch:14	batch id:90	 loss:1.561524
[Test] epoch:14	batch id:100	 loss:2.247972
[Test] epoch:14	batch id:110	 loss:1.497525
[Test] epoch:14	batch id:120	 loss:1.603388
[Test] epoch:14	batch id:130	 loss:1.608204
[Test] epoch:14	batch id:140	 loss:1.597360
[Test] epoch:14	batch id:150	 loss:1.901660
[Test] 14, loss: 1.741113, test acc: 0.810778,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:15	batch id:0	 lr:0.099429 loss:1.798582
[Train] epoch:15	batch id:10	 lr:0.099429 loss:1.728624
[Train] epoch:15	batch id:20	 lr:0.099429 loss:1.937065
[Train] epoch:15	batch id:30	 lr:0.099429 loss:2.103477
[Train] epoch:15	batch id:40	 lr:0.099429 loss:2.047522
[Train] epoch:15	batch id:50	 lr:0.099429 loss:2.084746
[Train] epoch:15	batch id:60	 lr:0.099429 loss:2.100135
[Train] epoch:15	batch id:70	 lr:0.099429 loss:1.825363
[Train] epoch:15	batch id:80	 lr:0.099429 loss:2.307283
[Train] epoch:15	batch id:90	 lr:0.099429 loss:1.733525
[Train] epoch:15	batch id:100	 lr:0.099429 loss:2.039278
[Train] epoch:15	batch id:110	 lr:0.099429 loss:2.045904
[Train] epoch:15	batch id:120	 lr:0.099429 loss:1.859818
[Train] epoch:15	batch id:130	 lr:0.099429 loss:2.008461
[Train] epoch:15	batch id:140	 lr:0.099429 loss:1.949499
[Train] epoch:15	batch id:150	 lr:0.099429 loss:2.066549
[Train] epoch:15	batch id:160	 lr:0.099429 loss:1.877625
[Train] epoch:15	batch id:170	 lr:0.099429 loss:2.050860
[Train] epoch:15	batch id:180	 lr:0.099429 loss:1.688964
[Train] epoch:15	batch id:190	 lr:0.099429 loss:1.944663
[Train] epoch:15	batch id:200	 lr:0.099429 loss:2.040040
[Train] epoch:15	batch id:210	 lr:0.099429 loss:2.227947
[Train] epoch:15	batch id:220	 lr:0.099429 loss:1.840282
[Train] epoch:15	batch id:230	 lr:0.099429 loss:1.861490
[Train] epoch:15	batch id:240	 lr:0.099429 loss:2.052652
[Train] epoch:15	batch id:250	 lr:0.099429 loss:1.929101
[Train] epoch:15	batch id:260	 lr:0.099429 loss:1.926427
[Train] epoch:15	batch id:270	 lr:0.099429 loss:1.715879
[Train] epoch:15	batch id:280	 lr:0.099429 loss:2.034723
[Train] epoch:15	batch id:290	 lr:0.099429 loss:2.099956
[Train] epoch:15	batch id:300	 lr:0.099429 loss:2.074284
[Train] 15, loss: 1.946375, train acc: 0.736156, 
[Test] epoch:15	batch id:0	 loss:1.563643
[Test] epoch:15	batch id:10	 loss:1.831518
[Test] epoch:15	batch id:20	 loss:1.549876
[Test] epoch:15	batch id:30	 loss:1.812198
[Test] epoch:15	batch id:40	 loss:1.791897
[Test] epoch:15	batch id:50	 loss:1.777300
[Test] epoch:15	batch id:60	 loss:1.547844
[Test] epoch:15	batch id:70	 loss:1.748020
[Test] epoch:15	batch id:80	 loss:1.827572
[Test] epoch:15	batch id:90	 loss:1.591224
[Test] epoch:15	batch id:100	 loss:2.263053
[Test] epoch:15	batch id:110	 loss:1.565976
[Test] epoch:15	batch id:120	 loss:1.716709
[Test] epoch:15	batch id:130	 loss:1.616120
[Test] epoch:15	batch id:140	 loss:1.581724
[Test] epoch:15	batch id:150	 loss:2.096530
[Test] 15, loss: 1.765244, test acc: 0.803890,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:16	batch id:0	 lr:0.099359 loss:1.788700
[Train] epoch:16	batch id:10	 lr:0.099359 loss:1.785482
[Train] epoch:16	batch id:20	 lr:0.099359 loss:1.857225
[Train] epoch:16	batch id:30	 lr:0.099359 loss:1.839938
[Train] epoch:16	batch id:40	 lr:0.099359 loss:1.874371
[Train] epoch:16	batch id:50	 lr:0.099359 loss:1.705188
[Train] epoch:16	batch id:60	 lr:0.099359 loss:1.896877
[Train] epoch:16	batch id:70	 lr:0.099359 loss:1.648005
[Train] epoch:16	batch id:80	 lr:0.099359 loss:2.010166
[Train] epoch:16	batch id:90	 lr:0.099359 loss:1.895914
[Train] epoch:16	batch id:100	 lr:0.099359 loss:1.906380
[Train] epoch:16	batch id:110	 lr:0.099359 loss:2.097424
[Train] epoch:16	batch id:120	 lr:0.099359 loss:1.865081
[Train] epoch:16	batch id:130	 lr:0.099359 loss:1.939282
[Train] epoch:16	batch id:140	 lr:0.099359 loss:1.827140
[Train] epoch:16	batch id:150	 lr:0.099359 loss:2.418590
[Train] epoch:16	batch id:160	 lr:0.099359 loss:1.859577
[Train] epoch:16	batch id:170	 lr:0.099359 loss:1.748051
[Train] epoch:16	batch id:180	 lr:0.099359 loss:1.785363
[Train] epoch:16	batch id:190	 lr:0.099359 loss:1.860980
[Train] epoch:16	batch id:200	 lr:0.099359 loss:1.863953
[Train] epoch:16	batch id:210	 lr:0.099359 loss:2.176301
[Train] epoch:16	batch id:220	 lr:0.099359 loss:2.036154
[Train] epoch:16	batch id:230	 lr:0.099359 loss:1.985654
[Train] epoch:16	batch id:240	 lr:0.099359 loss:2.013955
[Train] epoch:16	batch id:250	 lr:0.099359 loss:1.914782
[Train] epoch:16	batch id:260	 lr:0.099359 loss:2.172522
[Train] epoch:16	batch id:270	 lr:0.099359 loss:1.859642
[Train] epoch:16	batch id:280	 lr:0.099359 loss:1.912057
[Train] epoch:16	batch id:290	 lr:0.099359 loss:1.946033
[Train] epoch:16	batch id:300	 lr:0.099359 loss:1.866581
[Train] 16, loss: 1.939945, train acc: 0.742264, 
[Test] epoch:16	batch id:0	 loss:1.532850
[Test] epoch:16	batch id:10	 loss:1.858657
[Test] epoch:16	batch id:20	 loss:1.596997
[Test] epoch:16	batch id:30	 loss:1.778036
[Test] epoch:16	batch id:40	 loss:1.738875
[Test] epoch:16	batch id:50	 loss:1.842611
[Test] epoch:16	batch id:60	 loss:1.424102
[Test] epoch:16	batch id:70	 loss:1.736423
[Test] epoch:16	batch id:80	 loss:1.859550
[Test] epoch:16	batch id:90	 loss:1.587911
[Test] epoch:16	batch id:100	 loss:2.367768
[Test] epoch:16	batch id:110	 loss:1.578863
[Test] epoch:16	batch id:120	 loss:1.715949
[Test] epoch:16	batch id:130	 loss:1.784141
[Test] epoch:16	batch id:140	 loss:1.498534
[Test] epoch:16	batch id:150	 loss:2.076752
[Test] 16, loss: 1.753359, test acc: 0.817666,
Max Acc:0.817666
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:17	batch id:0	 lr:0.099286 loss:1.830444
[Train] epoch:17	batch id:10	 lr:0.099286 loss:1.940860
[Train] epoch:17	batch id:20	 lr:0.099286 loss:2.097445
[Train] epoch:17	batch id:30	 lr:0.099286 loss:1.707501
[Train] epoch:17	batch id:40	 lr:0.099286 loss:2.019971
[Train] epoch:17	batch id:50	 lr:0.099286 loss:2.028553
[Train] epoch:17	batch id:60	 lr:0.099286 loss:2.164042
[Train] epoch:17	batch id:70	 lr:0.099286 loss:1.582845
[Train] epoch:17	batch id:80	 lr:0.099286 loss:1.776804
[Train] epoch:17	batch id:90	 lr:0.099286 loss:1.894089
[Train] epoch:17	batch id:100	 lr:0.099286 loss:1.962848
[Train] epoch:17	batch id:110	 lr:0.099286 loss:1.871512
[Train] epoch:17	batch id:120	 lr:0.099286 loss:1.876088
[Train] epoch:17	batch id:130	 lr:0.099286 loss:2.179406
[Train] epoch:17	batch id:140	 lr:0.099286 loss:1.932661
[Train] epoch:17	batch id:150	 lr:0.099286 loss:1.792594
[Train] epoch:17	batch id:160	 lr:0.099286 loss:2.136116
[Train] epoch:17	batch id:170	 lr:0.099286 loss:1.817188
[Train] epoch:17	batch id:180	 lr:0.099286 loss:2.026610
[Train] epoch:17	batch id:190	 lr:0.099286 loss:1.939294
[Train] epoch:17	batch id:200	 lr:0.099286 loss:2.042614
[Train] epoch:17	batch id:210	 lr:0.099286 loss:1.879541
[Train] epoch:17	batch id:220	 lr:0.099286 loss:2.065103
[Train] epoch:17	batch id:230	 lr:0.099286 loss:1.779390
[Train] epoch:17	batch id:240	 lr:0.099286 loss:1.709530
[Train] epoch:17	batch id:250	 lr:0.099286 loss:1.850382
[Train] epoch:17	batch id:260	 lr:0.099286 loss:2.056412
[Train] epoch:17	batch id:270	 lr:0.099286 loss:1.922340
[Train] epoch:17	batch id:280	 lr:0.099286 loss:2.011826
[Train] epoch:17	batch id:290	 lr:0.099286 loss:1.851632
[Train] epoch:17	batch id:300	 lr:0.099286 loss:2.014445
[Train] 17, loss: 1.920850, train acc: 0.749593, 
[Test] epoch:17	batch id:0	 loss:1.552330
[Test] epoch:17	batch id:10	 loss:1.807358
[Test] epoch:17	batch id:20	 loss:1.560158
[Test] epoch:17	batch id:30	 loss:1.729109
[Test] epoch:17	batch id:40	 loss:1.693210
[Test] epoch:17	batch id:50	 loss:1.533430
[Test] epoch:17	batch id:60	 loss:1.402518
[Test] epoch:17	batch id:70	 loss:1.567410
[Test] epoch:17	batch id:80	 loss:1.808591
[Test] epoch:17	batch id:90	 loss:1.533338
[Test] epoch:17	batch id:100	 loss:2.070481
[Test] epoch:17	batch id:110	 loss:1.473206
[Test] epoch:17	batch id:120	 loss:1.663303
[Test] epoch:17	batch id:130	 loss:1.589438
[Test] epoch:17	batch id:140	 loss:1.527071
[Test] epoch:17	batch id:150	 loss:1.924121
[Test] 17, loss: 1.678953, test acc: 0.849271,
Max Acc:0.849271
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:18	batch id:0	 lr:0.099208 loss:2.082561
[Train] epoch:18	batch id:10	 lr:0.099208 loss:1.816152
[Train] epoch:18	batch id:20	 lr:0.099208 loss:1.965328
[Train] epoch:18	batch id:30	 lr:0.099208 loss:1.921062
[Train] epoch:18	batch id:40	 lr:0.099208 loss:1.740748
[Train] epoch:18	batch id:50	 lr:0.099208 loss:1.962425
[Train] epoch:18	batch id:60	 lr:0.099208 loss:1.698888
[Train] epoch:18	batch id:70	 lr:0.099208 loss:1.818820
[Train] epoch:18	batch id:80	 lr:0.099208 loss:1.807046
[Train] epoch:18	batch id:90	 lr:0.099208 loss:1.855703
[Train] epoch:18	batch id:100	 lr:0.099208 loss:1.835553
[Train] epoch:18	batch id:110	 lr:0.099208 loss:1.923013
[Train] epoch:18	batch id:120	 lr:0.099208 loss:1.689950
[Train] epoch:18	batch id:130	 lr:0.099208 loss:1.827039
[Train] epoch:18	batch id:140	 lr:0.099208 loss:1.867732
[Train] epoch:18	batch id:150	 lr:0.099208 loss:1.916164
[Train] epoch:18	batch id:160	 lr:0.099208 loss:1.892483
[Train] epoch:18	batch id:170	 lr:0.099208 loss:1.852844
[Train] epoch:18	batch id:180	 lr:0.099208 loss:1.905359
[Train] epoch:18	batch id:190	 lr:0.099208 loss:1.891750
[Train] epoch:18	batch id:200	 lr:0.099208 loss:1.876045
[Train] epoch:18	batch id:210	 lr:0.099208 loss:1.886393
[Train] epoch:18	batch id:220	 lr:0.099208 loss:1.919547
[Train] epoch:18	batch id:230	 lr:0.099208 loss:1.842374
[Train] epoch:18	batch id:240	 lr:0.099208 loss:1.942529
[Train] epoch:18	batch id:250	 lr:0.099208 loss:1.645903
[Train] epoch:18	batch id:260	 lr:0.099208 loss:1.834819
[Train] epoch:18	batch id:270	 lr:0.099208 loss:1.948195
[Train] epoch:18	batch id:280	 lr:0.099208 loss:1.767216
[Train] epoch:18	batch id:290	 lr:0.099208 loss:1.771003
[Train] epoch:18	batch id:300	 lr:0.099208 loss:1.690316
[Train] 18, loss: 1.905530, train acc: 0.756820, 
[Test] epoch:18	batch id:0	 loss:1.684527
[Test] epoch:18	batch id:10	 loss:1.804077
[Test] epoch:18	batch id:20	 loss:1.487628
[Test] epoch:18	batch id:30	 loss:1.763048
[Test] epoch:18	batch id:40	 loss:1.863409
[Test] epoch:18	batch id:50	 loss:1.855596
[Test] epoch:18	batch id:60	 loss:1.458319
[Test] epoch:18	batch id:70	 loss:1.685205
[Test] epoch:18	batch id:80	 loss:1.961671
[Test] epoch:18	batch id:90	 loss:1.546337
[Test] epoch:18	batch id:100	 loss:2.213028
[Test] epoch:18	batch id:110	 loss:1.538869
[Test] epoch:18	batch id:120	 loss:1.751633
[Test] epoch:18	batch id:130	 loss:1.516511
[Test] epoch:18	batch id:140	 loss:1.526389
[Test] epoch:18	batch id:150	 loss:2.111155
[Test] 18, loss: 1.753895, test acc: 0.807942,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:19	batch id:0	 lr:0.099127 loss:1.987148
[Train] epoch:19	batch id:10	 lr:0.099127 loss:1.953354
[Train] epoch:19	batch id:20	 lr:0.099127 loss:1.955875
[Train] epoch:19	batch id:30	 lr:0.099127 loss:1.980991
[Train] epoch:19	batch id:40	 lr:0.099127 loss:1.887247
[Train] epoch:19	batch id:50	 lr:0.099127 loss:1.893148
[Train] epoch:19	batch id:60	 lr:0.099127 loss:1.987219
[Train] epoch:19	batch id:70	 lr:0.099127 loss:1.822853
[Train] epoch:19	batch id:80	 lr:0.099127 loss:1.947382
[Train] epoch:19	batch id:90	 lr:0.099127 loss:1.695475
[Train] epoch:19	batch id:100	 lr:0.099127 loss:1.948227
[Train] epoch:19	batch id:110	 lr:0.099127 loss:1.984068
[Train] epoch:19	batch id:120	 lr:0.099127 loss:1.935264
[Train] epoch:19	batch id:130	 lr:0.099127 loss:1.929385
[Train] epoch:19	batch id:140	 lr:0.099127 loss:1.864652
[Train] epoch:19	batch id:150	 lr:0.099127 loss:2.126498
[Train] epoch:19	batch id:160	 lr:0.099127 loss:1.997146
[Train] epoch:19	batch id:170	 lr:0.099127 loss:1.941377
[Train] epoch:19	batch id:180	 lr:0.099127 loss:2.023999
[Train] epoch:19	batch id:190	 lr:0.099127 loss:1.954633
[Train] epoch:19	batch id:200	 lr:0.099127 loss:1.754514
[Train] epoch:19	batch id:210	 lr:0.099127 loss:1.961718
[Train] epoch:19	batch id:220	 lr:0.099127 loss:1.948241
[Train] epoch:19	batch id:230	 lr:0.099127 loss:1.772159
[Train] epoch:19	batch id:240	 lr:0.099127 loss:1.886516
[Train] epoch:19	batch id:250	 lr:0.099127 loss:1.751250
[Train] epoch:19	batch id:260	 lr:0.099127 loss:1.925546
[Train] epoch:19	batch id:270	 lr:0.099127 loss:2.055868
[Train] epoch:19	batch id:280	 lr:0.099127 loss:1.686149
[Train] epoch:19	batch id:290	 lr:0.099127 loss:2.186739
[Train] epoch:19	batch id:300	 lr:0.099127 loss:2.099779
[Train] 19, loss: 1.899079, train acc: 0.760586, 
[Test] epoch:19	batch id:0	 loss:1.468297
[Test] epoch:19	batch id:10	 loss:1.771431
[Test] epoch:19	batch id:20	 loss:1.553261
[Test] epoch:19	batch id:30	 loss:1.658509
[Test] epoch:19	batch id:40	 loss:1.634002
[Test] epoch:19	batch id:50	 loss:1.775735
[Test] epoch:19	batch id:60	 loss:1.404816
[Test] epoch:19	batch id:70	 loss:1.588157
[Test] epoch:19	batch id:80	 loss:1.818321
[Test] epoch:19	batch id:90	 loss:1.567160
[Test] epoch:19	batch id:100	 loss:2.274518
[Test] epoch:19	batch id:110	 loss:1.450549
[Test] epoch:19	batch id:120	 loss:1.713646
[Test] epoch:19	batch id:130	 loss:1.578074
[Test] epoch:19	batch id:140	 loss:1.517210
[Test] epoch:19	batch id:150	 loss:2.081005
[Test] 19, loss: 1.691464, test acc: 0.832253,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:20	batch id:0	 lr:0.099042 loss:1.798079
[Train] epoch:20	batch id:10	 lr:0.099042 loss:1.765032
[Train] epoch:20	batch id:20	 lr:0.099042 loss:1.886355
[Train] epoch:20	batch id:30	 lr:0.099042 loss:2.109185
[Train] epoch:20	batch id:40	 lr:0.099042 loss:1.972744
[Train] epoch:20	batch id:50	 lr:0.099042 loss:1.718704
[Train] epoch:20	batch id:60	 lr:0.099042 loss:1.908095
[Train] epoch:20	batch id:70	 lr:0.099042 loss:2.020955
[Train] epoch:20	batch id:80	 lr:0.099042 loss:1.912846
[Train] epoch:20	batch id:90	 lr:0.099042 loss:1.805311
[Train] epoch:20	batch id:100	 lr:0.099042 loss:2.006160
[Train] epoch:20	batch id:110	 lr:0.099042 loss:2.102967
[Train] epoch:20	batch id:120	 lr:0.099042 loss:1.983531
[Train] epoch:20	batch id:130	 lr:0.099042 loss:1.913182
[Train] epoch:20	batch id:140	 lr:0.099042 loss:1.944676
[Train] epoch:20	batch id:150	 lr:0.099042 loss:1.742842
[Train] epoch:20	batch id:160	 lr:0.099042 loss:2.024690
[Train] epoch:20	batch id:170	 lr:0.099042 loss:1.862223
[Train] epoch:20	batch id:180	 lr:0.099042 loss:1.787762
[Train] epoch:20	batch id:190	 lr:0.099042 loss:1.853708
[Train] epoch:20	batch id:200	 lr:0.099042 loss:1.855823
[Train] epoch:20	batch id:210	 lr:0.099042 loss:1.943594
[Train] epoch:20	batch id:220	 lr:0.099042 loss:1.920060
[Train] epoch:20	batch id:230	 lr:0.099042 loss:1.821196
[Train] epoch:20	batch id:240	 lr:0.099042 loss:1.964925
[Train] epoch:20	batch id:250	 lr:0.099042 loss:2.051693
[Train] epoch:20	batch id:260	 lr:0.099042 loss:1.879578
[Train] epoch:20	batch id:270	 lr:0.099042 loss:1.943176
[Train] epoch:20	batch id:280	 lr:0.099042 loss:2.002607
[Train] epoch:20	batch id:290	 lr:0.099042 loss:1.918596
[Train] epoch:20	batch id:300	 lr:0.099042 loss:1.919065
[Train] 20, loss: 1.888610, train acc: 0.764760, 
[Test] epoch:20	batch id:0	 loss:1.529575
[Test] epoch:20	batch id:10	 loss:1.647937
[Test] epoch:20	batch id:20	 loss:1.502612
[Test] epoch:20	batch id:30	 loss:1.645547
[Test] epoch:20	batch id:40	 loss:1.555107
[Test] epoch:20	batch id:50	 loss:1.617655
[Test] epoch:20	batch id:60	 loss:1.339015
[Test] epoch:20	batch id:70	 loss:1.509575
[Test] epoch:20	batch id:80	 loss:1.633681
[Test] epoch:20	batch id:90	 loss:1.526178
[Test] epoch:20	batch id:100	 loss:2.031304
[Test] epoch:20	batch id:110	 loss:1.529150
[Test] epoch:20	batch id:120	 loss:1.632050
[Test] epoch:20	batch id:130	 loss:1.628954
[Test] epoch:20	batch id:140	 loss:1.476223
[Test] epoch:20	batch id:150	 loss:2.119483
[Test] 20, loss: 1.656243, test acc: 0.846029,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:21	batch id:0	 lr:0.098953 loss:1.712282
[Train] epoch:21	batch id:10	 lr:0.098953 loss:2.032951
[Train] epoch:21	batch id:20	 lr:0.098953 loss:1.693023
[Train] epoch:21	batch id:30	 lr:0.098953 loss:1.856399
[Train] epoch:21	batch id:40	 lr:0.098953 loss:1.887506
[Train] epoch:21	batch id:50	 lr:0.098953 loss:1.982145
[Train] epoch:21	batch id:60	 lr:0.098953 loss:2.066108
[Train] epoch:21	batch id:70	 lr:0.098953 loss:1.902877
[Train] epoch:21	batch id:80	 lr:0.098953 loss:1.968015
[Train] epoch:21	batch id:90	 lr:0.098953 loss:1.659827
[Train] epoch:21	batch id:100	 lr:0.098953 loss:2.068904
[Train] epoch:21	batch id:110	 lr:0.098953 loss:1.850105
[Train] epoch:21	batch id:120	 lr:0.098953 loss:1.811522
[Train] epoch:21	batch id:130	 lr:0.098953 loss:1.845247
[Train] epoch:21	batch id:140	 lr:0.098953 loss:1.790420
[Train] epoch:21	batch id:150	 lr:0.098953 loss:1.880844
[Train] epoch:21	batch id:160	 lr:0.098953 loss:1.752722
[Train] epoch:21	batch id:170	 lr:0.098953 loss:1.704969
[Train] epoch:21	batch id:180	 lr:0.098953 loss:1.842356
[Train] epoch:21	batch id:190	 lr:0.098953 loss:1.929867
[Train] epoch:21	batch id:200	 lr:0.098953 loss:1.774556
[Train] epoch:21	batch id:210	 lr:0.098953 loss:1.733419
[Train] epoch:21	batch id:220	 lr:0.098953 loss:2.155127
[Train] epoch:21	batch id:230	 lr:0.098953 loss:1.851637
[Train] epoch:21	batch id:240	 lr:0.098953 loss:1.907627
[Train] epoch:21	batch id:250	 lr:0.098953 loss:1.790196
[Train] epoch:21	batch id:260	 lr:0.098953 loss:2.112151
[Train] epoch:21	batch id:270	 lr:0.098953 loss:1.917737
[Train] epoch:21	batch id:280	 lr:0.098953 loss:1.995528
[Train] epoch:21	batch id:290	 lr:0.098953 loss:1.920364
[Train] epoch:21	batch id:300	 lr:0.098953 loss:1.682514
[Train] 21, loss: 1.884742, train acc: 0.766083, 
[Test] epoch:21	batch id:0	 loss:1.421583
[Test] epoch:21	batch id:10	 loss:1.735829
[Test] epoch:21	batch id:20	 loss:1.497187
[Test] epoch:21	batch id:30	 loss:1.681786
[Test] epoch:21	batch id:40	 loss:1.765618
[Test] epoch:21	batch id:50	 loss:1.618289
[Test] epoch:21	batch id:60	 loss:1.415921
[Test] epoch:21	batch id:70	 loss:1.623262
[Test] epoch:21	batch id:80	 loss:1.665141
[Test] epoch:21	batch id:90	 loss:1.542603
[Test] epoch:21	batch id:100	 loss:2.043985
[Test] epoch:21	batch id:110	 loss:1.534181
[Test] epoch:21	batch id:120	 loss:1.531418
[Test] epoch:21	batch id:130	 loss:1.617366
[Test] epoch:21	batch id:140	 loss:1.488269
[Test] epoch:21	batch id:150	 loss:1.933297
[Test] 21, loss: 1.669058, test acc: 0.843598,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:22	batch id:0	 lr:0.098860 loss:2.065962
[Train] epoch:22	batch id:10	 lr:0.098860 loss:1.931826
[Train] epoch:22	batch id:20	 lr:0.098860 loss:1.901438
[Train] epoch:22	batch id:30	 lr:0.098860 loss:2.030279
[Train] epoch:22	batch id:40	 lr:0.098860 loss:1.865296
[Train] epoch:22	batch id:50	 lr:0.098860 loss:1.728379
[Train] epoch:22	batch id:60	 lr:0.098860 loss:1.942905
[Train] epoch:22	batch id:70	 lr:0.098860 loss:1.674945
[Train] epoch:22	batch id:80	 lr:0.098860 loss:1.707099
[Train] epoch:22	batch id:90	 lr:0.098860 loss:1.799284
[Train] epoch:22	batch id:100	 lr:0.098860 loss:1.950650
[Train] epoch:22	batch id:110	 lr:0.098860 loss:1.773930
[Train] epoch:22	batch id:120	 lr:0.098860 loss:1.971378
[Train] epoch:22	batch id:130	 lr:0.098860 loss:1.742682
[Train] epoch:22	batch id:140	 lr:0.098860 loss:1.706684
[Train] epoch:22	batch id:150	 lr:0.098860 loss:1.695439
[Train] epoch:22	batch id:160	 lr:0.098860 loss:1.992938
[Train] epoch:22	batch id:170	 lr:0.098860 loss:1.902652
[Train] epoch:22	batch id:180	 lr:0.098860 loss:1.944865
[Train] epoch:22	batch id:190	 lr:0.098860 loss:2.014387
[Train] epoch:22	batch id:200	 lr:0.098860 loss:1.681682
[Train] epoch:22	batch id:210	 lr:0.098860 loss:1.849990
[Train] epoch:22	batch id:220	 lr:0.098860 loss:1.875555
[Train] epoch:22	batch id:230	 lr:0.098860 loss:2.023487
[Train] epoch:22	batch id:240	 lr:0.098860 loss:1.765548
[Train] epoch:22	batch id:250	 lr:0.098860 loss:1.633842
[Train] epoch:22	batch id:260	 lr:0.098860 loss:2.053536
[Train] epoch:22	batch id:270	 lr:0.098860 loss:1.813177
[Train] epoch:22	batch id:280	 lr:0.098860 loss:2.125442
[Train] epoch:22	batch id:290	 lr:0.098860 loss:1.807251
[Train] epoch:22	batch id:300	 lr:0.098860 loss:1.878795
[Train] 22, loss: 1.868894, train acc: 0.770155, 
[Test] epoch:22	batch id:0	 loss:1.463815
[Test] epoch:22	batch id:10	 loss:1.761709
[Test] epoch:22	batch id:20	 loss:1.615812
[Test] epoch:22	batch id:30	 loss:1.561689
[Test] epoch:22	batch id:40	 loss:1.704810
[Test] epoch:22	batch id:50	 loss:1.691425
[Test] epoch:22	batch id:60	 loss:1.428262
[Test] epoch:22	batch id:70	 loss:1.547701
[Test] epoch:22	batch id:80	 loss:1.687455
[Test] epoch:22	batch id:90	 loss:1.535255
[Test] epoch:22	batch id:100	 loss:2.182930
[Test] epoch:22	batch id:110	 loss:1.489046
[Test] epoch:22	batch id:120	 loss:1.562188
[Test] epoch:22	batch id:130	 loss:1.587749
[Test] epoch:22	batch id:140	 loss:1.516979
[Test] epoch:22	batch id:150	 loss:1.951272
[Test] 22, loss: 1.676611, test acc: 0.847650,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:23	batch id:0	 lr:0.098763 loss:1.841515
[Train] epoch:23	batch id:10	 lr:0.098763 loss:1.826816
[Train] epoch:23	batch id:20	 lr:0.098763 loss:1.804945
[Train] epoch:23	batch id:30	 lr:0.098763 loss:1.743790
[Train] epoch:23	batch id:40	 lr:0.098763 loss:2.058962
[Train] epoch:23	batch id:50	 lr:0.098763 loss:1.644318
[Train] epoch:23	batch id:60	 lr:0.098763 loss:2.272273
[Train] epoch:23	batch id:70	 lr:0.098763 loss:2.017034
[Train] epoch:23	batch id:80	 lr:0.098763 loss:2.094454
[Train] epoch:23	batch id:90	 lr:0.098763 loss:1.810003
[Train] epoch:23	batch id:100	 lr:0.098763 loss:1.962274
[Train] epoch:23	batch id:110	 lr:0.098763 loss:1.896912
[Train] epoch:23	batch id:120	 lr:0.098763 loss:1.735156
[Train] epoch:23	batch id:130	 lr:0.098763 loss:1.963245
[Train] epoch:23	batch id:140	 lr:0.098763 loss:1.617273
[Train] epoch:23	batch id:150	 lr:0.098763 loss:1.990722
[Train] epoch:23	batch id:160	 lr:0.098763 loss:1.778291
[Train] epoch:23	batch id:170	 lr:0.098763 loss:1.749929
[Train] epoch:23	batch id:180	 lr:0.098763 loss:1.945839
[Train] epoch:23	batch id:190	 lr:0.098763 loss:1.638168
[Train] epoch:23	batch id:200	 lr:0.098763 loss:1.749452
[Train] epoch:23	batch id:210	 lr:0.098763 loss:1.822106
[Train] epoch:23	batch id:220	 lr:0.098763 loss:1.844517
[Train] epoch:23	batch id:230	 lr:0.098763 loss:1.980558
[Train] epoch:23	batch id:240	 lr:0.098763 loss:1.753886
[Train] epoch:23	batch id:250	 lr:0.098763 loss:1.836136
[Train] epoch:23	batch id:260	 lr:0.098763 loss:1.820440
[Train] epoch:23	batch id:270	 lr:0.098763 loss:2.123341
[Train] epoch:23	batch id:280	 lr:0.098763 loss:1.875568
[Train] epoch:23	batch id:290	 lr:0.098763 loss:1.860057
[Train] epoch:23	batch id:300	 lr:0.098763 loss:2.160400
[Train] 23, loss: 1.855923, train acc: 0.779418, 
[Test] epoch:23	batch id:0	 loss:1.462515
[Test] epoch:23	batch id:10	 loss:1.757496
[Test] epoch:23	batch id:20	 loss:1.536411
[Test] epoch:23	batch id:30	 loss:1.670720
[Test] epoch:23	batch id:40	 loss:1.652169
[Test] epoch:23	batch id:50	 loss:1.749249
[Test] epoch:23	batch id:60	 loss:1.425528
[Test] epoch:23	batch id:70	 loss:1.677074
[Test] epoch:23	batch id:80	 loss:1.746345
[Test] epoch:23	batch id:90	 loss:1.593494
[Test] epoch:23	batch id:100	 loss:2.472984
[Test] epoch:23	batch id:110	 loss:1.550707
[Test] epoch:23	batch id:120	 loss:1.725578
[Test] epoch:23	batch id:130	 loss:1.545178
[Test] epoch:23	batch id:140	 loss:1.500454
[Test] epoch:23	batch id:150	 loss:1.981886
[Test] 23, loss: 1.734023, test acc: 0.816045,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:24	batch id:0	 lr:0.098662 loss:1.971359
[Train] epoch:24	batch id:10	 lr:0.098662 loss:1.991781
[Train] epoch:24	batch id:20	 lr:0.098662 loss:1.742080
[Train] epoch:24	batch id:30	 lr:0.098662 loss:1.749219
[Train] epoch:24	batch id:40	 lr:0.098662 loss:2.004753
[Train] epoch:24	batch id:50	 lr:0.098662 loss:1.847069
[Train] epoch:24	batch id:60	 lr:0.098662 loss:1.901821
[Train] epoch:24	batch id:70	 lr:0.098662 loss:1.866424
[Train] epoch:24	batch id:80	 lr:0.098662 loss:1.821171
[Train] epoch:24	batch id:90	 lr:0.098662 loss:1.776579
[Train] epoch:24	batch id:100	 lr:0.098662 loss:1.875907
[Train] epoch:24	batch id:110	 lr:0.098662 loss:2.094122
[Train] epoch:24	batch id:120	 lr:0.098662 loss:1.977281
[Train] epoch:24	batch id:130	 lr:0.098662 loss:1.968781
[Train] epoch:24	batch id:140	 lr:0.098662 loss:1.920292
[Train] epoch:24	batch id:150	 lr:0.098662 loss:1.958267
[Train] epoch:24	batch id:160	 lr:0.098662 loss:1.918251
[Train] epoch:24	batch id:170	 lr:0.098662 loss:2.057309
[Train] epoch:24	batch id:180	 lr:0.098662 loss:1.745201
[Train] epoch:24	batch id:190	 lr:0.098662 loss:1.914771
[Train] epoch:24	batch id:200	 lr:0.098662 loss:1.488440
[Train] epoch:24	batch id:210	 lr:0.098662 loss:1.747763
[Train] epoch:24	batch id:220	 lr:0.098662 loss:2.011638
[Train] epoch:24	batch id:230	 lr:0.098662 loss:1.814081
[Train] epoch:24	batch id:240	 lr:0.098662 loss:2.305629
[Train] epoch:24	batch id:250	 lr:0.098662 loss:1.781451
[Train] epoch:24	batch id:260	 lr:0.098662 loss:1.807835
[Train] epoch:24	batch id:270	 lr:0.098662 loss:1.884405
[Train] epoch:24	batch id:280	 lr:0.098662 loss:1.835032
[Train] epoch:24	batch id:290	 lr:0.098662 loss:1.875376
[Train] epoch:24	batch id:300	 lr:0.098662 loss:1.681686
[Train] 24, loss: 1.853862, train acc: 0.776771, 
[Test] epoch:24	batch id:0	 loss:1.463151
[Test] epoch:24	batch id:10	 loss:1.634568
[Test] epoch:24	batch id:20	 loss:1.550635
[Test] epoch:24	batch id:30	 loss:1.548793
[Test] epoch:24	batch id:40	 loss:1.483010
[Test] epoch:24	batch id:50	 loss:1.537646
[Test] epoch:24	batch id:60	 loss:1.329955
[Test] epoch:24	batch id:70	 loss:1.542805
[Test] epoch:24	batch id:80	 loss:1.743665
[Test] epoch:24	batch id:90	 loss:1.519287
[Test] epoch:24	batch id:100	 loss:2.003124
[Test] epoch:24	batch id:110	 loss:1.483699
[Test] epoch:24	batch id:120	 loss:1.548063
[Test] epoch:24	batch id:130	 loss:1.537675
[Test] epoch:24	batch id:140	 loss:1.564028
[Test] epoch:24	batch id:150	 loss:1.853007
[Test] 24, loss: 1.630158, test acc: 0.867909,
Max Acc:0.867909
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:25	batch id:0	 lr:0.098557 loss:1.810929
[Train] epoch:25	batch id:10	 lr:0.098557 loss:1.794383
[Train] epoch:25	batch id:20	 lr:0.098557 loss:1.719283
[Train] epoch:25	batch id:30	 lr:0.098557 loss:1.966010
[Train] epoch:25	batch id:40	 lr:0.098557 loss:2.110414
[Train] epoch:25	batch id:50	 lr:0.098557 loss:1.827830
[Train] epoch:25	batch id:60	 lr:0.098557 loss:1.811585
[Train] epoch:25	batch id:70	 lr:0.098557 loss:1.776594
[Train] epoch:25	batch id:80	 lr:0.098557 loss:1.864830
[Train] epoch:25	batch id:90	 lr:0.098557 loss:1.974761
[Train] epoch:25	batch id:100	 lr:0.098557 loss:1.653733
[Train] epoch:25	batch id:110	 lr:0.098557 loss:1.666312
[Train] epoch:25	batch id:120	 lr:0.098557 loss:1.778914
[Train] epoch:25	batch id:130	 lr:0.098557 loss:1.778448
[Train] epoch:25	batch id:140	 lr:0.098557 loss:1.902686
[Train] epoch:25	batch id:150	 lr:0.098557 loss:1.931023
[Train] epoch:25	batch id:160	 lr:0.098557 loss:1.486215
[Train] epoch:25	batch id:170	 lr:0.098557 loss:1.721940
[Train] epoch:25	batch id:180	 lr:0.098557 loss:1.920087
[Train] epoch:25	batch id:190	 lr:0.098557 loss:1.726338
[Train] epoch:25	batch id:200	 lr:0.098557 loss:1.941221
[Train] epoch:25	batch id:210	 lr:0.098557 loss:1.933074
[Train] epoch:25	batch id:220	 lr:0.098557 loss:1.901075
[Train] epoch:25	batch id:230	 lr:0.098557 loss:1.769627
[Train] epoch:25	batch id:240	 lr:0.098557 loss:1.582252
[Train] epoch:25	batch id:250	 lr:0.098557 loss:1.764198
[Train] epoch:25	batch id:260	 lr:0.098557 loss:1.917889
[Train] epoch:25	batch id:270	 lr:0.098557 loss:1.889973
[Train] epoch:25	batch id:280	 lr:0.098557 loss:1.811487
[Train] epoch:25	batch id:290	 lr:0.098557 loss:1.830945
[Train] epoch:25	batch id:300	 lr:0.098557 loss:1.868234
[Train] 25, loss: 1.833678, train acc: 0.788986, 
[Test] epoch:25	batch id:0	 loss:1.475359
[Test] epoch:25	batch id:10	 loss:1.557012
[Test] epoch:25	batch id:20	 loss:1.590799
[Test] epoch:25	batch id:30	 loss:1.578562
[Test] epoch:25	batch id:40	 loss:1.524591
[Test] epoch:25	batch id:50	 loss:1.562884
[Test] epoch:25	batch id:60	 loss:1.363458
[Test] epoch:25	batch id:70	 loss:1.588921
[Test] epoch:25	batch id:80	 loss:1.650890
[Test] epoch:25	batch id:90	 loss:1.587497
[Test] epoch:25	batch id:100	 loss:2.036978
[Test] epoch:25	batch id:110	 loss:1.481899
[Test] epoch:25	batch id:120	 loss:1.571801
[Test] epoch:25	batch id:130	 loss:1.530223
[Test] epoch:25	batch id:140	 loss:1.570447
[Test] epoch:25	batch id:150	 loss:2.044272
[Test] 25, loss: 1.630375, test acc: 0.860211,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:26	batch id:0	 lr:0.098449 loss:1.829537
[Train] epoch:26	batch id:10	 lr:0.098449 loss:1.841675
[Train] epoch:26	batch id:20	 lr:0.098449 loss:2.171698
[Train] epoch:26	batch id:30	 lr:0.098449 loss:1.921709
[Train] epoch:26	batch id:40	 lr:0.098449 loss:1.731960
[Train] epoch:26	batch id:50	 lr:0.098449 loss:1.939509
[Train] epoch:26	batch id:60	 lr:0.098449 loss:1.919167
[Train] epoch:26	batch id:70	 lr:0.098449 loss:1.707763
[Train] epoch:26	batch id:80	 lr:0.098449 loss:1.799710
[Train] epoch:26	batch id:90	 lr:0.098449 loss:1.900467
[Train] epoch:26	batch id:100	 lr:0.098449 loss:1.885361
[Train] epoch:26	batch id:110	 lr:0.098449 loss:1.639990
[Train] epoch:26	batch id:120	 lr:0.098449 loss:1.917870
[Train] epoch:26	batch id:130	 lr:0.098449 loss:2.003738
[Train] epoch:26	batch id:140	 lr:0.098449 loss:1.808770
[Train] epoch:26	batch id:150	 lr:0.098449 loss:1.989547
[Train] epoch:26	batch id:160	 lr:0.098449 loss:1.687412
[Train] epoch:26	batch id:170	 lr:0.098449 loss:1.778594
[Train] epoch:26	batch id:180	 lr:0.098449 loss:1.939853
[Train] epoch:26	batch id:190	 lr:0.098449 loss:1.870497
[Train] epoch:26	batch id:200	 lr:0.098449 loss:1.821080
[Train] epoch:26	batch id:210	 lr:0.098449 loss:1.909284
[Train] epoch:26	batch id:220	 lr:0.098449 loss:1.763277
[Train] epoch:26	batch id:230	 lr:0.098449 loss:1.819058
[Train] epoch:26	batch id:240	 lr:0.098449 loss:1.644087
[Train] epoch:26	batch id:250	 lr:0.098449 loss:1.900267
[Train] epoch:26	batch id:260	 lr:0.098449 loss:1.701056
[Train] epoch:26	batch id:270	 lr:0.098449 loss:2.042094
[Train] epoch:26	batch id:280	 lr:0.098449 loss:1.856098
[Train] epoch:26	batch id:290	 lr:0.098449 loss:2.038192
[Train] epoch:26	batch id:300	 lr:0.098449 loss:1.614717
[Train] 26, loss: 1.828594, train acc: 0.787765, 
[Test] epoch:26	batch id:0	 loss:1.411352
[Test] epoch:26	batch id:10	 loss:1.638407
[Test] epoch:26	batch id:20	 loss:1.591701
[Test] epoch:26	batch id:30	 loss:1.554342
[Test] epoch:26	batch id:40	 loss:1.717996
[Test] epoch:26	batch id:50	 loss:1.543908
[Test] epoch:26	batch id:60	 loss:1.373699
[Test] epoch:26	batch id:70	 loss:1.631566
[Test] epoch:26	batch id:80	 loss:1.649082
[Test] epoch:26	batch id:90	 loss:1.549666
[Test] epoch:26	batch id:100	 loss:2.020379
[Test] epoch:26	batch id:110	 loss:1.527640
[Test] epoch:26	batch id:120	 loss:1.727155
[Test] epoch:26	batch id:130	 loss:1.581506
[Test] epoch:26	batch id:140	 loss:1.636961
[Test] epoch:26	batch id:150	 loss:1.968164
[Test] 26, loss: 1.676675, test acc: 0.842382,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:27	batch id:0	 lr:0.098336 loss:1.712499
[Train] epoch:27	batch id:10	 lr:0.098336 loss:1.975566
[Train] epoch:27	batch id:20	 lr:0.098336 loss:1.833034
[Train] epoch:27	batch id:30	 lr:0.098336 loss:1.784904
[Train] epoch:27	batch id:40	 lr:0.098336 loss:1.726762
[Train] epoch:27	batch id:50	 lr:0.098336 loss:2.047082
[Train] epoch:27	batch id:60	 lr:0.098336 loss:1.742064
[Train] epoch:27	batch id:70	 lr:0.098336 loss:1.937453
[Train] epoch:27	batch id:80	 lr:0.098336 loss:2.087226
[Train] epoch:27	batch id:90	 lr:0.098336 loss:1.866057
[Train] epoch:27	batch id:100	 lr:0.098336 loss:1.783949
[Train] epoch:27	batch id:110	 lr:0.098336 loss:1.940480
[Train] epoch:27	batch id:120	 lr:0.098336 loss:1.960381
[Train] epoch:27	batch id:130	 lr:0.098336 loss:1.803082
[Train] epoch:27	batch id:140	 lr:0.098336 loss:1.897203
[Train] epoch:27	batch id:150	 lr:0.098336 loss:1.956770
[Train] epoch:27	batch id:160	 lr:0.098336 loss:1.807291
[Train] epoch:27	batch id:170	 lr:0.098336 loss:1.786440
[Train] epoch:27	batch id:180	 lr:0.098336 loss:1.987646
[Train] epoch:27	batch id:190	 lr:0.098336 loss:2.017524
[Train] epoch:27	batch id:200	 lr:0.098336 loss:1.677171
[Train] epoch:27	batch id:210	 lr:0.098336 loss:1.913405
[Train] epoch:27	batch id:220	 lr:0.098336 loss:1.947290
[Train] epoch:27	batch id:230	 lr:0.098336 loss:1.781698
[Train] epoch:27	batch id:240	 lr:0.098336 loss:1.726294
[Train] epoch:27	batch id:250	 lr:0.098336 loss:1.814501
[Train] epoch:27	batch id:260	 lr:0.098336 loss:1.765919
[Train] epoch:27	batch id:270	 lr:0.098336 loss:1.775691
[Train] epoch:27	batch id:280	 lr:0.098336 loss:1.698083
[Train] epoch:27	batch id:290	 lr:0.098336 loss:1.958921
[Train] epoch:27	batch id:300	 lr:0.098336 loss:1.775220
[Train] 27, loss: 1.828369, train acc: 0.793363, 
[Test] epoch:27	batch id:0	 loss:1.460771
[Test] epoch:27	batch id:10	 loss:1.633522
[Test] epoch:27	batch id:20	 loss:1.465723
[Test] epoch:27	batch id:30	 loss:1.487448
[Test] epoch:27	batch id:40	 loss:1.569317
[Test] epoch:27	batch id:50	 loss:1.567474
[Test] epoch:27	batch id:60	 loss:1.356924
[Test] epoch:27	batch id:70	 loss:1.544728
[Test] epoch:27	batch id:80	 loss:1.752875
[Test] epoch:27	batch id:90	 loss:1.524330
[Test] epoch:27	batch id:100	 loss:2.062476
[Test] epoch:27	batch id:110	 loss:1.600049
[Test] epoch:27	batch id:120	 loss:1.668044
[Test] epoch:27	batch id:130	 loss:1.647809
[Test] epoch:27	batch id:140	 loss:1.596056
[Test] epoch:27	batch id:150	 loss:2.049445
[Test] 27, loss: 1.644836, test acc: 0.863452,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:28	batch id:0	 lr:0.098220 loss:1.895388
[Train] epoch:28	batch id:10	 lr:0.098220 loss:1.742857
[Train] epoch:28	batch id:20	 lr:0.098220 loss:1.968453
[Train] epoch:28	batch id:30	 lr:0.098220 loss:1.838566
[Train] epoch:28	batch id:40	 lr:0.098220 loss:1.884702
[Train] epoch:28	batch id:50	 lr:0.098220 loss:2.059387
[Train] epoch:28	batch id:60	 lr:0.098220 loss:1.991165
[Train] epoch:28	batch id:70	 lr:0.098220 loss:1.962854
[Train] epoch:28	batch id:80	 lr:0.098220 loss:1.931240
[Train] epoch:28	batch id:90	 lr:0.098220 loss:1.881090
[Train] epoch:28	batch id:100	 lr:0.098220 loss:1.839217
[Train] epoch:28	batch id:110	 lr:0.098220 loss:1.885820
[Train] epoch:28	batch id:120	 lr:0.098220 loss:1.927886
[Train] epoch:28	batch id:130	 lr:0.098220 loss:1.951959
[Train] epoch:28	batch id:140	 lr:0.098220 loss:1.683495
[Train] epoch:28	batch id:150	 lr:0.098220 loss:1.626575
[Train] epoch:28	batch id:160	 lr:0.098220 loss:1.930489
[Train] epoch:28	batch id:170	 lr:0.098220 loss:1.840768
[Train] epoch:28	batch id:180	 lr:0.098220 loss:1.797121
[Train] epoch:28	batch id:190	 lr:0.098220 loss:1.661012
[Train] epoch:28	batch id:200	 lr:0.098220 loss:1.842812
[Train] epoch:28	batch id:210	 lr:0.098220 loss:1.833134
[Train] epoch:28	batch id:220	 lr:0.098220 loss:1.970181
[Train] epoch:28	batch id:230	 lr:0.098220 loss:1.953764
[Train] epoch:28	batch id:240	 lr:0.098220 loss:1.917420
[Train] epoch:28	batch id:250	 lr:0.098220 loss:1.962330
[Train] epoch:28	batch id:260	 lr:0.098220 loss:1.933110
[Train] epoch:28	batch id:270	 lr:0.098220 loss:1.721527
[Train] epoch:28	batch id:280	 lr:0.098220 loss:1.684536
[Train] epoch:28	batch id:290	 lr:0.098220 loss:1.681998
[Train] epoch:28	batch id:300	 lr:0.098220 loss:1.689224
[Train] 28, loss: 1.825608, train acc: 0.791633, 
[Test] epoch:28	batch id:0	 loss:1.399756
[Test] epoch:28	batch id:10	 loss:1.660905
[Test] epoch:28	batch id:20	 loss:1.500364
[Test] epoch:28	batch id:30	 loss:1.524864
[Test] epoch:28	batch id:40	 loss:1.755862
[Test] epoch:28	batch id:50	 loss:1.623426
[Test] epoch:28	batch id:60	 loss:1.367752
[Test] epoch:28	batch id:70	 loss:1.576679
[Test] epoch:28	batch id:80	 loss:1.592652
[Test] epoch:28	batch id:90	 loss:1.514030
[Test] epoch:28	batch id:100	 loss:2.005072
[Test] epoch:28	batch id:110	 loss:1.534505
[Test] epoch:28	batch id:120	 loss:1.491208
[Test] epoch:28	batch id:130	 loss:1.568064
[Test] epoch:28	batch id:140	 loss:1.499776
[Test] epoch:28	batch id:150	 loss:1.919367
[Test] 28, loss: 1.626521, test acc: 0.865478,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:29	batch id:0	 lr:0.098100 loss:1.746155
[Train] epoch:29	batch id:10	 lr:0.098100 loss:1.811361
[Train] epoch:29	batch id:20	 lr:0.098100 loss:1.853388
[Train] epoch:29	batch id:30	 lr:0.098100 loss:1.712694
[Train] epoch:29	batch id:40	 lr:0.098100 loss:1.812844
[Train] epoch:29	batch id:50	 lr:0.098100 loss:1.917943
[Train] epoch:29	batch id:60	 lr:0.098100 loss:1.630616
[Train] epoch:29	batch id:70	 lr:0.098100 loss:1.951833
[Train] epoch:29	batch id:80	 lr:0.098100 loss:1.593667
[Train] epoch:29	batch id:90	 lr:0.098100 loss:1.919913
[Train] epoch:29	batch id:100	 lr:0.098100 loss:1.874767
[Train] epoch:29	batch id:110	 lr:0.098100 loss:1.827864
[Train] epoch:29	batch id:120	 lr:0.098100 loss:1.787403
[Train] epoch:29	batch id:130	 lr:0.098100 loss:1.734617
[Train] epoch:29	batch id:140	 lr:0.098100 loss:1.799453
[Train] epoch:29	batch id:150	 lr:0.098100 loss:1.916167
[Train] epoch:29	batch id:160	 lr:0.098100 loss:1.653764
[Train] epoch:29	batch id:170	 lr:0.098100 loss:1.744814
[Train] epoch:29	batch id:180	 lr:0.098100 loss:1.774381
[Train] epoch:29	batch id:190	 lr:0.098100 loss:1.740461
[Train] epoch:29	batch id:200	 lr:0.098100 loss:1.806117
[Train] epoch:29	batch id:210	 lr:0.098100 loss:1.659053
[Train] epoch:29	batch id:220	 lr:0.098100 loss:1.689551
[Train] epoch:29	batch id:230	 lr:0.098100 loss:1.693834
[Train] epoch:29	batch id:240	 lr:0.098100 loss:1.888471
[Train] epoch:29	batch id:250	 lr:0.098100 loss:1.662674
[Train] epoch:29	batch id:260	 lr:0.098100 loss:1.771836
[Train] epoch:29	batch id:270	 lr:0.098100 loss:1.569127
[Train] epoch:29	batch id:280	 lr:0.098100 loss:2.200917
[Train] epoch:29	batch id:290	 lr:0.098100 loss:1.743001
[Train] epoch:29	batch id:300	 lr:0.098100 loss:1.726946
[Train] 29, loss: 1.807123, train acc: 0.800387, 
[Test] epoch:29	batch id:0	 loss:1.454888
[Test] epoch:29	batch id:10	 loss:1.667331
[Test] epoch:29	batch id:20	 loss:1.509735
[Test] epoch:29	batch id:30	 loss:1.577502
[Test] epoch:29	batch id:40	 loss:1.526811
[Test] epoch:29	batch id:50	 loss:1.690214
[Test] epoch:29	batch id:60	 loss:1.380149
[Test] epoch:29	batch id:70	 loss:1.687358
[Test] epoch:29	batch id:80	 loss:1.717268
[Test] epoch:29	batch id:90	 loss:1.524394
[Test] epoch:29	batch id:100	 loss:2.078526
[Test] epoch:29	batch id:110	 loss:1.408724
[Test] epoch:29	batch id:120	 loss:1.702963
[Test] epoch:29	batch id:130	 loss:1.611573
[Test] epoch:29	batch id:140	 loss:1.634870
[Test] epoch:29	batch id:150	 loss:1.962692
[Test] 29, loss: 1.660249, test acc: 0.841572,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:30	batch id:0	 lr:0.097976 loss:1.704340
[Train] epoch:30	batch id:10	 lr:0.097976 loss:1.644915
[Train] epoch:30	batch id:20	 lr:0.097976 loss:1.923746
[Train] epoch:30	batch id:30	 lr:0.097976 loss:1.619754
[Train] epoch:30	batch id:40	 lr:0.097976 loss:1.592211
[Train] epoch:30	batch id:50	 lr:0.097976 loss:1.776766
[Train] epoch:30	batch id:60	 lr:0.097976 loss:1.796866
[Train] epoch:30	batch id:70	 lr:0.097976 loss:1.806806
[Train] epoch:30	batch id:80	 lr:0.097976 loss:1.684271
[Train] epoch:30	batch id:90	 lr:0.097976 loss:1.822030
[Train] epoch:30	batch id:100	 lr:0.097976 loss:1.886356
[Train] epoch:30	batch id:110	 lr:0.097976 loss:1.857703
[Train] epoch:30	batch id:120	 lr:0.097976 loss:1.803774
[Train] epoch:30	batch id:130	 lr:0.097976 loss:1.742646
[Train] epoch:30	batch id:140	 lr:0.097976 loss:1.720836
[Train] epoch:30	batch id:150	 lr:0.097976 loss:1.844117
[Train] epoch:30	batch id:160	 lr:0.097976 loss:1.722044
[Train] epoch:30	batch id:170	 lr:0.097976 loss:1.726280
[Train] epoch:30	batch id:180	 lr:0.097976 loss:1.847165
[Train] epoch:30	batch id:190	 lr:0.097976 loss:1.831493
[Train] epoch:30	batch id:200	 lr:0.097976 loss:1.741694
[Train] epoch:30	batch id:210	 lr:0.097976 loss:1.794114
[Train] epoch:30	batch id:220	 lr:0.097976 loss:1.565711
[Train] epoch:30	batch id:230	 lr:0.097976 loss:1.818417
[Train] epoch:30	batch id:240	 lr:0.097976 loss:1.799965
[Train] epoch:30	batch id:250	 lr:0.097976 loss:1.832555
[Train] epoch:30	batch id:260	 lr:0.097976 loss:1.825260
[Train] epoch:30	batch id:270	 lr:0.097976 loss:1.883768
[Train] epoch:30	batch id:280	 lr:0.097976 loss:1.805357
[Train] epoch:30	batch id:290	 lr:0.097976 loss:1.656120
[Train] epoch:30	batch id:300	 lr:0.097976 loss:1.672119
[Train] 30, loss: 1.806193, train acc: 0.797537, 
[Test] epoch:30	batch id:0	 loss:1.501958
[Test] epoch:30	batch id:10	 loss:1.675272
[Test] epoch:30	batch id:20	 loss:1.555200
[Test] epoch:30	batch id:30	 loss:1.622669
[Test] epoch:30	batch id:40	 loss:1.571138
[Test] epoch:30	batch id:50	 loss:1.605680
[Test] epoch:30	batch id:60	 loss:1.355178
[Test] epoch:30	batch id:70	 loss:1.542594
[Test] epoch:30	batch id:80	 loss:1.751305
[Test] epoch:30	batch id:90	 loss:1.531113
[Test] epoch:30	batch id:100	 loss:1.977439
[Test] epoch:30	batch id:110	 loss:1.595423
[Test] epoch:30	batch id:120	 loss:1.620930
[Test] epoch:30	batch id:130	 loss:1.517685
[Test] epoch:30	batch id:140	 loss:1.613829
[Test] epoch:30	batch id:150	 loss:2.047516
[Test] 30, loss: 1.632175, test acc: 0.867099,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:31	batch id:0	 lr:0.097848 loss:1.799669
[Train] epoch:31	batch id:10	 lr:0.097848 loss:1.811602
[Train] epoch:31	batch id:20	 lr:0.097848 loss:1.640817
[Train] epoch:31	batch id:30	 lr:0.097848 loss:1.887044
[Train] epoch:31	batch id:40	 lr:0.097848 loss:1.881781
[Train] epoch:31	batch id:50	 lr:0.097848 loss:1.713874
[Train] epoch:31	batch id:60	 lr:0.097848 loss:1.761653
[Train] epoch:31	batch id:70	 lr:0.097848 loss:1.851590
[Train] epoch:31	batch id:80	 lr:0.097848 loss:1.962240
[Train] epoch:31	batch id:90	 lr:0.097848 loss:1.762760
[Train] epoch:31	batch id:100	 lr:0.097848 loss:1.899386
[Train] epoch:31	batch id:110	 lr:0.097848 loss:1.696877
[Train] epoch:31	batch id:120	 lr:0.097848 loss:1.649982
[Train] epoch:31	batch id:130	 lr:0.097848 loss:1.958338
[Train] epoch:31	batch id:140	 lr:0.097848 loss:1.776166
[Train] epoch:31	batch id:150	 lr:0.097848 loss:1.674519
[Train] epoch:31	batch id:160	 lr:0.097848 loss:1.801092
[Train] epoch:31	batch id:170	 lr:0.097848 loss:1.826089
[Train] epoch:31	batch id:180	 lr:0.097848 loss:1.901882
[Train] epoch:31	batch id:190	 lr:0.097848 loss:1.724738
[Train] epoch:31	batch id:200	 lr:0.097848 loss:1.777016
[Train] epoch:31	batch id:210	 lr:0.097848 loss:1.571104
[Train] epoch:31	batch id:220	 lr:0.097848 loss:1.804474
[Train] epoch:31	batch id:230	 lr:0.097848 loss:1.690579
[Train] epoch:31	batch id:240	 lr:0.097848 loss:1.820883
[Train] epoch:31	batch id:250	 lr:0.097848 loss:1.698540
[Train] epoch:31	batch id:260	 lr:0.097848 loss:1.690820
[Train] epoch:31	batch id:270	 lr:0.097848 loss:1.728644
[Train] epoch:31	batch id:280	 lr:0.097848 loss:1.912409
[Train] epoch:31	batch id:290	 lr:0.097848 loss:1.823619
[Train] epoch:31	batch id:300	 lr:0.097848 loss:1.767893
[Train] 31, loss: 1.792213, train acc: 0.812602, 
[Test] epoch:31	batch id:0	 loss:1.426023
[Test] epoch:31	batch id:10	 loss:1.694352
[Test] epoch:31	batch id:20	 loss:1.510696
[Test] epoch:31	batch id:30	 loss:1.583164
[Test] epoch:31	batch id:40	 loss:1.593847
[Test] epoch:31	batch id:50	 loss:1.573640
[Test] epoch:31	batch id:60	 loss:1.360644
[Test] epoch:31	batch id:70	 loss:1.535811
[Test] epoch:31	batch id:80	 loss:1.789298
[Test] epoch:31	batch id:90	 loss:1.502316
[Test] epoch:31	batch id:100	 loss:1.935746
[Test] epoch:31	batch id:110	 loss:1.508467
[Test] epoch:31	batch id:120	 loss:1.544639
[Test] epoch:31	batch id:130	 loss:1.562977
[Test] epoch:31	batch id:140	 loss:1.542215
[Test] epoch:31	batch id:150	 loss:2.082035
[Test] 31, loss: 1.617638, test acc: 0.873987,
Max Acc:0.873987
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:32	batch id:0	 lr:0.097717 loss:1.932194
[Train] epoch:32	batch id:10	 lr:0.097717 loss:1.838573
[Train] epoch:32	batch id:20	 lr:0.097717 loss:1.756649
[Train] epoch:32	batch id:30	 lr:0.097717 loss:1.665654
[Train] epoch:32	batch id:40	 lr:0.097717 loss:1.779621
[Train] epoch:32	batch id:50	 lr:0.097717 loss:1.707530
[Train] epoch:32	batch id:60	 lr:0.097717 loss:1.639678
[Train] epoch:32	batch id:70	 lr:0.097717 loss:1.725689
[Train] epoch:32	batch id:80	 lr:0.097717 loss:1.657026
[Train] epoch:32	batch id:90	 lr:0.097717 loss:1.705204
[Train] epoch:32	batch id:100	 lr:0.097717 loss:1.684020
[Train] epoch:32	batch id:110	 lr:0.097717 loss:1.654271
[Train] epoch:32	batch id:120	 lr:0.097717 loss:1.693581
[Train] epoch:32	batch id:130	 lr:0.097717 loss:1.779889
[Train] epoch:32	batch id:140	 lr:0.097717 loss:1.634683
[Train] epoch:32	batch id:150	 lr:0.097717 loss:1.726556
[Train] epoch:32	batch id:160	 lr:0.097717 loss:1.764583
[Train] epoch:32	batch id:170	 lr:0.097717 loss:1.794801
[Train] epoch:32	batch id:180	 lr:0.097717 loss:1.814863
[Train] epoch:32	batch id:190	 lr:0.097717 loss:1.833682
[Train] epoch:32	batch id:200	 lr:0.097717 loss:1.720866
[Train] epoch:32	batch id:210	 lr:0.097717 loss:1.841445
[Train] epoch:32	batch id:220	 lr:0.097717 loss:1.600161
[Train] epoch:32	batch id:230	 lr:0.097717 loss:1.909075
[Train] epoch:32	batch id:240	 lr:0.097717 loss:1.751501
[Train] epoch:32	batch id:250	 lr:0.097717 loss:1.820974
[Train] epoch:32	batch id:260	 lr:0.097717 loss:1.728285
[Train] epoch:32	batch id:270	 lr:0.097717 loss:1.612508
[Train] epoch:32	batch id:280	 lr:0.097717 loss:1.854879
[Train] epoch:32	batch id:290	 lr:0.097717 loss:1.647507
[Train] epoch:32	batch id:300	 lr:0.097717 loss:1.805912
[Train] 32, loss: 1.782938, train acc: 0.811075, 
[Test] epoch:32	batch id:0	 loss:1.402227
[Test] epoch:32	batch id:10	 loss:1.600611
[Test] epoch:32	batch id:20	 loss:1.561126
[Test] epoch:32	batch id:30	 loss:1.500580
[Test] epoch:32	batch id:40	 loss:1.497446
[Test] epoch:32	batch id:50	 loss:1.561318
[Test] epoch:32	batch id:60	 loss:1.318791
[Test] epoch:32	batch id:70	 loss:1.581993
[Test] epoch:32	batch id:80	 loss:1.743661
[Test] epoch:32	batch id:90	 loss:1.578619
[Test] epoch:32	batch id:100	 loss:1.861681
[Test] epoch:32	batch id:110	 loss:1.517871
[Test] epoch:32	batch id:120	 loss:1.531811
[Test] epoch:32	batch id:130	 loss:1.571141
[Test] epoch:32	batch id:140	 loss:1.564009
[Test] epoch:32	batch id:150	 loss:1.928619
[Test] 32, loss: 1.629572, test acc: 0.863857,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:33	batch id:0	 lr:0.097581 loss:1.724326
[Train] epoch:33	batch id:10	 lr:0.097581 loss:1.760332
[Train] epoch:33	batch id:20	 lr:0.097581 loss:1.857158
[Train] epoch:33	batch id:30	 lr:0.097581 loss:1.792712
[Train] epoch:33	batch id:40	 lr:0.097581 loss:1.830782
[Train] epoch:33	batch id:50	 lr:0.097581 loss:1.931639
[Train] epoch:33	batch id:60	 lr:0.097581 loss:1.744286
[Train] epoch:33	batch id:70	 lr:0.097581 loss:2.024434
[Train] epoch:33	batch id:80	 lr:0.097581 loss:1.575563
[Train] epoch:33	batch id:90	 lr:0.097581 loss:1.870754
[Train] epoch:33	batch id:100	 lr:0.097581 loss:2.014789
[Train] epoch:33	batch id:110	 lr:0.097581 loss:1.948920
[Train] epoch:33	batch id:120	 lr:0.097581 loss:1.969333
[Train] epoch:33	batch id:130	 lr:0.097581 loss:1.845257
[Train] epoch:33	batch id:140	 lr:0.097581 loss:1.751311
[Train] epoch:33	batch id:150	 lr:0.097581 loss:2.068617
[Train] epoch:33	batch id:160	 lr:0.097581 loss:1.587890
[Train] epoch:33	batch id:170	 lr:0.097581 loss:1.668686
[Train] epoch:33	batch id:180	 lr:0.097581 loss:1.980564
[Train] epoch:33	batch id:190	 lr:0.097581 loss:1.766809
[Train] epoch:33	batch id:200	 lr:0.097581 loss:1.937306
[Train] epoch:33	batch id:210	 lr:0.097581 loss:1.925306
[Train] epoch:33	batch id:220	 lr:0.097581 loss:1.758034
[Train] epoch:33	batch id:230	 lr:0.097581 loss:1.839576
[Train] epoch:33	batch id:240	 lr:0.097581 loss:1.743792
[Train] epoch:33	batch id:250	 lr:0.097581 loss:1.731063
[Train] epoch:33	batch id:260	 lr:0.097581 loss:1.976207
[Train] epoch:33	batch id:270	 lr:0.097581 loss:1.837957
[Train] epoch:33	batch id:280	 lr:0.097581 loss:1.819743
[Train] epoch:33	batch id:290	 lr:0.097581 loss:1.618603
[Train] epoch:33	batch id:300	 lr:0.097581 loss:1.772205
[Train] 33, loss: 1.791188, train acc: 0.809344, 
[Test] epoch:33	batch id:0	 loss:1.456313
[Test] epoch:33	batch id:10	 loss:1.669117
[Test] epoch:33	batch id:20	 loss:1.500468
[Test] epoch:33	batch id:30	 loss:1.561372
[Test] epoch:33	batch id:40	 loss:1.565988
[Test] epoch:33	batch id:50	 loss:1.479810
[Test] epoch:33	batch id:60	 loss:1.345250
[Test] epoch:33	batch id:70	 loss:1.624394
[Test] epoch:33	batch id:80	 loss:1.608338
[Test] epoch:33	batch id:90	 loss:1.585138
[Test] epoch:33	batch id:100	 loss:2.019422
[Test] epoch:33	batch id:110	 loss:1.548831
[Test] epoch:33	batch id:120	 loss:1.591099
[Test] epoch:33	batch id:130	 loss:1.503878
[Test] epoch:33	batch id:140	 loss:1.407259
[Test] epoch:33	batch id:150	 loss:1.898566
[Test] 33, loss: 1.610802, test acc: 0.875608,
Max Acc:0.875608
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:34	batch id:0	 lr:0.097442 loss:1.632771
[Train] epoch:34	batch id:10	 lr:0.097442 loss:1.717405
[Train] epoch:34	batch id:20	 lr:0.097442 loss:1.865039
[Train] epoch:34	batch id:30	 lr:0.097442 loss:1.748742
[Train] epoch:34	batch id:40	 lr:0.097442 loss:1.941712
[Train] epoch:34	batch id:50	 lr:0.097442 loss:1.696622
[Train] epoch:34	batch id:60	 lr:0.097442 loss:1.790568
[Train] epoch:34	batch id:70	 lr:0.097442 loss:1.692686
[Train] epoch:34	batch id:80	 lr:0.097442 loss:1.757854
[Train] epoch:34	batch id:90	 lr:0.097442 loss:1.777233
[Train] epoch:34	batch id:100	 lr:0.097442 loss:1.833918
[Train] epoch:34	batch id:110	 lr:0.097442 loss:1.646993
[Train] epoch:34	batch id:120	 lr:0.097442 loss:1.862256
[Train] epoch:34	batch id:130	 lr:0.097442 loss:1.663575
[Train] epoch:34	batch id:140	 lr:0.097442 loss:1.728192
[Train] epoch:34	batch id:150	 lr:0.097442 loss:1.751160
[Train] epoch:34	batch id:160	 lr:0.097442 loss:1.772095
[Train] epoch:34	batch id:170	 lr:0.097442 loss:1.566433
[Train] epoch:34	batch id:180	 lr:0.097442 loss:1.823257
[Train] epoch:34	batch id:190	 lr:0.097442 loss:1.752541
[Train] epoch:34	batch id:200	 lr:0.097442 loss:1.977664
[Train] epoch:34	batch id:210	 lr:0.097442 loss:1.737810
[Train] epoch:34	batch id:220	 lr:0.097442 loss:1.944550
[Train] epoch:34	batch id:230	 lr:0.097442 loss:1.655400
[Train] epoch:34	batch id:240	 lr:0.097442 loss:1.672304
[Train] epoch:34	batch id:250	 lr:0.097442 loss:1.634169
[Train] epoch:34	batch id:260	 lr:0.097442 loss:1.622371
[Train] epoch:34	batch id:270	 lr:0.097442 loss:1.885579
[Train] epoch:34	batch id:280	 lr:0.097442 loss:1.727437
[Train] epoch:34	batch id:290	 lr:0.097442 loss:1.579799
[Train] epoch:34	batch id:300	 lr:0.097442 loss:1.855206
[Train] 34, loss: 1.781933, train acc: 0.815248, 
[Test] epoch:34	batch id:0	 loss:1.501036
[Test] epoch:34	batch id:10	 loss:1.567258
[Test] epoch:34	batch id:20	 loss:1.535313
[Test] epoch:34	batch id:30	 loss:1.513012
[Test] epoch:34	batch id:40	 loss:1.594503
[Test] epoch:34	batch id:50	 loss:1.561911
[Test] epoch:34	batch id:60	 loss:1.333524
[Test] epoch:34	batch id:70	 loss:1.490486
[Test] epoch:34	batch id:80	 loss:1.697434
[Test] epoch:34	batch id:90	 loss:1.543803
[Test] epoch:34	batch id:100	 loss:2.021503
[Test] epoch:34	batch id:110	 loss:1.457721
[Test] epoch:34	batch id:120	 loss:1.570319
[Test] epoch:34	batch id:130	 loss:1.523246
[Test] epoch:34	batch id:140	 loss:1.487510
[Test] epoch:34	batch id:150	 loss:1.959666
[Test] 34, loss: 1.599274, test acc: 0.877634,
Max Acc:0.877634
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:35	batch id:0	 lr:0.097299 loss:1.998484
[Train] epoch:35	batch id:10	 lr:0.097299 loss:1.679493
[Train] epoch:35	batch id:20	 lr:0.097299 loss:1.673596
[Train] epoch:35	batch id:30	 lr:0.097299 loss:1.844620
[Train] epoch:35	batch id:40	 lr:0.097299 loss:1.876622
[Train] epoch:35	batch id:50	 lr:0.097299 loss:2.079780
[Train] epoch:35	batch id:60	 lr:0.097299 loss:1.795413
[Train] epoch:35	batch id:70	 lr:0.097299 loss:1.835311
[Train] epoch:35	batch id:80	 lr:0.097299 loss:1.976675
[Train] epoch:35	batch id:90	 lr:0.097299 loss:1.889784
[Train] epoch:35	batch id:100	 lr:0.097299 loss:1.748683
[Train] epoch:35	batch id:110	 lr:0.097299 loss:1.944013
[Train] epoch:35	batch id:120	 lr:0.097299 loss:1.841445
[Train] epoch:35	batch id:130	 lr:0.097299 loss:1.814991
[Train] epoch:35	batch id:140	 lr:0.097299 loss:1.655192
[Train] epoch:35	batch id:150	 lr:0.097299 loss:1.750706
[Train] epoch:35	batch id:160	 lr:0.097299 loss:1.820511
[Train] epoch:35	batch id:170	 lr:0.097299 loss:1.832384
[Train] epoch:35	batch id:180	 lr:0.097299 loss:1.647820
[Train] epoch:35	batch id:190	 lr:0.097299 loss:1.590544
[Train] epoch:35	batch id:200	 lr:0.097299 loss:1.768359
[Train] epoch:35	batch id:210	 lr:0.097299 loss:1.832705
[Train] epoch:35	batch id:220	 lr:0.097299 loss:1.713536
[Train] epoch:35	batch id:230	 lr:0.097299 loss:1.883958
[Train] epoch:35	batch id:240	 lr:0.097299 loss:2.042941
[Train] epoch:35	batch id:250	 lr:0.097299 loss:1.958447
[Train] epoch:35	batch id:260	 lr:0.097299 loss:1.893884
[Train] epoch:35	batch id:270	 lr:0.097299 loss:1.768462
[Train] epoch:35	batch id:280	 lr:0.097299 loss:1.538456
[Train] epoch:35	batch id:290	 lr:0.097299 loss:1.815310
[Train] epoch:35	batch id:300	 lr:0.097299 loss:1.644885
[Train] 35, loss: 1.784047, train acc: 0.807919, 
[Test] epoch:35	batch id:0	 loss:1.458365
[Test] epoch:35	batch id:10	 loss:1.601891
[Test] epoch:35	batch id:20	 loss:1.544490
[Test] epoch:35	batch id:30	 loss:1.614805
[Test] epoch:35	batch id:40	 loss:1.571490
[Test] epoch:35	batch id:50	 loss:1.728253
[Test] epoch:35	batch id:60	 loss:1.433627
[Test] epoch:35	batch id:70	 loss:1.569950
[Test] epoch:35	batch id:80	 loss:1.687968
[Test] epoch:35	batch id:90	 loss:1.562660
[Test] epoch:35	batch id:100	 loss:1.926554
[Test] epoch:35	batch id:110	 loss:1.637823
[Test] epoch:35	batch id:120	 loss:1.661854
[Test] epoch:35	batch id:130	 loss:1.624980
[Test] epoch:35	batch id:140	 loss:1.584682
[Test] epoch:35	batch id:150	 loss:2.202292
[Test] 35, loss: 1.694686, test acc: 0.852917,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:36	batch id:0	 lr:0.097152 loss:1.773910
[Train] epoch:36	batch id:10	 lr:0.097152 loss:1.802857
[Train] epoch:36	batch id:20	 lr:0.097152 loss:1.584000
[Train] epoch:36	batch id:30	 lr:0.097152 loss:1.823618
[Train] epoch:36	batch id:40	 lr:0.097152 loss:1.671081
[Train] epoch:36	batch id:50	 lr:0.097152 loss:1.659848
[Train] epoch:36	batch id:60	 lr:0.097152 loss:1.758022
[Train] epoch:36	batch id:70	 lr:0.097152 loss:1.855211
[Train] epoch:36	batch id:80	 lr:0.097152 loss:1.652147
[Train] epoch:36	batch id:90	 lr:0.097152 loss:1.841003
[Train] epoch:36	batch id:100	 lr:0.097152 loss:1.834083
[Train] epoch:36	batch id:110	 lr:0.097152 loss:1.579043
[Train] epoch:36	batch id:120	 lr:0.097152 loss:1.964812
[Train] epoch:36	batch id:130	 lr:0.097152 loss:1.733946
[Train] epoch:36	batch id:140	 lr:0.097152 loss:1.877708
[Train] epoch:36	batch id:150	 lr:0.097152 loss:1.706116
[Train] epoch:36	batch id:160	 lr:0.097152 loss:1.894670
[Train] epoch:36	batch id:170	 lr:0.097152 loss:1.739614
[Train] epoch:36	batch id:180	 lr:0.097152 loss:1.876798
[Train] epoch:36	batch id:190	 lr:0.097152 loss:1.610532
[Train] epoch:36	batch id:200	 lr:0.097152 loss:1.637375
[Train] epoch:36	batch id:210	 lr:0.097152 loss:1.814718
[Train] epoch:36	batch id:220	 lr:0.097152 loss:1.717506
[Train] epoch:36	batch id:230	 lr:0.097152 loss:1.698248
[Train] epoch:36	batch id:240	 lr:0.097152 loss:1.881979
[Train] epoch:36	batch id:250	 lr:0.097152 loss:1.815998
[Train] epoch:36	batch id:260	 lr:0.097152 loss:1.848104
[Train] epoch:36	batch id:270	 lr:0.097152 loss:1.696174
[Train] epoch:36	batch id:280	 lr:0.097152 loss:1.642757
[Train] epoch:36	batch id:290	 lr:0.097152 loss:1.706447
[Train] epoch:36	batch id:300	 lr:0.097152 loss:2.030082
[Train] 36, loss: 1.769333, train acc: 0.819320, 
[Test] epoch:36	batch id:0	 loss:1.477967
[Test] epoch:36	batch id:10	 loss:1.725867
[Test] epoch:36	batch id:20	 loss:1.578997
[Test] epoch:36	batch id:30	 loss:1.571367
[Test] epoch:36	batch id:40	 loss:1.717608
[Test] epoch:36	batch id:50	 loss:1.631971
[Test] epoch:36	batch id:60	 loss:1.341636
[Test] epoch:36	batch id:70	 loss:1.626619
[Test] epoch:36	batch id:80	 loss:1.719056
[Test] epoch:36	batch id:90	 loss:1.581838
[Test] epoch:36	batch id:100	 loss:1.948494
[Test] epoch:36	batch id:110	 loss:1.680571
[Test] epoch:36	batch id:120	 loss:1.604105
[Test] epoch:36	batch id:130	 loss:1.709380
[Test] epoch:36	batch id:140	 loss:1.538061
[Test] epoch:36	batch id:150	 loss:1.925084
[Test] 36, loss: 1.648601, test acc: 0.852512,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:37	batch id:0	 lr:0.097002 loss:1.793400
[Train] epoch:37	batch id:10	 lr:0.097002 loss:2.127839
[Train] epoch:37	batch id:20	 lr:0.097002 loss:1.641091
[Train] epoch:37	batch id:30	 lr:0.097002 loss:1.845341
[Train] epoch:37	batch id:40	 lr:0.097002 loss:1.726003
[Train] epoch:37	batch id:50	 lr:0.097002 loss:1.663080
[Train] epoch:37	batch id:60	 lr:0.097002 loss:1.721606
[Train] epoch:37	batch id:70	 lr:0.097002 loss:1.722586
[Train] epoch:37	batch id:80	 lr:0.097002 loss:1.741862
[Train] epoch:37	batch id:90	 lr:0.097002 loss:1.845669
[Train] epoch:37	batch id:100	 lr:0.097002 loss:1.808522
[Train] epoch:37	batch id:110	 lr:0.097002 loss:1.673813
[Train] epoch:37	batch id:120	 lr:0.097002 loss:1.759644
[Train] epoch:37	batch id:130	 lr:0.097002 loss:1.843042
[Train] epoch:37	batch id:140	 lr:0.097002 loss:1.777372
[Train] epoch:37	batch id:150	 lr:0.097002 loss:1.866127
[Train] epoch:37	batch id:160	 lr:0.097002 loss:1.821414
[Train] epoch:37	batch id:170	 lr:0.097002 loss:1.934274
[Train] epoch:37	batch id:180	 lr:0.097002 loss:1.566998
[Train] epoch:37	batch id:190	 lr:0.097002 loss:1.718245
[Train] epoch:37	batch id:200	 lr:0.097002 loss:1.920738
[Train] epoch:37	batch id:210	 lr:0.097002 loss:1.554027
[Train] epoch:37	batch id:220	 lr:0.097002 loss:1.794461
[Train] epoch:37	batch id:230	 lr:0.097002 loss:1.927818
[Train] epoch:37	batch id:240	 lr:0.097002 loss:1.768588
[Train] epoch:37	batch id:250	 lr:0.097002 loss:1.623667
[Train] epoch:37	batch id:260	 lr:0.097002 loss:1.743386
[Train] epoch:37	batch id:270	 lr:0.097002 loss:1.940861
[Train] epoch:37	batch id:280	 lr:0.097002 loss:1.865927
[Train] epoch:37	batch id:290	 lr:0.097002 loss:1.874032
[Train] epoch:37	batch id:300	 lr:0.097002 loss:1.795122
[Train] 37, loss: 1.763695, train acc: 0.821152, 
[Test] epoch:37	batch id:0	 loss:1.462694
[Test] epoch:37	batch id:10	 loss:1.621149
[Test] epoch:37	batch id:20	 loss:1.515977
[Test] epoch:37	batch id:30	 loss:1.426493
[Test] epoch:37	batch id:40	 loss:1.626605
[Test] epoch:37	batch id:50	 loss:1.569648
[Test] epoch:37	batch id:60	 loss:1.356752
[Test] epoch:37	batch id:70	 loss:1.557190
[Test] epoch:37	batch id:80	 loss:1.611404
[Test] epoch:37	batch id:90	 loss:1.539126
[Test] epoch:37	batch id:100	 loss:2.171161
[Test] epoch:37	batch id:110	 loss:1.581231
[Test] epoch:37	batch id:120	 loss:1.524997
[Test] epoch:37	batch id:130	 loss:1.560964
[Test] epoch:37	batch id:140	 loss:1.513748
[Test] epoch:37	batch id:150	 loss:1.919011
[Test] 37, loss: 1.633051, test acc: 0.863452,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:38	batch id:0	 lr:0.096848 loss:1.957403
[Train] epoch:38	batch id:10	 lr:0.096848 loss:1.576852
[Train] epoch:38	batch id:20	 lr:0.096848 loss:1.706083
[Train] epoch:38	batch id:30	 lr:0.096848 loss:1.702600
[Train] epoch:38	batch id:40	 lr:0.096848 loss:1.654451
[Train] epoch:38	batch id:50	 lr:0.096848 loss:1.769609
[Train] epoch:38	batch id:60	 lr:0.096848 loss:1.583165
[Train] epoch:38	batch id:70	 lr:0.096848 loss:1.829889
[Train] epoch:38	batch id:80	 lr:0.096848 loss:1.818368
[Train] epoch:38	batch id:90	 lr:0.096848 loss:1.856375
[Train] epoch:38	batch id:100	 lr:0.096848 loss:1.725352
[Train] epoch:38	batch id:110	 lr:0.096848 loss:1.775951
[Train] epoch:38	batch id:120	 lr:0.096848 loss:1.697146
[Train] epoch:38	batch id:130	 lr:0.096848 loss:1.786046
[Train] epoch:38	batch id:140	 lr:0.096848 loss:1.763283
[Train] epoch:38	batch id:150	 lr:0.096848 loss:1.800817
[Train] epoch:38	batch id:160	 lr:0.096848 loss:1.859457
[Train] epoch:38	batch id:170	 lr:0.096848 loss:1.855684
[Train] epoch:38	batch id:180	 lr:0.096848 loss:1.769203
[Train] epoch:38	batch id:190	 lr:0.096848 loss:1.684365
[Train] epoch:38	batch id:200	 lr:0.096848 loss:1.734473
[Train] epoch:38	batch id:210	 lr:0.096848 loss:1.643836
[Train] epoch:38	batch id:220	 lr:0.096848 loss:1.931575
[Train] epoch:38	batch id:230	 lr:0.096848 loss:1.722242
[Train] epoch:38	batch id:240	 lr:0.096848 loss:1.692027
[Train] epoch:38	batch id:250	 lr:0.096848 loss:1.771863
[Train] epoch:38	batch id:260	 lr:0.096848 loss:1.774185
[Train] epoch:38	batch id:270	 lr:0.096848 loss:1.882937
[Train] epoch:38	batch id:280	 lr:0.096848 loss:1.870788
[Train] epoch:38	batch id:290	 lr:0.096848 loss:1.959835
[Train] epoch:38	batch id:300	 lr:0.096848 loss:1.623766
[Train] 38, loss: 1.759920, train acc: 0.822476, 
[Test] epoch:38	batch id:0	 loss:1.479526
[Test] epoch:38	batch id:10	 loss:1.713226
[Test] epoch:38	batch id:20	 loss:1.551908
[Test] epoch:38	batch id:30	 loss:1.710247
[Test] epoch:38	batch id:40	 loss:1.696461
[Test] epoch:38	batch id:50	 loss:1.680363
[Test] epoch:38	batch id:60	 loss:1.349404
[Test] epoch:38	batch id:70	 loss:1.567912
[Test] epoch:38	batch id:80	 loss:1.740684
[Test] epoch:38	batch id:90	 loss:1.560130
[Test] epoch:38	batch id:100	 loss:2.027976
[Test] epoch:38	batch id:110	 loss:1.559848
[Test] epoch:38	batch id:120	 loss:1.666780
[Test] epoch:38	batch id:130	 loss:1.629899
[Test] epoch:38	batch id:140	 loss:1.567735
[Test] epoch:38	batch id:150	 loss:1.950943
[Test] 38, loss: 1.656768, test acc: 0.848865,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:39	batch id:0	 lr:0.096690 loss:1.562972
[Train] epoch:39	batch id:10	 lr:0.096690 loss:1.757942
[Train] epoch:39	batch id:20	 lr:0.096690 loss:1.828083
[Train] epoch:39	batch id:30	 lr:0.096690 loss:1.794304
[Train] epoch:39	batch id:40	 lr:0.096690 loss:1.620894
[Train] epoch:39	batch id:50	 lr:0.096690 loss:1.842996
[Train] epoch:39	batch id:60	 lr:0.096690 loss:1.766849
[Train] epoch:39	batch id:70	 lr:0.096690 loss:1.656415
[Train] epoch:39	batch id:80	 lr:0.096690 loss:1.721194
[Train] epoch:39	batch id:90	 lr:0.096690 loss:1.617827
[Train] epoch:39	batch id:100	 lr:0.096690 loss:1.697617
[Train] epoch:39	batch id:110	 lr:0.096690 loss:1.708113
[Train] epoch:39	batch id:120	 lr:0.096690 loss:1.735736
[Train] epoch:39	batch id:130	 lr:0.096690 loss:1.892025
[Train] epoch:39	batch id:140	 lr:0.096690 loss:1.782803
[Train] epoch:39	batch id:150	 lr:0.096690 loss:1.866359
[Train] epoch:39	batch id:160	 lr:0.096690 loss:1.830922
[Train] epoch:39	batch id:170	 lr:0.096690 loss:1.747201
[Train] epoch:39	batch id:180	 lr:0.096690 loss:1.712862
[Train] epoch:39	batch id:190	 lr:0.096690 loss:1.587574
[Train] epoch:39	batch id:200	 lr:0.096690 loss:1.686068
[Train] epoch:39	batch id:210	 lr:0.096690 loss:1.706269
[Train] epoch:39	batch id:220	 lr:0.096690 loss:1.573143
[Train] epoch:39	batch id:230	 lr:0.096690 loss:1.797905
[Train] epoch:39	batch id:240	 lr:0.096690 loss:1.872867
[Train] epoch:39	batch id:250	 lr:0.096690 loss:1.698282
[Train] epoch:39	batch id:260	 lr:0.096690 loss:1.632661
[Train] epoch:39	batch id:270	 lr:0.096690 loss:1.959530
[Train] epoch:39	batch id:280	 lr:0.096690 loss:1.573903
[Train] epoch:39	batch id:290	 lr:0.096690 loss:1.926412
[Train] epoch:39	batch id:300	 lr:0.096690 loss:1.884096
[Train] 39, loss: 1.756102, train acc: 0.823493, 
[Test] epoch:39	batch id:0	 loss:1.398992
[Test] epoch:39	batch id:10	 loss:1.586419
[Test] epoch:39	batch id:20	 loss:1.479501
[Test] epoch:39	batch id:30	 loss:1.560869
[Test] epoch:39	batch id:40	 loss:1.586498
[Test] epoch:39	batch id:50	 loss:1.625459
[Test] epoch:39	batch id:60	 loss:1.336159
[Test] epoch:39	batch id:70	 loss:1.638097
[Test] epoch:39	batch id:80	 loss:1.741724
[Test] epoch:39	batch id:90	 loss:1.522930
[Test] epoch:39	batch id:100	 loss:1.912006
[Test] epoch:39	batch id:110	 loss:1.524738
[Test] epoch:39	batch id:120	 loss:1.466998
[Test] epoch:39	batch id:130	 loss:1.542353
[Test] epoch:39	batch id:140	 loss:1.448195
[Test] epoch:39	batch id:150	 loss:1.895086
[Test] 39, loss: 1.593483, test acc: 0.883306,
Max Acc:0.883306
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:40	batch id:0	 lr:0.096528 loss:1.799571
[Train] epoch:40	batch id:10	 lr:0.096528 loss:1.805985
[Train] epoch:40	batch id:20	 lr:0.096528 loss:1.919505
[Train] epoch:40	batch id:30	 lr:0.096528 loss:1.841713
[Train] epoch:40	batch id:40	 lr:0.096528 loss:1.542974
[Train] epoch:40	batch id:50	 lr:0.096528 loss:1.711560
[Train] epoch:40	batch id:60	 lr:0.096528 loss:1.645154
[Train] epoch:40	batch id:70	 lr:0.096528 loss:1.791651
[Train] epoch:40	batch id:80	 lr:0.096528 loss:1.834537
[Train] epoch:40	batch id:90	 lr:0.096528 loss:1.715319
[Train] epoch:40	batch id:100	 lr:0.096528 loss:2.111300
[Train] epoch:40	batch id:110	 lr:0.096528 loss:2.039382
[Train] epoch:40	batch id:120	 lr:0.096528 loss:1.920901
[Train] epoch:40	batch id:130	 lr:0.096528 loss:1.917002
[Train] epoch:40	batch id:140	 lr:0.096528 loss:1.788929
[Train] epoch:40	batch id:150	 lr:0.096528 loss:1.717226
[Train] epoch:40	batch id:160	 lr:0.096528 loss:1.654858
[Train] epoch:40	batch id:170	 lr:0.096528 loss:1.631314
[Train] epoch:40	batch id:180	 lr:0.096528 loss:1.649110
[Train] epoch:40	batch id:190	 lr:0.096528 loss:1.709399
[Train] epoch:40	batch id:200	 lr:0.096528 loss:1.713676
[Train] epoch:40	batch id:210	 lr:0.096528 loss:1.667532
[Train] epoch:40	batch id:220	 lr:0.096528 loss:1.641140
[Train] epoch:40	batch id:230	 lr:0.096528 loss:1.907550
[Train] epoch:40	batch id:240	 lr:0.096528 loss:1.684520
[Train] epoch:40	batch id:250	 lr:0.096528 loss:1.751003
[Train] epoch:40	batch id:260	 lr:0.096528 loss:1.897632
[Train] epoch:40	batch id:270	 lr:0.096528 loss:1.678913
[Train] epoch:40	batch id:280	 lr:0.096528 loss:1.543897
[Train] epoch:40	batch id:290	 lr:0.096528 loss:1.613609
[Train] epoch:40	batch id:300	 lr:0.096528 loss:1.748909
[Train] 40, loss: 1.740634, train acc: 0.828685, 
[Test] epoch:40	batch id:0	 loss:1.482611
[Test] epoch:40	batch id:10	 loss:1.695467
[Test] epoch:40	batch id:20	 loss:1.564588
[Test] epoch:40	batch id:30	 loss:1.508937
[Test] epoch:40	batch id:40	 loss:1.611628
[Test] epoch:40	batch id:50	 loss:1.741190
[Test] epoch:40	batch id:60	 loss:1.378739
[Test] epoch:40	batch id:70	 loss:1.567108
[Test] epoch:40	batch id:80	 loss:1.722938
[Test] epoch:40	batch id:90	 loss:1.598524
[Test] epoch:40	batch id:100	 loss:1.959504
[Test] epoch:40	batch id:110	 loss:1.612084
[Test] epoch:40	batch id:120	 loss:1.591570
[Test] epoch:40	batch id:130	 loss:1.651747
[Test] epoch:40	batch id:140	 loss:1.599793
[Test] epoch:40	batch id:150	 loss:2.187892
[Test] 40, loss: 1.646864, test acc: 0.860211,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:41	batch id:0	 lr:0.096363 loss:1.580580
[Train] epoch:41	batch id:10	 lr:0.096363 loss:1.586611
[Train] epoch:41	batch id:20	 lr:0.096363 loss:1.667166
[Train] epoch:41	batch id:30	 lr:0.096363 loss:1.646402
[Train] epoch:41	batch id:40	 lr:0.096363 loss:1.611803
[Train] epoch:41	batch id:50	 lr:0.096363 loss:1.695496
[Train] epoch:41	batch id:60	 lr:0.096363 loss:1.828289
[Train] epoch:41	batch id:70	 lr:0.096363 loss:1.696708
[Train] epoch:41	batch id:80	 lr:0.096363 loss:1.573779
[Train] epoch:41	batch id:90	 lr:0.096363 loss:1.481370
[Train] epoch:41	batch id:100	 lr:0.096363 loss:1.729131
[Train] epoch:41	batch id:110	 lr:0.096363 loss:1.945886
[Train] epoch:41	batch id:120	 lr:0.096363 loss:1.842390
[Train] epoch:41	batch id:130	 lr:0.096363 loss:1.601933
[Train] epoch:41	batch id:140	 lr:0.096363 loss:1.607252
[Train] epoch:41	batch id:150	 lr:0.096363 loss:1.915710
[Train] epoch:41	batch id:160	 lr:0.096363 loss:1.727180
[Train] epoch:41	batch id:170	 lr:0.096363 loss:1.898081
[Train] epoch:41	batch id:180	 lr:0.096363 loss:1.970309
[Train] epoch:41	batch id:190	 lr:0.096363 loss:1.746050
[Train] epoch:41	batch id:200	 lr:0.096363 loss:1.799900
[Train] epoch:41	batch id:210	 lr:0.096363 loss:1.778910
[Train] epoch:41	batch id:220	 lr:0.096363 loss:1.871009
[Train] epoch:41	batch id:230	 lr:0.096363 loss:1.478781
[Train] epoch:41	batch id:240	 lr:0.096363 loss:1.689729
[Train] epoch:41	batch id:250	 lr:0.096363 loss:1.807522
[Train] epoch:41	batch id:260	 lr:0.096363 loss:1.775063
[Train] epoch:41	batch id:270	 lr:0.096363 loss:1.822936
[Train] epoch:41	batch id:280	 lr:0.096363 loss:1.789768
[Train] epoch:41	batch id:290	 lr:0.096363 loss:1.783672
[Train] epoch:41	batch id:300	 lr:0.096363 loss:1.715908
[Train] 41, loss: 1.749184, train acc: 0.826344, 
[Test] epoch:41	batch id:0	 loss:1.373579
[Test] epoch:41	batch id:10	 loss:1.612421
[Test] epoch:41	batch id:20	 loss:1.550116
[Test] epoch:41	batch id:30	 loss:1.551502
[Test] epoch:41	batch id:40	 loss:1.631337
[Test] epoch:41	batch id:50	 loss:1.620916
[Test] epoch:41	batch id:60	 loss:1.349347
[Test] epoch:41	batch id:70	 loss:1.556084
[Test] epoch:41	batch id:80	 loss:1.721129
[Test] epoch:41	batch id:90	 loss:1.575787
[Test] epoch:41	batch id:100	 loss:2.118501
[Test] epoch:41	batch id:110	 loss:1.584155
[Test] epoch:41	batch id:120	 loss:1.510468
[Test] epoch:41	batch id:130	 loss:1.537647
[Test] epoch:41	batch id:140	 loss:1.584241
[Test] epoch:41	batch id:150	 loss:2.026690
[Test] 41, loss: 1.613182, test acc: 0.859806,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:42	batch id:0	 lr:0.096193 loss:1.872139
[Train] epoch:42	batch id:10	 lr:0.096193 loss:1.562155
[Train] epoch:42	batch id:20	 lr:0.096193 loss:1.665931
[Train] epoch:42	batch id:30	 lr:0.096193 loss:1.828017
[Train] epoch:42	batch id:40	 lr:0.096193 loss:1.689930
[Train] epoch:42	batch id:50	 lr:0.096193 loss:1.811755
[Train] epoch:42	batch id:60	 lr:0.096193 loss:1.683890
[Train] epoch:42	batch id:70	 lr:0.096193 loss:1.750248
[Train] epoch:42	batch id:80	 lr:0.096193 loss:1.886754
[Train] epoch:42	batch id:90	 lr:0.096193 loss:1.694700
[Train] epoch:42	batch id:100	 lr:0.096193 loss:1.625929
[Train] epoch:42	batch id:110	 lr:0.096193 loss:1.811031
[Train] epoch:42	batch id:120	 lr:0.096193 loss:1.747497
[Train] epoch:42	batch id:130	 lr:0.096193 loss:1.675288
[Train] epoch:42	batch id:140	 lr:0.096193 loss:1.592587
[Train] epoch:42	batch id:150	 lr:0.096193 loss:1.638353
[Train] epoch:42	batch id:160	 lr:0.096193 loss:1.931720
[Train] epoch:42	batch id:170	 lr:0.096193 loss:1.794506
[Train] epoch:42	batch id:180	 lr:0.096193 loss:1.673334
[Train] epoch:42	batch id:190	 lr:0.096193 loss:1.890261
[Train] epoch:42	batch id:200	 lr:0.096193 loss:1.698564
[Train] epoch:42	batch id:210	 lr:0.096193 loss:1.742758
[Train] epoch:42	batch id:220	 lr:0.096193 loss:1.876887
[Train] epoch:42	batch id:230	 lr:0.096193 loss:1.690459
[Train] epoch:42	batch id:240	 lr:0.096193 loss:1.572061
[Train] epoch:42	batch id:250	 lr:0.096193 loss:1.596536
[Train] epoch:42	batch id:260	 lr:0.096193 loss:1.642205
[Train] epoch:42	batch id:270	 lr:0.096193 loss:1.847551
[Train] epoch:42	batch id:280	 lr:0.096193 loss:1.669162
[Train] epoch:42	batch id:290	 lr:0.096193 loss:1.813076
[Train] epoch:42	batch id:300	 lr:0.096193 loss:1.868710
[Train] 42, loss: 1.745057, train acc: 0.830314, 
[Test] epoch:42	batch id:0	 loss:1.501816
[Test] epoch:42	batch id:10	 loss:1.654745
[Test] epoch:42	batch id:20	 loss:1.542292
[Test] epoch:42	batch id:30	 loss:1.541624
[Test] epoch:42	batch id:40	 loss:1.507479
[Test] epoch:42	batch id:50	 loss:1.588243
[Test] epoch:42	batch id:60	 loss:1.309542
[Test] epoch:42	batch id:70	 loss:1.449189
[Test] epoch:42	batch id:80	 loss:1.604738
[Test] epoch:42	batch id:90	 loss:1.524652
[Test] epoch:42	batch id:100	 loss:1.959648
[Test] epoch:42	batch id:110	 loss:1.434961
[Test] epoch:42	batch id:120	 loss:1.491727
[Test] epoch:42	batch id:130	 loss:1.549626
[Test] epoch:42	batch id:140	 loss:1.445988
[Test] epoch:42	batch id:150	 loss:1.862404
[Test] 42, loss: 1.575500, test acc: 0.878039,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:43	batch id:0	 lr:0.096021 loss:1.835015
[Train] epoch:43	batch id:10	 lr:0.096021 loss:1.779827
[Train] epoch:43	batch id:20	 lr:0.096021 loss:1.829904
[Train] epoch:43	batch id:30	 lr:0.096021 loss:1.876098
[Train] epoch:43	batch id:40	 lr:0.096021 loss:1.802112
[Train] epoch:43	batch id:50	 lr:0.096021 loss:1.569869
[Train] epoch:43	batch id:60	 lr:0.096021 loss:1.697951
[Train] epoch:43	batch id:70	 lr:0.096021 loss:1.794321
[Train] epoch:43	batch id:80	 lr:0.096021 loss:1.752185
[Train] epoch:43	batch id:90	 lr:0.096021 loss:1.762370
[Train] epoch:43	batch id:100	 lr:0.096021 loss:1.722775
[Train] epoch:43	batch id:110	 lr:0.096021 loss:1.650318
[Train] epoch:43	batch id:120	 lr:0.096021 loss:1.672843
[Train] epoch:43	batch id:130	 lr:0.096021 loss:1.945030
[Train] epoch:43	batch id:140	 lr:0.096021 loss:1.867431
[Train] epoch:43	batch id:150	 lr:0.096021 loss:1.749786
[Train] epoch:43	batch id:160	 lr:0.096021 loss:1.705138
[Train] epoch:43	batch id:170	 lr:0.096021 loss:1.891838
[Train] epoch:43	batch id:180	 lr:0.096021 loss:1.839406
[Train] epoch:43	batch id:190	 lr:0.096021 loss:1.810884
[Train] epoch:43	batch id:200	 lr:0.096021 loss:1.689607
[Train] epoch:43	batch id:210	 lr:0.096021 loss:1.569728
[Train] epoch:43	batch id:220	 lr:0.096021 loss:1.797422
[Train] epoch:43	batch id:230	 lr:0.096021 loss:1.644925
[Train] epoch:43	batch id:240	 lr:0.096021 loss:1.670741
[Train] epoch:43	batch id:250	 lr:0.096021 loss:1.586207
[Train] epoch:43	batch id:260	 lr:0.096021 loss:1.735500
[Train] epoch:43	batch id:270	 lr:0.096021 loss:1.812629
[Train] epoch:43	batch id:280	 lr:0.096021 loss:1.685731
[Train] epoch:43	batch id:290	 lr:0.096021 loss:1.820903
[Train] epoch:43	batch id:300	 lr:0.096021 loss:1.642853
[Train] 43, loss: 1.732811, train acc: 0.832960, 
[Test] epoch:43	batch id:0	 loss:1.416783
[Test] epoch:43	batch id:10	 loss:1.657493
[Test] epoch:43	batch id:20	 loss:1.567852
[Test] epoch:43	batch id:30	 loss:1.601901
[Test] epoch:43	batch id:40	 loss:1.598110
[Test] epoch:43	batch id:50	 loss:1.604656
[Test] epoch:43	batch id:60	 loss:1.346704
[Test] epoch:43	batch id:70	 loss:1.480889
[Test] epoch:43	batch id:80	 loss:1.632857
[Test] epoch:43	batch id:90	 loss:1.657692
[Test] epoch:43	batch id:100	 loss:2.168263
[Test] epoch:43	batch id:110	 loss:1.544569
[Test] epoch:43	batch id:120	 loss:1.622816
[Test] epoch:43	batch id:130	 loss:1.534627
[Test] epoch:43	batch id:140	 loss:1.515542
[Test] epoch:43	batch id:150	 loss:1.939131
[Test] 43, loss: 1.610467, test acc: 0.872366,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:44	batch id:0	 lr:0.095844 loss:1.696937
[Train] epoch:44	batch id:10	 lr:0.095844 loss:1.821673
[Train] epoch:44	batch id:20	 lr:0.095844 loss:1.685977
[Train] epoch:44	batch id:30	 lr:0.095844 loss:1.586973
[Train] epoch:44	batch id:40	 lr:0.095844 loss:1.678657
[Train] epoch:44	batch id:50	 lr:0.095844 loss:1.722794
[Train] epoch:44	batch id:60	 lr:0.095844 loss:1.663984
[Train] epoch:44	batch id:70	 lr:0.095844 loss:1.787503
[Train] epoch:44	batch id:80	 lr:0.095844 loss:1.744384
[Train] epoch:44	batch id:90	 lr:0.095844 loss:1.772190
[Train] epoch:44	batch id:100	 lr:0.095844 loss:1.800955
[Train] epoch:44	batch id:110	 lr:0.095844 loss:1.563236
[Train] epoch:44	batch id:120	 lr:0.095844 loss:1.719320
[Train] epoch:44	batch id:130	 lr:0.095844 loss:1.540418
[Train] epoch:44	batch id:140	 lr:0.095844 loss:1.807461
[Train] epoch:44	batch id:150	 lr:0.095844 loss:1.647467
[Train] epoch:44	batch id:160	 lr:0.095844 loss:1.735484
[Train] epoch:44	batch id:170	 lr:0.095844 loss:1.825362
[Train] epoch:44	batch id:180	 lr:0.095844 loss:1.733460
[Train] epoch:44	batch id:190	 lr:0.095844 loss:1.753737
[Train] epoch:44	batch id:200	 lr:0.095844 loss:1.605684
[Train] epoch:44	batch id:210	 lr:0.095844 loss:1.829859
[Train] epoch:44	batch id:220	 lr:0.095844 loss:1.885342
[Train] epoch:44	batch id:230	 lr:0.095844 loss:1.739623
[Train] epoch:44	batch id:240	 lr:0.095844 loss:1.650785
[Train] epoch:44	batch id:250	 lr:0.095844 loss:1.615331
[Train] epoch:44	batch id:260	 lr:0.095844 loss:1.684928
[Train] epoch:44	batch id:270	 lr:0.095844 loss:1.732170
[Train] epoch:44	batch id:280	 lr:0.095844 loss:1.717268
[Train] epoch:44	batch id:290	 lr:0.095844 loss:1.812115
[Train] epoch:44	batch id:300	 lr:0.095844 loss:1.794742
[Train] 44, loss: 1.733305, train acc: 0.835301, 
[Test] epoch:44	batch id:0	 loss:1.470306
[Test] epoch:44	batch id:10	 loss:1.553573
[Test] epoch:44	batch id:20	 loss:1.508706
[Test] epoch:44	batch id:30	 loss:1.507510
[Test] epoch:44	batch id:40	 loss:1.515477
[Test] epoch:44	batch id:50	 loss:1.587439
[Test] epoch:44	batch id:60	 loss:1.372046
[Test] epoch:44	batch id:70	 loss:1.530113
[Test] epoch:44	batch id:80	 loss:1.784096
[Test] epoch:44	batch id:90	 loss:1.518829
[Test] epoch:44	batch id:100	 loss:2.008306
[Test] epoch:44	batch id:110	 loss:1.518808
[Test] epoch:44	batch id:120	 loss:1.516247
[Test] epoch:44	batch id:130	 loss:1.623777
[Test] epoch:44	batch id:140	 loss:1.484947
[Test] epoch:44	batch id:150	 loss:2.021052
[Test] 44, loss: 1.605719, test acc: 0.880875,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:45	batch id:0	 lr:0.095664 loss:1.786200
[Train] epoch:45	batch id:10	 lr:0.095664 loss:1.864415
[Train] epoch:45	batch id:20	 lr:0.095664 loss:1.692424
[Train] epoch:45	batch id:30	 lr:0.095664 loss:1.911350
[Train] epoch:45	batch id:40	 lr:0.095664 loss:1.720315
[Train] epoch:45	batch id:50	 lr:0.095664 loss:1.577110
[Train] epoch:45	batch id:60	 lr:0.095664 loss:1.897238
[Train] epoch:45	batch id:70	 lr:0.095664 loss:1.736969
[Train] epoch:45	batch id:80	 lr:0.095664 loss:1.901071
[Train] epoch:45	batch id:90	 lr:0.095664 loss:1.707757
[Train] epoch:45	batch id:100	 lr:0.095664 loss:1.660489
[Train] epoch:45	batch id:110	 lr:0.095664 loss:1.899156
[Train] epoch:45	batch id:120	 lr:0.095664 loss:1.663919
[Train] epoch:45	batch id:130	 lr:0.095664 loss:1.601104
[Train] epoch:45	batch id:140	 lr:0.095664 loss:1.879288
[Train] epoch:45	batch id:150	 lr:0.095664 loss:1.712395
[Train] epoch:45	batch id:160	 lr:0.095664 loss:1.703119
[Train] epoch:45	batch id:170	 lr:0.095664 loss:1.783867
[Train] epoch:45	batch id:180	 lr:0.095664 loss:1.649097
[Train] epoch:45	batch id:190	 lr:0.095664 loss:1.703246
[Train] epoch:45	batch id:200	 lr:0.095664 loss:1.827936
[Train] epoch:45	batch id:210	 lr:0.095664 loss:1.759495
[Train] epoch:45	batch id:220	 lr:0.095664 loss:1.865233
[Train] epoch:45	batch id:230	 lr:0.095664 loss:1.731546
[Train] epoch:45	batch id:240	 lr:0.095664 loss:1.762573
[Train] epoch:45	batch id:250	 lr:0.095664 loss:1.670286
[Train] epoch:45	batch id:260	 lr:0.095664 loss:1.637769
[Train] epoch:45	batch id:270	 lr:0.095664 loss:1.952228
[Train] epoch:45	batch id:280	 lr:0.095664 loss:1.598015
[Train] epoch:45	batch id:290	 lr:0.095664 loss:1.565404
[Train] epoch:45	batch id:300	 lr:0.095664 loss:1.954936
[Train] 45, loss: 1.735836, train acc: 0.834182, 
[Test] epoch:45	batch id:0	 loss:1.392349
[Test] epoch:45	batch id:10	 loss:1.561004
[Test] epoch:45	batch id:20	 loss:1.491356
[Test] epoch:45	batch id:30	 loss:1.572486
[Test] epoch:45	batch id:40	 loss:1.574167
[Test] epoch:45	batch id:50	 loss:1.663101
[Test] epoch:45	batch id:60	 loss:1.389438
[Test] epoch:45	batch id:70	 loss:1.625427
[Test] epoch:45	batch id:80	 loss:1.767914
[Test] epoch:45	batch id:90	 loss:1.530176
[Test] epoch:45	batch id:100	 loss:2.202808
[Test] epoch:45	batch id:110	 loss:1.476814
[Test] epoch:45	batch id:120	 loss:1.499763
[Test] epoch:45	batch id:130	 loss:1.593561
[Test] epoch:45	batch id:140	 loss:1.429140
[Test] epoch:45	batch id:150	 loss:1.840257
[Test] 45, loss: 1.624783, test acc: 0.856564,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:46	batch id:0	 lr:0.095480 loss:1.624982
[Train] epoch:46	batch id:10	 lr:0.095480 loss:1.591478
[Train] epoch:46	batch id:20	 lr:0.095480 loss:1.736492
[Train] epoch:46	batch id:30	 lr:0.095480 loss:1.695631
[Train] epoch:46	batch id:40	 lr:0.095480 loss:1.866396
[Train] epoch:46	batch id:50	 lr:0.095480 loss:1.623993
[Train] epoch:46	batch id:60	 lr:0.095480 loss:1.631513
[Train] epoch:46	batch id:70	 lr:0.095480 loss:1.985778
[Train] epoch:46	batch id:80	 lr:0.095480 loss:1.599781
[Train] epoch:46	batch id:90	 lr:0.095480 loss:1.797469
[Train] epoch:46	batch id:100	 lr:0.095480 loss:1.698097
[Train] epoch:46	batch id:110	 lr:0.095480 loss:1.584682
[Train] epoch:46	batch id:120	 lr:0.095480 loss:2.060807
[Train] epoch:46	batch id:130	 lr:0.095480 loss:1.734562
[Train] epoch:46	batch id:140	 lr:0.095480 loss:1.596569
[Train] epoch:46	batch id:150	 lr:0.095480 loss:1.619276
[Train] epoch:46	batch id:160	 lr:0.095480 loss:1.639161
[Train] epoch:46	batch id:170	 lr:0.095480 loss:1.632450
[Train] epoch:46	batch id:180	 lr:0.095480 loss:1.847173
[Train] epoch:46	batch id:190	 lr:0.095480 loss:1.627887
[Train] epoch:46	batch id:200	 lr:0.095480 loss:1.603303
[Train] epoch:46	batch id:210	 lr:0.095480 loss:1.784551
[Train] epoch:46	batch id:220	 lr:0.095480 loss:1.774581
[Train] epoch:46	batch id:230	 lr:0.095480 loss:1.738838
[Train] epoch:46	batch id:240	 lr:0.095480 loss:1.731563
[Train] epoch:46	batch id:250	 lr:0.095480 loss:1.759567
[Train] epoch:46	batch id:260	 lr:0.095480 loss:1.771160
[Train] epoch:46	batch id:270	 lr:0.095480 loss:1.667757
[Train] epoch:46	batch id:280	 lr:0.095480 loss:1.632733
[Train] epoch:46	batch id:290	 lr:0.095480 loss:1.625986
[Train] epoch:46	batch id:300	 lr:0.095480 loss:1.710154
[Train] 46, loss: 1.715789, train acc: 0.844157, 
[Test] epoch:46	batch id:0	 loss:1.382573
[Test] epoch:46	batch id:10	 loss:1.621450
[Test] epoch:46	batch id:20	 loss:1.421094
[Test] epoch:46	batch id:30	 loss:1.508662
[Test] epoch:46	batch id:40	 loss:1.596321
[Test] epoch:46	batch id:50	 loss:1.475438
[Test] epoch:46	batch id:60	 loss:1.337920
[Test] epoch:46	batch id:70	 loss:1.538706
[Test] epoch:46	batch id:80	 loss:1.757364
[Test] epoch:46	batch id:90	 loss:1.592927
[Test] epoch:46	batch id:100	 loss:2.099367
[Test] epoch:46	batch id:110	 loss:1.424025
[Test] epoch:46	batch id:120	 loss:1.525597
[Test] epoch:46	batch id:130	 loss:1.548380
[Test] epoch:46	batch id:140	 loss:1.533640
[Test] epoch:46	batch id:150	 loss:2.089171
[Test] 46, loss: 1.585302, test acc: 0.884522,
Max Acc:0.884522
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:47	batch id:0	 lr:0.095293 loss:1.710344
[Train] epoch:47	batch id:10	 lr:0.095293 loss:1.721369
[Train] epoch:47	batch id:20	 lr:0.095293 loss:1.593957
[Train] epoch:47	batch id:30	 lr:0.095293 loss:1.651375
[Train] epoch:47	batch id:40	 lr:0.095293 loss:1.691556
[Train] epoch:47	batch id:50	 lr:0.095293 loss:1.664948
[Train] epoch:47	batch id:60	 lr:0.095293 loss:1.915828
[Train] epoch:47	batch id:70	 lr:0.095293 loss:1.734517
[Train] epoch:47	batch id:80	 lr:0.095293 loss:1.644715
[Train] epoch:47	batch id:90	 lr:0.095293 loss:1.648606
[Train] epoch:47	batch id:100	 lr:0.095293 loss:1.905258
[Train] epoch:47	batch id:110	 lr:0.095293 loss:1.731505
[Train] epoch:47	batch id:120	 lr:0.095293 loss:1.658576
[Train] epoch:47	batch id:130	 lr:0.095293 loss:1.655439
[Train] epoch:47	batch id:140	 lr:0.095293 loss:1.835800
[Train] epoch:47	batch id:150	 lr:0.095293 loss:1.714793
[Train] epoch:47	batch id:160	 lr:0.095293 loss:1.599323
[Train] epoch:47	batch id:170	 lr:0.095293 loss:1.752587
[Train] epoch:47	batch id:180	 lr:0.095293 loss:1.758201
[Train] epoch:47	batch id:190	 lr:0.095293 loss:1.881663
[Train] epoch:47	batch id:200	 lr:0.095293 loss:1.754808
[Train] epoch:47	batch id:210	 lr:0.095293 loss:1.747938
[Train] epoch:47	batch id:220	 lr:0.095293 loss:1.699353
[Train] epoch:47	batch id:230	 lr:0.095293 loss:1.719785
[Train] epoch:47	batch id:240	 lr:0.095293 loss:1.823653
[Train] epoch:47	batch id:250	 lr:0.095293 loss:1.490067
[Train] epoch:47	batch id:260	 lr:0.095293 loss:1.518548
[Train] epoch:47	batch id:270	 lr:0.095293 loss:1.788365
[Train] epoch:47	batch id:280	 lr:0.095293 loss:1.704608
[Train] epoch:47	batch id:290	 lr:0.095293 loss:1.749779
[Train] epoch:47	batch id:300	 lr:0.095293 loss:1.711049
[Train] 47, loss: 1.720626, train acc: 0.836625, 
[Test] epoch:47	batch id:0	 loss:1.509255
[Test] epoch:47	batch id:10	 loss:1.912468
[Test] epoch:47	batch id:20	 loss:1.569983
[Test] epoch:47	batch id:30	 loss:1.654373
[Test] epoch:47	batch id:40	 loss:1.643826
[Test] epoch:47	batch id:50	 loss:1.575283
[Test] epoch:47	batch id:60	 loss:1.322559
[Test] epoch:47	batch id:70	 loss:1.658154
[Test] epoch:47	batch id:80	 loss:1.838476
[Test] epoch:47	batch id:90	 loss:1.762959
[Test] epoch:47	batch id:100	 loss:2.101254
[Test] epoch:47	batch id:110	 loss:1.544589
[Test] epoch:47	batch id:120	 loss:1.689167
[Test] epoch:47	batch id:130	 loss:1.501567
[Test] epoch:47	batch id:140	 loss:1.569001
[Test] epoch:47	batch id:150	 loss:2.032601
[Test] 47, loss: 1.659381, test acc: 0.848055,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:48	batch id:0	 lr:0.095102 loss:1.795074
[Train] epoch:48	batch id:10	 lr:0.095102 loss:1.585589
[Train] epoch:48	batch id:20	 lr:0.095102 loss:1.837313
[Train] epoch:48	batch id:30	 lr:0.095102 loss:1.638842
[Train] epoch:48	batch id:40	 lr:0.095102 loss:1.659125
[Train] epoch:48	batch id:50	 lr:0.095102 loss:2.007563
[Train] epoch:48	batch id:60	 lr:0.095102 loss:1.725387
[Train] epoch:48	batch id:70	 lr:0.095102 loss:1.715788
[Train] epoch:48	batch id:80	 lr:0.095102 loss:1.783562
[Train] epoch:48	batch id:90	 lr:0.095102 loss:1.805853
[Train] epoch:48	batch id:100	 lr:0.095102 loss:1.628267
[Train] epoch:48	batch id:110	 lr:0.095102 loss:1.808190
[Train] epoch:48	batch id:120	 lr:0.095102 loss:1.859052
[Train] epoch:48	batch id:130	 lr:0.095102 loss:1.788934
[Train] epoch:48	batch id:140	 lr:0.095102 loss:1.729387
[Train] epoch:48	batch id:150	 lr:0.095102 loss:1.644033
[Train] epoch:48	batch id:160	 lr:0.095102 loss:1.747399
[Train] epoch:48	batch id:170	 lr:0.095102 loss:1.659644
[Train] epoch:48	batch id:180	 lr:0.095102 loss:1.748511
[Train] epoch:48	batch id:190	 lr:0.095102 loss:1.700482
[Train] epoch:48	batch id:200	 lr:0.095102 loss:1.919719
[Train] epoch:48	batch id:210	 lr:0.095102 loss:1.754616
[Train] epoch:48	batch id:220	 lr:0.095102 loss:1.597170
[Train] epoch:48	batch id:230	 lr:0.095102 loss:1.849445
[Train] epoch:48	batch id:240	 lr:0.095102 loss:1.617080
[Train] epoch:48	batch id:250	 lr:0.095102 loss:1.765946
[Train] epoch:48	batch id:260	 lr:0.095102 loss:1.735188
[Train] epoch:48	batch id:270	 lr:0.095102 loss:1.518956
[Train] epoch:48	batch id:280	 lr:0.095102 loss:1.653194
[Train] epoch:48	batch id:290	 lr:0.095102 loss:1.682824
[Train] epoch:48	batch id:300	 lr:0.095102 loss:1.732255
[Train] 48, loss: 1.712263, train acc: 0.844971, 
[Test] epoch:48	batch id:0	 loss:1.357939
[Test] epoch:48	batch id:10	 loss:1.578077
[Test] epoch:48	batch id:20	 loss:1.473536
[Test] epoch:48	batch id:30	 loss:1.627670
[Test] epoch:48	batch id:40	 loss:1.538314
[Test] epoch:48	batch id:50	 loss:1.532013
[Test] epoch:48	batch id:60	 loss:1.334774
[Test] epoch:48	batch id:70	 loss:1.440073
[Test] epoch:48	batch id:80	 loss:1.707134
[Test] epoch:48	batch id:90	 loss:1.562402
[Test] epoch:48	batch id:100	 loss:1.907898
[Test] epoch:48	batch id:110	 loss:1.399880
[Test] epoch:48	batch id:120	 loss:1.539891
[Test] epoch:48	batch id:130	 loss:1.548058
[Test] epoch:48	batch id:140	 loss:1.449226
[Test] epoch:48	batch id:150	 loss:2.035441
[Test] 48, loss: 1.597752, test acc: 0.873177,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:49	batch id:0	 lr:0.094907 loss:1.707398
[Train] epoch:49	batch id:10	 lr:0.094907 loss:1.601430
[Train] epoch:49	batch id:20	 lr:0.094907 loss:1.891710
[Train] epoch:49	batch id:30	 lr:0.094907 loss:1.696108
[Train] epoch:49	batch id:40	 lr:0.094907 loss:1.743280
[Train] epoch:49	batch id:50	 lr:0.094907 loss:1.566811
[Train] epoch:49	batch id:60	 lr:0.094907 loss:1.790817
[Train] epoch:49	batch id:70	 lr:0.094907 loss:1.603689
[Train] epoch:49	batch id:80	 lr:0.094907 loss:1.624791
[Train] epoch:49	batch id:90	 lr:0.094907 loss:1.915430
[Train] epoch:49	batch id:100	 lr:0.094907 loss:1.593620
[Train] epoch:49	batch id:110	 lr:0.094907 loss:1.645562
[Train] epoch:49	batch id:120	 lr:0.094907 loss:1.595264
[Train] epoch:49	batch id:130	 lr:0.094907 loss:1.745759
[Train] epoch:49	batch id:140	 lr:0.094907 loss:1.626315
[Train] epoch:49	batch id:150	 lr:0.094907 loss:1.505510
[Train] epoch:49	batch id:160	 lr:0.094907 loss:1.592695
[Train] epoch:49	batch id:170	 lr:0.094907 loss:1.844874
[Train] epoch:49	batch id:180	 lr:0.094907 loss:1.528527
[Train] epoch:49	batch id:190	 lr:0.094907 loss:1.824337
[Train] epoch:49	batch id:200	 lr:0.094907 loss:1.716470
[Train] epoch:49	batch id:210	 lr:0.094907 loss:1.813683
[Train] epoch:49	batch id:220	 lr:0.094907 loss:1.424822
[Train] epoch:49	batch id:230	 lr:0.094907 loss:1.573681
[Train] epoch:49	batch id:240	 lr:0.094907 loss:1.732783
[Train] epoch:49	batch id:250	 lr:0.094907 loss:1.681585
[Train] epoch:49	batch id:260	 lr:0.094907 loss:1.682062
[Train] epoch:49	batch id:270	 lr:0.094907 loss:1.602946
[Train] epoch:49	batch id:280	 lr:0.094907 loss:1.711774
[Train] epoch:49	batch id:290	 lr:0.094907 loss:1.604494
[Train] epoch:49	batch id:300	 lr:0.094907 loss:1.777841
[Train] 49, loss: 1.709818, train acc: 0.847109, 
[Test] epoch:49	batch id:0	 loss:1.435848
[Test] epoch:49	batch id:10	 loss:1.560324
[Test] epoch:49	batch id:20	 loss:1.516599
[Test] epoch:49	batch id:30	 loss:1.690753
[Test] epoch:49	batch id:40	 loss:1.496013
[Test] epoch:49	batch id:50	 loss:1.601863
[Test] epoch:49	batch id:60	 loss:1.404804
[Test] epoch:49	batch id:70	 loss:1.579278
[Test] epoch:49	batch id:80	 loss:1.672865
[Test] epoch:49	batch id:90	 loss:1.701693
[Test] epoch:49	batch id:100	 loss:2.057063
[Test] epoch:49	batch id:110	 loss:1.546489
[Test] epoch:49	batch id:120	 loss:1.549361
[Test] epoch:49	batch id:130	 loss:1.588306
[Test] epoch:49	batch id:140	 loss:1.478512
[Test] epoch:49	batch id:150	 loss:2.071937
[Test] 49, loss: 1.623625, test acc: 0.882091,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:50	batch id:0	 lr:0.094709 loss:1.623268
[Train] epoch:50	batch id:10	 lr:0.094709 loss:1.674577
[Train] epoch:50	batch id:20	 lr:0.094709 loss:1.742459
[Train] epoch:50	batch id:30	 lr:0.094709 loss:1.598681
[Train] epoch:50	batch id:40	 lr:0.094709 loss:1.808712
[Train] epoch:50	batch id:50	 lr:0.094709 loss:1.689377
[Train] epoch:50	batch id:60	 lr:0.094709 loss:1.681354
[Train] epoch:50	batch id:70	 lr:0.094709 loss:1.652562
[Train] epoch:50	batch id:80	 lr:0.094709 loss:1.713571
[Train] epoch:50	batch id:90	 lr:0.094709 loss:1.870435
[Train] epoch:50	batch id:100	 lr:0.094709 loss:1.656353
[Train] epoch:50	batch id:110	 lr:0.094709 loss:1.654245
[Train] epoch:50	batch id:120	 lr:0.094709 loss:1.640592
[Train] epoch:50	batch id:130	 lr:0.094709 loss:1.513751
[Train] epoch:50	batch id:140	 lr:0.094709 loss:1.687780
[Train] epoch:50	batch id:150	 lr:0.094709 loss:1.594612
[Train] epoch:50	batch id:160	 lr:0.094709 loss:1.632396
[Train] epoch:50	batch id:170	 lr:0.094709 loss:1.574601
[Train] epoch:50	batch id:180	 lr:0.094709 loss:1.615211
[Train] epoch:50	batch id:190	 lr:0.094709 loss:1.811180
[Train] epoch:50	batch id:200	 lr:0.094709 loss:1.812056
[Train] epoch:50	batch id:210	 lr:0.094709 loss:1.773960
[Train] epoch:50	batch id:220	 lr:0.094709 loss:1.844126
[Train] epoch:50	batch id:230	 lr:0.094709 loss:1.774410
[Train] epoch:50	batch id:240	 lr:0.094709 loss:1.786857
[Train] epoch:50	batch id:250	 lr:0.094709 loss:1.655831
[Train] epoch:50	batch id:260	 lr:0.094709 loss:1.675768
[Train] epoch:50	batch id:270	 lr:0.094709 loss:1.625022
[Train] epoch:50	batch id:280	 lr:0.094709 loss:1.584096
[Train] epoch:50	batch id:290	 lr:0.094709 loss:1.554710
[Train] epoch:50	batch id:300	 lr:0.094709 loss:1.732562
[Train] 50, loss: 1.710939, train acc: 0.846498, 
[Test] epoch:50	batch id:0	 loss:1.412293
[Test] epoch:50	batch id:10	 loss:1.581930
[Test] epoch:50	batch id:20	 loss:1.565032
[Test] epoch:50	batch id:30	 loss:1.487595
[Test] epoch:50	batch id:40	 loss:1.551039
[Test] epoch:50	batch id:50	 loss:1.720186
[Test] epoch:50	batch id:60	 loss:1.334553
[Test] epoch:50	batch id:70	 loss:1.571051
[Test] epoch:50	batch id:80	 loss:1.644416
[Test] epoch:50	batch id:90	 loss:1.650340
[Test] epoch:50	batch id:100	 loss:1.882837
[Test] epoch:50	batch id:110	 loss:1.681542
[Test] epoch:50	batch id:120	 loss:1.506076
[Test] epoch:50	batch id:130	 loss:1.620917
[Test] epoch:50	batch id:140	 loss:1.590138
[Test] epoch:50	batch id:150	 loss:1.851472
[Test] 50, loss: 1.604291, test acc: 0.884117,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:51	batch id:0	 lr:0.094508 loss:1.625394
[Train] epoch:51	batch id:10	 lr:0.094508 loss:1.653839
[Train] epoch:51	batch id:20	 lr:0.094508 loss:1.683609
[Train] epoch:51	batch id:30	 lr:0.094508 loss:1.874220
[Train] epoch:51	batch id:40	 lr:0.094508 loss:1.656573
[Train] epoch:51	batch id:50	 lr:0.094508 loss:1.762429
[Train] epoch:51	batch id:60	 lr:0.094508 loss:1.667295
[Train] epoch:51	batch id:70	 lr:0.094508 loss:1.764436
[Train] epoch:51	batch id:80	 lr:0.094508 loss:1.550607
[Train] epoch:51	batch id:90	 lr:0.094508 loss:1.782914
[Train] epoch:51	batch id:100	 lr:0.094508 loss:1.672859
[Train] epoch:51	batch id:110	 lr:0.094508 loss:1.777107
[Train] epoch:51	batch id:120	 lr:0.094508 loss:1.533096
[Train] epoch:51	batch id:130	 lr:0.094508 loss:1.867772
[Train] epoch:51	batch id:140	 lr:0.094508 loss:1.834107
[Train] epoch:51	batch id:150	 lr:0.094508 loss:1.805236
[Train] epoch:51	batch id:160	 lr:0.094508 loss:1.782879
[Train] epoch:51	batch id:170	 lr:0.094508 loss:1.534520
[Train] epoch:51	batch id:180	 lr:0.094508 loss:1.605871
[Train] epoch:51	batch id:190	 lr:0.094508 loss:1.598482
[Train] epoch:51	batch id:200	 lr:0.094508 loss:1.706964
[Train] epoch:51	batch id:210	 lr:0.094508 loss:1.984381
[Train] epoch:51	batch id:220	 lr:0.094508 loss:1.682738
[Train] epoch:51	batch id:230	 lr:0.094508 loss:1.787588
[Train] epoch:51	batch id:240	 lr:0.094508 loss:1.487592
[Train] epoch:51	batch id:250	 lr:0.094508 loss:1.656714
[Train] epoch:51	batch id:260	 lr:0.094508 loss:1.707013
[Train] epoch:51	batch id:270	 lr:0.094508 loss:1.688957
[Train] epoch:51	batch id:280	 lr:0.094508 loss:1.810485
[Train] epoch:51	batch id:290	 lr:0.094508 loss:1.592214
[Train] epoch:51	batch id:300	 lr:0.094508 loss:1.799978
[Train] 51, loss: 1.701848, train acc: 0.848127, 
[Test] epoch:51	batch id:0	 loss:1.374399
[Test] epoch:51	batch id:10	 loss:1.716652
[Test] epoch:51	batch id:20	 loss:1.507620
[Test] epoch:51	batch id:30	 loss:1.580294
[Test] epoch:51	batch id:40	 loss:1.584399
[Test] epoch:51	batch id:50	 loss:1.533474
[Test] epoch:51	batch id:60	 loss:1.321181
[Test] epoch:51	batch id:70	 loss:1.507294
[Test] epoch:51	batch id:80	 loss:1.671522
[Test] epoch:51	batch id:90	 loss:1.580962
[Test] epoch:51	batch id:100	 loss:2.030068
[Test] epoch:51	batch id:110	 loss:1.641632
[Test] epoch:51	batch id:120	 loss:1.636675
[Test] epoch:51	batch id:130	 loss:1.602812
[Test] epoch:51	batch id:140	 loss:1.564640
[Test] epoch:51	batch id:150	 loss:2.003311
[Test] 51, loss: 1.609222, test acc: 0.860616,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:52	batch id:0	 lr:0.094302 loss:1.600816
[Train] epoch:52	batch id:10	 lr:0.094302 loss:1.765772
[Train] epoch:52	batch id:20	 lr:0.094302 loss:1.838494
[Train] epoch:52	batch id:30	 lr:0.094302 loss:1.635159
[Train] epoch:52	batch id:40	 lr:0.094302 loss:1.674329
[Train] epoch:52	batch id:50	 lr:0.094302 loss:1.696974
[Train] epoch:52	batch id:60	 lr:0.094302 loss:1.907835
[Train] epoch:52	batch id:70	 lr:0.094302 loss:1.635093
[Train] epoch:52	batch id:80	 lr:0.094302 loss:1.659588
[Train] epoch:52	batch id:90	 lr:0.094302 loss:1.698820
[Train] epoch:52	batch id:100	 lr:0.094302 loss:1.678490
[Train] epoch:52	batch id:110	 lr:0.094302 loss:1.527245
[Train] epoch:52	batch id:120	 lr:0.094302 loss:1.766182
[Train] epoch:52	batch id:130	 lr:0.094302 loss:1.805130
[Train] epoch:52	batch id:140	 lr:0.094302 loss:1.750146
[Train] epoch:52	batch id:150	 lr:0.094302 loss:1.741729
[Train] epoch:52	batch id:160	 lr:0.094302 loss:1.801049
[Train] epoch:52	batch id:170	 lr:0.094302 loss:1.766127
[Train] epoch:52	batch id:180	 lr:0.094302 loss:1.682272
[Train] epoch:52	batch id:190	 lr:0.094302 loss:1.660645
[Train] epoch:52	batch id:200	 lr:0.094302 loss:1.720252
[Train] epoch:52	batch id:210	 lr:0.094302 loss:1.636934
[Train] epoch:52	batch id:220	 lr:0.094302 loss:1.667079
[Train] epoch:52	batch id:230	 lr:0.094302 loss:1.699894
[Train] epoch:52	batch id:240	 lr:0.094302 loss:1.690796
[Train] epoch:52	batch id:250	 lr:0.094302 loss:1.650920
[Train] epoch:52	batch id:260	 lr:0.094302 loss:1.773605
[Train] epoch:52	batch id:270	 lr:0.094302 loss:1.795106
[Train] epoch:52	batch id:280	 lr:0.094302 loss:1.808973
[Train] epoch:52	batch id:290	 lr:0.094302 loss:1.859540
[Train] epoch:52	batch id:300	 lr:0.094302 loss:1.708086
[Train] 52, loss: 1.711007, train acc: 0.841612, 
[Test] epoch:52	batch id:0	 loss:1.379343
[Test] epoch:52	batch id:10	 loss:1.541568
[Test] epoch:52	batch id:20	 loss:1.531276
[Test] epoch:52	batch id:30	 loss:1.459708
[Test] epoch:52	batch id:40	 loss:1.592364
[Test] epoch:52	batch id:50	 loss:1.555471
[Test] epoch:52	batch id:60	 loss:1.370901
[Test] epoch:52	batch id:70	 loss:1.501896
[Test] epoch:52	batch id:80	 loss:1.589857
[Test] epoch:52	batch id:90	 loss:1.526372
[Test] epoch:52	batch id:100	 loss:1.895917
[Test] epoch:52	batch id:110	 loss:1.606858
[Test] epoch:52	batch id:120	 loss:1.498517
[Test] epoch:52	batch id:130	 loss:1.616269
[Test] epoch:52	batch id:140	 loss:1.460366
[Test] epoch:52	batch id:150	 loss:2.003375
[Test] 52, loss: 1.582137, test acc: 0.882496,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:53	batch id:0	 lr:0.094093 loss:1.832453
[Train] epoch:53	batch id:10	 lr:0.094093 loss:1.626163
[Train] epoch:53	batch id:20	 lr:0.094093 loss:1.625799
[Train] epoch:53	batch id:30	 lr:0.094093 loss:1.725235
[Train] epoch:53	batch id:40	 lr:0.094093 loss:1.714936
[Train] epoch:53	batch id:50	 lr:0.094093 loss:1.688633
[Train] epoch:53	batch id:60	 lr:0.094093 loss:1.674930
[Train] epoch:53	batch id:70	 lr:0.094093 loss:1.818254
[Train] epoch:53	batch id:80	 lr:0.094093 loss:1.693193
[Train] epoch:53	batch id:90	 lr:0.094093 loss:1.485073
[Train] epoch:53	batch id:100	 lr:0.094093 loss:1.518863
[Train] epoch:53	batch id:110	 lr:0.094093 loss:1.807772
[Train] epoch:53	batch id:120	 lr:0.094093 loss:1.679219
[Train] epoch:53	batch id:130	 lr:0.094093 loss:1.755774
[Train] epoch:53	batch id:140	 lr:0.094093 loss:1.589292
[Train] epoch:53	batch id:150	 lr:0.094093 loss:1.617008
[Train] epoch:53	batch id:160	 lr:0.094093 loss:1.608706
[Train] epoch:53	batch id:170	 lr:0.094093 loss:1.876091
[Train] epoch:53	batch id:180	 lr:0.094093 loss:1.509957
[Train] epoch:53	batch id:190	 lr:0.094093 loss:1.575824
[Train] epoch:53	batch id:200	 lr:0.094093 loss:1.784550
[Train] epoch:53	batch id:210	 lr:0.094093 loss:1.637875
[Train] epoch:53	batch id:220	 lr:0.094093 loss:1.729875
[Train] epoch:53	batch id:230	 lr:0.094093 loss:1.647764
[Train] epoch:53	batch id:240	 lr:0.094093 loss:1.749754
[Train] epoch:53	batch id:250	 lr:0.094093 loss:1.614534
[Train] epoch:53	batch id:260	 lr:0.094093 loss:1.576958
[Train] epoch:53	batch id:270	 lr:0.094093 loss:1.641045
[Train] epoch:53	batch id:280	 lr:0.094093 loss:1.813675
[Train] epoch:53	batch id:290	 lr:0.094093 loss:1.745581
[Train] epoch:53	batch id:300	 lr:0.094093 loss:1.463406
[Train] 53, loss: 1.695932, train acc: 0.851893, 
[Test] epoch:53	batch id:0	 loss:1.406933
[Test] epoch:53	batch id:10	 loss:1.590740
[Test] epoch:53	batch id:20	 loss:1.518027
[Test] epoch:53	batch id:30	 loss:1.529475
[Test] epoch:53	batch id:40	 loss:1.473058
[Test] epoch:53	batch id:50	 loss:1.531528
[Test] epoch:53	batch id:60	 loss:1.342747
[Test] epoch:53	batch id:70	 loss:1.498758
[Test] epoch:53	batch id:80	 loss:1.752573
[Test] epoch:53	batch id:90	 loss:1.532159
[Test] epoch:53	batch id:100	 loss:2.077780
[Test] epoch:53	batch id:110	 loss:1.394808
[Test] epoch:53	batch id:120	 loss:1.466439
[Test] epoch:53	batch id:130	 loss:1.509313
[Test] epoch:53	batch id:140	 loss:1.535171
[Test] epoch:53	batch id:150	 loss:2.054229
[Test] 53, loss: 1.584516, test acc: 0.880065,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:54	batch id:0	 lr:0.093881 loss:1.734918
[Train] epoch:54	batch id:10	 lr:0.093881 loss:1.735278
[Train] epoch:54	batch id:20	 lr:0.093881 loss:1.730860
[Train] epoch:54	batch id:30	 lr:0.093881 loss:1.864380
[Train] epoch:54	batch id:40	 lr:0.093881 loss:1.739290
[Train] epoch:54	batch id:50	 lr:0.093881 loss:1.819999
[Train] epoch:54	batch id:60	 lr:0.093881 loss:1.602805
[Train] epoch:54	batch id:70	 lr:0.093881 loss:1.561735
[Train] epoch:54	batch id:80	 lr:0.093881 loss:1.611107
[Train] epoch:54	batch id:90	 lr:0.093881 loss:1.503330
[Train] epoch:54	batch id:100	 lr:0.093881 loss:1.878671
[Train] epoch:54	batch id:110	 lr:0.093881 loss:1.745930
[Train] epoch:54	batch id:120	 lr:0.093881 loss:1.607748
[Train] epoch:54	batch id:130	 lr:0.093881 loss:1.597830
[Train] epoch:54	batch id:140	 lr:0.093881 loss:1.538862
[Train] epoch:54	batch id:150	 lr:0.093881 loss:1.841537
[Train] epoch:54	batch id:160	 lr:0.093881 loss:1.685395
[Train] epoch:54	batch id:170	 lr:0.093881 loss:1.676169
[Train] epoch:54	batch id:180	 lr:0.093881 loss:1.565340
[Train] epoch:54	batch id:190	 lr:0.093881 loss:1.478470
[Train] epoch:54	batch id:200	 lr:0.093881 loss:1.580014
[Train] epoch:54	batch id:210	 lr:0.093881 loss:1.805134
[Train] epoch:54	batch id:220	 lr:0.093881 loss:1.603915
[Train] epoch:54	batch id:230	 lr:0.093881 loss:1.799487
[Train] epoch:54	batch id:240	 lr:0.093881 loss:1.635504
[Train] epoch:54	batch id:250	 lr:0.093881 loss:1.615025
[Train] epoch:54	batch id:260	 lr:0.093881 loss:1.907984
[Train] epoch:54	batch id:270	 lr:0.093881 loss:1.759297
[Train] epoch:54	batch id:280	 lr:0.093881 loss:1.534936
[Train] epoch:54	batch id:290	 lr:0.093881 loss:1.741011
[Train] epoch:54	batch id:300	 lr:0.093881 loss:1.727762
[Train] 54, loss: 1.699506, train acc: 0.846906, 
[Test] epoch:54	batch id:0	 loss:1.405822
[Test] epoch:54	batch id:10	 loss:1.512012
[Test] epoch:54	batch id:20	 loss:1.531585
[Test] epoch:54	batch id:30	 loss:1.489954
[Test] epoch:54	batch id:40	 loss:1.528218
[Test] epoch:54	batch id:50	 loss:1.456623
[Test] epoch:54	batch id:60	 loss:1.347668
[Test] epoch:54	batch id:70	 loss:1.433020
[Test] epoch:54	batch id:80	 loss:1.514450
[Test] epoch:54	batch id:90	 loss:1.455445
[Test] epoch:54	batch id:100	 loss:1.931466
[Test] epoch:54	batch id:110	 loss:1.507356
[Test] epoch:54	batch id:120	 loss:1.486615
[Test] epoch:54	batch id:130	 loss:1.558395
[Test] epoch:54	batch id:140	 loss:1.484790
[Test] epoch:54	batch id:150	 loss:1.918837
[Test] 54, loss: 1.566074, test acc: 0.895867,
Max Acc:0.895867
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:55	batch id:0	 lr:0.093665 loss:1.690729
[Train] epoch:55	batch id:10	 lr:0.093665 loss:1.556184
[Train] epoch:55	batch id:20	 lr:0.093665 loss:1.549242
[Train] epoch:55	batch id:30	 lr:0.093665 loss:1.828701
[Train] epoch:55	batch id:40	 lr:0.093665 loss:1.859450
[Train] epoch:55	batch id:50	 lr:0.093665 loss:1.769361
[Train] epoch:55	batch id:60	 lr:0.093665 loss:1.736382
[Train] epoch:55	batch id:70	 lr:0.093665 loss:1.838857
[Train] epoch:55	batch id:80	 lr:0.093665 loss:1.618379
[Train] epoch:55	batch id:90	 lr:0.093665 loss:1.806041
[Train] epoch:55	batch id:100	 lr:0.093665 loss:1.753896
[Train] epoch:55	batch id:110	 lr:0.093665 loss:1.585936
[Train] epoch:55	batch id:120	 lr:0.093665 loss:1.610800
[Train] epoch:55	batch id:130	 lr:0.093665 loss:1.515777
[Train] epoch:55	batch id:140	 lr:0.093665 loss:1.551154
[Train] epoch:55	batch id:150	 lr:0.093665 loss:1.824551
[Train] epoch:55	batch id:160	 lr:0.093665 loss:1.742994
[Train] epoch:55	batch id:170	 lr:0.093665 loss:1.520374
[Train] epoch:55	batch id:180	 lr:0.093665 loss:1.602045
[Train] epoch:55	batch id:190	 lr:0.093665 loss:1.695385
[Train] epoch:55	batch id:200	 lr:0.093665 loss:1.713940
[Train] epoch:55	batch id:210	 lr:0.093665 loss:1.708157
[Train] epoch:55	batch id:220	 lr:0.093665 loss:1.537729
[Train] epoch:55	batch id:230	 lr:0.093665 loss:1.729125
[Train] epoch:55	batch id:240	 lr:0.093665 loss:1.668363
[Train] epoch:55	batch id:250	 lr:0.093665 loss:1.698229
[Train] epoch:55	batch id:260	 lr:0.093665 loss:1.749634
[Train] epoch:55	batch id:270	 lr:0.093665 loss:1.734955
[Train] epoch:55	batch id:280	 lr:0.093665 loss:1.757358
[Train] epoch:55	batch id:290	 lr:0.093665 loss:1.956858
[Train] epoch:55	batch id:300	 lr:0.093665 loss:1.827761
[Train] 55, loss: 1.693724, train acc: 0.849959, 
[Test] epoch:55	batch id:0	 loss:1.331298
[Test] epoch:55	batch id:10	 loss:1.578535
[Test] epoch:55	batch id:20	 loss:1.480593
[Test] epoch:55	batch id:30	 loss:1.509698
[Test] epoch:55	batch id:40	 loss:1.485681
[Test] epoch:55	batch id:50	 loss:1.503720
[Test] epoch:55	batch id:60	 loss:1.313394
[Test] epoch:55	batch id:70	 loss:1.497516
[Test] epoch:55	batch id:80	 loss:1.651095
[Test] epoch:55	batch id:90	 loss:1.437152
[Test] epoch:55	batch id:100	 loss:1.971593
[Test] epoch:55	batch id:110	 loss:1.543103
[Test] epoch:55	batch id:120	 loss:1.642814
[Test] epoch:55	batch id:130	 loss:1.561958
[Test] epoch:55	batch id:140	 loss:1.462622
[Test] epoch:55	batch id:150	 loss:1.787867
[Test] 55, loss: 1.558607, test acc: 0.882901,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:56	batch id:0	 lr:0.093446 loss:1.734532
[Train] epoch:56	batch id:10	 lr:0.093446 loss:1.574871
[Train] epoch:56	batch id:20	 lr:0.093446 loss:1.542937
[Train] epoch:56	batch id:30	 lr:0.093446 loss:1.787027
[Train] epoch:56	batch id:40	 lr:0.093446 loss:1.544361
[Train] epoch:56	batch id:50	 lr:0.093446 loss:1.633272
[Train] epoch:56	batch id:60	 lr:0.093446 loss:1.676942
[Train] epoch:56	batch id:70	 lr:0.093446 loss:1.652229
[Train] epoch:56	batch id:80	 lr:0.093446 loss:1.780212
[Train] epoch:56	batch id:90	 lr:0.093446 loss:1.651759
[Train] epoch:56	batch id:100	 lr:0.093446 loss:1.628452
[Train] epoch:56	batch id:110	 lr:0.093446 loss:1.782676
[Train] epoch:56	batch id:120	 lr:0.093446 loss:1.635994
[Train] epoch:56	batch id:130	 lr:0.093446 loss:1.778244
[Train] epoch:56	batch id:140	 lr:0.093446 loss:1.850157
[Train] epoch:56	batch id:150	 lr:0.093446 loss:1.831612
[Train] epoch:56	batch id:160	 lr:0.093446 loss:1.713284
[Train] epoch:56	batch id:170	 lr:0.093446 loss:1.888464
[Train] epoch:56	batch id:180	 lr:0.093446 loss:1.707360
[Train] epoch:56	batch id:190	 lr:0.093446 loss:1.830552
[Train] epoch:56	batch id:200	 lr:0.093446 loss:1.633620
[Train] epoch:56	batch id:210	 lr:0.093446 loss:1.723023
[Train] epoch:56	batch id:220	 lr:0.093446 loss:1.754540
[Train] epoch:56	batch id:230	 lr:0.093446 loss:1.485102
[Train] epoch:56	batch id:240	 lr:0.093446 loss:1.919704
[Train] epoch:56	batch id:250	 lr:0.093446 loss:1.799830
[Train] epoch:56	batch id:260	 lr:0.093446 loss:1.660540
[Train] epoch:56	batch id:270	 lr:0.093446 loss:1.758250
[Train] epoch:56	batch id:280	 lr:0.093446 loss:1.729967
[Train] epoch:56	batch id:290	 lr:0.093446 loss:1.622936
[Train] epoch:56	batch id:300	 lr:0.093446 loss:1.684365
[Train] 56, loss: 1.702779, train acc: 0.848941, 
[Test] epoch:56	batch id:0	 loss:1.386840
[Test] epoch:56	batch id:10	 loss:1.466682
[Test] epoch:56	batch id:20	 loss:1.428197
[Test] epoch:56	batch id:30	 loss:1.452469
[Test] epoch:56	batch id:40	 loss:1.528479
[Test] epoch:56	batch id:50	 loss:1.625754
[Test] epoch:56	batch id:60	 loss:1.295802
[Test] epoch:56	batch id:70	 loss:1.550359
[Test] epoch:56	batch id:80	 loss:1.699353
[Test] epoch:56	batch id:90	 loss:1.557065
[Test] epoch:56	batch id:100	 loss:1.857943
[Test] epoch:56	batch id:110	 loss:1.467333
[Test] epoch:56	batch id:120	 loss:1.561392
[Test] epoch:56	batch id:130	 loss:1.547963
[Test] epoch:56	batch id:140	 loss:1.420256
[Test] epoch:56	batch id:150	 loss:2.027099
[Test] 56, loss: 1.576387, test acc: 0.875203,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:57	batch id:0	 lr:0.093223 loss:1.650604
[Train] epoch:57	batch id:10	 lr:0.093223 loss:1.709737
[Train] epoch:57	batch id:20	 lr:0.093223 loss:1.745740
[Train] epoch:57	batch id:30	 lr:0.093223 loss:1.678129
[Train] epoch:57	batch id:40	 lr:0.093223 loss:1.534221
[Train] epoch:57	batch id:50	 lr:0.093223 loss:1.679604
[Train] epoch:57	batch id:60	 lr:0.093223 loss:1.699627
[Train] epoch:57	batch id:70	 lr:0.093223 loss:1.978067
[Train] epoch:57	batch id:80	 lr:0.093223 loss:1.776405
[Train] epoch:57	batch id:90	 lr:0.093223 loss:1.675894
[Train] epoch:57	batch id:100	 lr:0.093223 loss:1.775666
[Train] epoch:57	batch id:110	 lr:0.093223 loss:1.804093
[Train] epoch:57	batch id:120	 lr:0.093223 loss:1.717705
[Train] epoch:57	batch id:130	 lr:0.093223 loss:1.874148
[Train] epoch:57	batch id:140	 lr:0.093223 loss:1.943207
[Train] epoch:57	batch id:150	 lr:0.093223 loss:1.707254
[Train] epoch:57	batch id:160	 lr:0.093223 loss:1.614501
[Train] epoch:57	batch id:170	 lr:0.093223 loss:1.725344
[Train] epoch:57	batch id:180	 lr:0.093223 loss:1.766006
[Train] epoch:57	batch id:190	 lr:0.093223 loss:1.739215
[Train] epoch:57	batch id:200	 lr:0.093223 loss:1.738942
[Train] epoch:57	batch id:210	 lr:0.093223 loss:1.562686
[Train] epoch:57	batch id:220	 lr:0.093223 loss:1.484021
[Train] epoch:57	batch id:230	 lr:0.093223 loss:1.765594
[Train] epoch:57	batch id:240	 lr:0.093223 loss:1.708178
[Train] epoch:57	batch id:250	 lr:0.093223 loss:1.687474
[Train] epoch:57	batch id:260	 lr:0.093223 loss:1.709707
[Train] epoch:57	batch id:270	 lr:0.093223 loss:1.978614
[Train] epoch:57	batch id:280	 lr:0.093223 loss:1.624202
[Train] epoch:57	batch id:290	 lr:0.093223 loss:1.492589
[Train] epoch:57	batch id:300	 lr:0.093223 loss:1.554405
[Train] 57, loss: 1.689550, train acc: 0.854336, 
[Test] epoch:57	batch id:0	 loss:1.498556
[Test] epoch:57	batch id:10	 loss:1.623856
[Test] epoch:57	batch id:20	 loss:1.525899
[Test] epoch:57	batch id:30	 loss:1.461847
[Test] epoch:57	batch id:40	 loss:1.601000
[Test] epoch:57	batch id:50	 loss:1.598132
[Test] epoch:57	batch id:60	 loss:1.367063
[Test] epoch:57	batch id:70	 loss:1.479728
[Test] epoch:57	batch id:80	 loss:1.677384
[Test] epoch:57	batch id:90	 loss:1.557416
[Test] epoch:57	batch id:100	 loss:1.834545
[Test] epoch:57	batch id:110	 loss:1.554836
[Test] epoch:57	batch id:120	 loss:1.420680
[Test] epoch:57	batch id:130	 loss:1.579413
[Test] epoch:57	batch id:140	 loss:1.473328
[Test] epoch:57	batch id:150	 loss:2.038673
[Test] 57, loss: 1.611597, test acc: 0.871961,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:58	batch id:0	 lr:0.092997 loss:1.901162
[Train] epoch:58	batch id:10	 lr:0.092997 loss:1.711794
[Train] epoch:58	batch id:20	 lr:0.092997 loss:1.692331
[Train] epoch:58	batch id:30	 lr:0.092997 loss:1.584665
[Train] epoch:58	batch id:40	 lr:0.092997 loss:1.563098
[Train] epoch:58	batch id:50	 lr:0.092997 loss:1.641456
[Train] epoch:58	batch id:60	 lr:0.092997 loss:1.551080
[Train] epoch:58	batch id:70	 lr:0.092997 loss:1.721518
[Train] epoch:58	batch id:80	 lr:0.092997 loss:1.666004
[Train] epoch:58	batch id:90	 lr:0.092997 loss:1.447726
[Train] epoch:58	batch id:100	 lr:0.092997 loss:1.590479
[Train] epoch:58	batch id:110	 lr:0.092997 loss:1.596950
[Train] epoch:58	batch id:120	 lr:0.092997 loss:1.674876
[Train] epoch:58	batch id:130	 lr:0.092997 loss:1.584892
[Train] epoch:58	batch id:140	 lr:0.092997 loss:1.466233
[Train] epoch:58	batch id:150	 lr:0.092997 loss:1.932630
[Train] epoch:58	batch id:160	 lr:0.092997 loss:1.619730
[Train] epoch:58	batch id:170	 lr:0.092997 loss:1.981693
[Train] epoch:58	batch id:180	 lr:0.092997 loss:1.540950
[Train] epoch:58	batch id:190	 lr:0.092997 loss:1.681580
[Train] epoch:58	batch id:200	 lr:0.092997 loss:1.625420
[Train] epoch:58	batch id:210	 lr:0.092997 loss:1.534943
[Train] epoch:58	batch id:220	 lr:0.092997 loss:1.884419
[Train] epoch:58	batch id:230	 lr:0.092997 loss:1.741079
[Train] epoch:58	batch id:240	 lr:0.092997 loss:1.678380
[Train] epoch:58	batch id:250	 lr:0.092997 loss:1.645398
[Train] epoch:58	batch id:260	 lr:0.092997 loss:1.610337
[Train] epoch:58	batch id:270	 lr:0.092997 loss:1.775571
[Train] epoch:58	batch id:280	 lr:0.092997 loss:1.675470
[Train] epoch:58	batch id:290	 lr:0.092997 loss:1.708855
[Train] epoch:58	batch id:300	 lr:0.092997 loss:1.583542
[Train] 58, loss: 1.681601, train acc: 0.854438, 
[Test] epoch:58	batch id:0	 loss:1.417503
[Test] epoch:58	batch id:10	 loss:1.544608
[Test] epoch:58	batch id:20	 loss:1.488210
[Test] epoch:58	batch id:30	 loss:1.536375
[Test] epoch:58	batch id:40	 loss:1.601373
[Test] epoch:58	batch id:50	 loss:1.515650
[Test] epoch:58	batch id:60	 loss:1.351538
[Test] epoch:58	batch id:70	 loss:1.518798
[Test] epoch:58	batch id:80	 loss:1.784242
[Test] epoch:58	batch id:90	 loss:1.627962
[Test] epoch:58	batch id:100	 loss:1.996511
[Test] epoch:58	batch id:110	 loss:1.582970
[Test] epoch:58	batch id:120	 loss:1.488670
[Test] epoch:58	batch id:130	 loss:1.676644
[Test] epoch:58	batch id:140	 loss:1.544574
[Test] epoch:58	batch id:150	 loss:2.054290
[Test] 58, loss: 1.616606, test acc: 0.867504,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:59	batch id:0	 lr:0.092768 loss:1.706871
[Train] epoch:59	batch id:10	 lr:0.092768 loss:1.803263
[Train] epoch:59	batch id:20	 lr:0.092768 loss:1.586832
[Train] epoch:59	batch id:30	 lr:0.092768 loss:1.677256
[Train] epoch:59	batch id:40	 lr:0.092768 loss:1.487762
[Train] epoch:59	batch id:50	 lr:0.092768 loss:1.613348
[Train] epoch:59	batch id:60	 lr:0.092768 loss:1.720194
[Train] epoch:59	batch id:70	 lr:0.092768 loss:1.781776
[Train] epoch:59	batch id:80	 lr:0.092768 loss:1.919288
[Train] epoch:59	batch id:90	 lr:0.092768 loss:1.662556
[Train] epoch:59	batch id:100	 lr:0.092768 loss:1.681512
[Train] epoch:59	batch id:110	 lr:0.092768 loss:1.610884
[Train] epoch:59	batch id:120	 lr:0.092768 loss:1.621762
[Train] epoch:59	batch id:130	 lr:0.092768 loss:1.686227
[Train] epoch:59	batch id:140	 lr:0.092768 loss:1.597175
[Train] epoch:59	batch id:150	 lr:0.092768 loss:1.817061
[Train] epoch:59	batch id:160	 lr:0.092768 loss:1.721146
[Train] epoch:59	batch id:170	 lr:0.092768 loss:1.727018
[Train] epoch:59	batch id:180	 lr:0.092768 loss:1.627523
[Train] epoch:59	batch id:190	 lr:0.092768 loss:1.577227
[Train] epoch:59	batch id:200	 lr:0.092768 loss:1.617128
[Train] epoch:59	batch id:210	 lr:0.092768 loss:1.813741
[Train] epoch:59	batch id:220	 lr:0.092768 loss:1.653519
[Train] epoch:59	batch id:230	 lr:0.092768 loss:1.975487
[Train] epoch:59	batch id:240	 lr:0.092768 loss:1.738343
[Train] epoch:59	batch id:250	 lr:0.092768 loss:1.646508
[Train] epoch:59	batch id:260	 lr:0.092768 loss:1.583767
[Train] epoch:59	batch id:270	 lr:0.092768 loss:1.689625
[Train] epoch:59	batch id:280	 lr:0.092768 loss:1.488292
[Train] epoch:59	batch id:290	 lr:0.092768 loss:1.654235
[Train] epoch:59	batch id:300	 lr:0.092768 loss:1.591091
[Train] 59, loss: 1.684422, train acc: 0.856779, 
[Test] epoch:59	batch id:0	 loss:1.359589
[Test] epoch:59	batch id:10	 loss:1.582248
[Test] epoch:59	batch id:20	 loss:1.682804
[Test] epoch:59	batch id:30	 loss:1.479722
[Test] epoch:59	batch id:40	 loss:1.508511
[Test] epoch:59	batch id:50	 loss:1.553892
[Test] epoch:59	batch id:60	 loss:1.350253
[Test] epoch:59	batch id:70	 loss:1.544678
[Test] epoch:59	batch id:80	 loss:1.718408
[Test] epoch:59	batch id:90	 loss:1.516135
[Test] epoch:59	batch id:100	 loss:1.957687
[Test] epoch:59	batch id:110	 loss:1.620849
[Test] epoch:59	batch id:120	 loss:1.603731
[Test] epoch:59	batch id:130	 loss:1.523579
[Test] epoch:59	batch id:140	 loss:1.588173
[Test] epoch:59	batch id:150	 loss:1.943590
[Test] 59, loss: 1.604093, test acc: 0.886143,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:60	batch id:0	 lr:0.092535 loss:1.492851
[Train] epoch:60	batch id:10	 lr:0.092535 loss:1.761008
[Train] epoch:60	batch id:20	 lr:0.092535 loss:1.492139
[Train] epoch:60	batch id:30	 lr:0.092535 loss:1.521265
[Train] epoch:60	batch id:40	 lr:0.092535 loss:1.697830
[Train] epoch:60	batch id:50	 lr:0.092535 loss:1.656509
[Train] epoch:60	batch id:60	 lr:0.092535 loss:1.620638
[Train] epoch:60	batch id:70	 lr:0.092535 loss:1.671666
[Train] epoch:60	batch id:80	 lr:0.092535 loss:1.738578
[Train] epoch:60	batch id:90	 lr:0.092535 loss:1.741909
[Train] epoch:60	batch id:100	 lr:0.092535 loss:1.610832
[Train] epoch:60	batch id:110	 lr:0.092535 loss:1.659853
[Train] epoch:60	batch id:120	 lr:0.092535 loss:1.578512
[Train] epoch:60	batch id:130	 lr:0.092535 loss:1.709004
[Train] epoch:60	batch id:140	 lr:0.092535 loss:1.744857
[Train] epoch:60	batch id:150	 lr:0.092535 loss:1.632500
[Train] epoch:60	batch id:160	 lr:0.092535 loss:1.807910
[Train] epoch:60	batch id:170	 lr:0.092535 loss:1.685049
[Train] epoch:60	batch id:180	 lr:0.092535 loss:1.654956
[Train] epoch:60	batch id:190	 lr:0.092535 loss:1.692767
[Train] epoch:60	batch id:200	 lr:0.092535 loss:1.779523
[Train] epoch:60	batch id:210	 lr:0.092535 loss:1.627207
[Train] epoch:60	batch id:220	 lr:0.092535 loss:1.497972
[Train] epoch:60	batch id:230	 lr:0.092535 loss:1.869531
[Train] epoch:60	batch id:240	 lr:0.092535 loss:1.661385
[Train] epoch:60	batch id:250	 lr:0.092535 loss:1.561809
[Train] epoch:60	batch id:260	 lr:0.092535 loss:1.588405
[Train] epoch:60	batch id:270	 lr:0.092535 loss:1.786278
[Train] epoch:60	batch id:280	 lr:0.092535 loss:1.704941
[Train] epoch:60	batch id:290	 lr:0.092535 loss:1.617721
[Train] epoch:60	batch id:300	 lr:0.092535 loss:1.713733
[Train] 60, loss: 1.678774, train acc: 0.857085, 
[Test] epoch:60	batch id:0	 loss:1.379298
[Test] epoch:60	batch id:10	 loss:1.480341
[Test] epoch:60	batch id:20	 loss:1.491404
[Test] epoch:60	batch id:30	 loss:1.463403
[Test] epoch:60	batch id:40	 loss:1.451110
[Test] epoch:60	batch id:50	 loss:1.595478
[Test] epoch:60	batch id:60	 loss:1.333735
[Test] epoch:60	batch id:70	 loss:1.600262
[Test] epoch:60	batch id:80	 loss:1.644025
[Test] epoch:60	batch id:90	 loss:1.529606
[Test] epoch:60	batch id:100	 loss:1.864396
[Test] epoch:60	batch id:110	 loss:1.486976
[Test] epoch:60	batch id:120	 loss:1.533936
[Test] epoch:60	batch id:130	 loss:1.792391
[Test] epoch:60	batch id:140	 loss:1.421856
[Test] epoch:60	batch id:150	 loss:2.014851
[Test] 60, loss: 1.589438, test acc: 0.875608,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:61	batch id:0	 lr:0.092298 loss:1.723719
[Train] epoch:61	batch id:10	 lr:0.092298 loss:1.895880
[Train] epoch:61	batch id:20	 lr:0.092298 loss:1.480076
[Train] epoch:61	batch id:30	 lr:0.092298 loss:1.773109
[Train] epoch:61	batch id:40	 lr:0.092298 loss:1.557107
[Train] epoch:61	batch id:50	 lr:0.092298 loss:1.736184
[Train] epoch:61	batch id:60	 lr:0.092298 loss:1.625192
[Train] epoch:61	batch id:70	 lr:0.092298 loss:1.797187
[Train] epoch:61	batch id:80	 lr:0.092298 loss:1.717168
[Train] epoch:61	batch id:90	 lr:0.092298 loss:1.664672
[Train] epoch:61	batch id:100	 lr:0.092298 loss:1.556131
[Train] epoch:61	batch id:110	 lr:0.092298 loss:1.872955
[Train] epoch:61	batch id:120	 lr:0.092298 loss:1.628338
[Train] epoch:61	batch id:130	 lr:0.092298 loss:1.664249
[Train] epoch:61	batch id:140	 lr:0.092298 loss:1.558847
[Train] epoch:61	batch id:150	 lr:0.092298 loss:1.757636
[Train] epoch:61	batch id:160	 lr:0.092298 loss:1.623327
[Train] epoch:61	batch id:170	 lr:0.092298 loss:1.585699
[Train] epoch:61	batch id:180	 lr:0.092298 loss:1.672305
[Train] epoch:61	batch id:190	 lr:0.092298 loss:1.662598
[Train] epoch:61	batch id:200	 lr:0.092298 loss:1.720374
[Train] epoch:61	batch id:210	 lr:0.092298 loss:1.681865
[Train] epoch:61	batch id:220	 lr:0.092298 loss:1.579665
[Train] epoch:61	batch id:230	 lr:0.092298 loss:1.634603
[Train] epoch:61	batch id:240	 lr:0.092298 loss:1.789107
[Train] epoch:61	batch id:250	 lr:0.092298 loss:1.603897
[Train] epoch:61	batch id:260	 lr:0.092298 loss:1.604774
[Train] epoch:61	batch id:270	 lr:0.092298 loss:1.893174
[Train] epoch:61	batch id:280	 lr:0.092298 loss:1.597435
[Train] epoch:61	batch id:290	 lr:0.092298 loss:1.597142
[Train] epoch:61	batch id:300	 lr:0.092298 loss:1.674827
[Train] 61, loss: 1.680100, train acc: 0.857288, 
[Test] epoch:61	batch id:0	 loss:1.482248
[Test] epoch:61	batch id:10	 loss:1.571215
[Test] epoch:61	batch id:20	 loss:1.610704
[Test] epoch:61	batch id:30	 loss:1.616749
[Test] epoch:61	batch id:40	 loss:1.443438
[Test] epoch:61	batch id:50	 loss:1.569849
[Test] epoch:61	batch id:60	 loss:1.381550
[Test] epoch:61	batch id:70	 loss:1.561958
[Test] epoch:61	batch id:80	 loss:1.659882
[Test] epoch:61	batch id:90	 loss:1.605058
[Test] epoch:61	batch id:100	 loss:2.036312
[Test] epoch:61	batch id:110	 loss:1.520484
[Test] epoch:61	batch id:120	 loss:1.516010
[Test] epoch:61	batch id:130	 loss:1.526307
[Test] epoch:61	batch id:140	 loss:1.610033
[Test] epoch:61	batch id:150	 loss:1.899945
[Test] 61, loss: 1.611548, test acc: 0.888979,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:62	batch id:0	 lr:0.092058 loss:1.591812
[Train] epoch:62	batch id:10	 lr:0.092058 loss:1.602914
[Train] epoch:62	batch id:20	 lr:0.092058 loss:1.624882
[Train] epoch:62	batch id:30	 lr:0.092058 loss:1.694370
[Train] epoch:62	batch id:40	 lr:0.092058 loss:1.871331
[Train] epoch:62	batch id:50	 lr:0.092058 loss:1.673693
[Train] epoch:62	batch id:60	 lr:0.092058 loss:1.595079
[Train] epoch:62	batch id:70	 lr:0.092058 loss:1.688618
[Train] epoch:62	batch id:80	 lr:0.092058 loss:1.705660
[Train] epoch:62	batch id:90	 lr:0.092058 loss:1.578796
[Train] epoch:62	batch id:100	 lr:0.092058 loss:1.525457
[Train] epoch:62	batch id:110	 lr:0.092058 loss:1.594735
[Train] epoch:62	batch id:120	 lr:0.092058 loss:1.564162
[Train] epoch:62	batch id:130	 lr:0.092058 loss:1.798735
[Train] epoch:62	batch id:140	 lr:0.092058 loss:1.667414
[Train] epoch:62	batch id:150	 lr:0.092058 loss:1.742960
[Train] epoch:62	batch id:160	 lr:0.092058 loss:1.700420
[Train] epoch:62	batch id:170	 lr:0.092058 loss:1.619929
[Train] epoch:62	batch id:180	 lr:0.092058 loss:1.697315
[Train] epoch:62	batch id:190	 lr:0.092058 loss:1.613724
[Train] epoch:62	batch id:200	 lr:0.092058 loss:1.625338
[Train] epoch:62	batch id:210	 lr:0.092058 loss:1.487972
[Train] epoch:62	batch id:220	 lr:0.092058 loss:2.024265
[Train] epoch:62	batch id:230	 lr:0.092058 loss:1.847455
[Train] epoch:62	batch id:240	 lr:0.092058 loss:1.799585
[Train] epoch:62	batch id:250	 lr:0.092058 loss:1.684174
[Train] epoch:62	batch id:260	 lr:0.092058 loss:1.472336
[Train] epoch:62	batch id:270	 lr:0.092058 loss:1.840411
[Train] epoch:62	batch id:280	 lr:0.092058 loss:1.558192
[Train] epoch:62	batch id:290	 lr:0.092058 loss:1.776282
[Train] epoch:62	batch id:300	 lr:0.092058 loss:1.652143
[Train] 62, loss: 1.669087, train acc: 0.863396, 
[Test] epoch:62	batch id:0	 loss:1.406954
[Test] epoch:62	batch id:10	 loss:1.661982
[Test] epoch:62	batch id:20	 loss:1.489974
[Test] epoch:62	batch id:30	 loss:1.563063
[Test] epoch:62	batch id:40	 loss:1.482464
[Test] epoch:62	batch id:50	 loss:1.488266
[Test] epoch:62	batch id:60	 loss:1.330535
[Test] epoch:62	batch id:70	 loss:1.470897
[Test] epoch:62	batch id:80	 loss:1.609399
[Test] epoch:62	batch id:90	 loss:1.551348
[Test] epoch:62	batch id:100	 loss:1.819889
[Test] epoch:62	batch id:110	 loss:1.451066
[Test] epoch:62	batch id:120	 loss:1.520477
[Test] epoch:62	batch id:130	 loss:1.599719
[Test] epoch:62	batch id:140	 loss:1.437494
[Test] epoch:62	batch id:150	 loss:1.940718
[Test] 62, loss: 1.569481, test acc: 0.886953,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:63	batch id:0	 lr:0.091815 loss:1.684302
[Train] epoch:63	batch id:10	 lr:0.091815 loss:1.639464
[Train] epoch:63	batch id:20	 lr:0.091815 loss:1.728643
[Train] epoch:63	batch id:30	 lr:0.091815 loss:1.602654
[Train] epoch:63	batch id:40	 lr:0.091815 loss:1.611861
[Train] epoch:63	batch id:50	 lr:0.091815 loss:1.631647
[Train] epoch:63	batch id:60	 lr:0.091815 loss:1.666585
[Train] epoch:63	batch id:70	 lr:0.091815 loss:1.433244
[Train] epoch:63	batch id:80	 lr:0.091815 loss:1.703530
[Train] epoch:63	batch id:90	 lr:0.091815 loss:1.678513
[Train] epoch:63	batch id:100	 lr:0.091815 loss:1.799133
[Train] epoch:63	batch id:110	 lr:0.091815 loss:1.563612
[Train] epoch:63	batch id:120	 lr:0.091815 loss:1.722637
[Train] epoch:63	batch id:130	 lr:0.091815 loss:1.659354
[Train] epoch:63	batch id:140	 lr:0.091815 loss:1.731912
[Train] epoch:63	batch id:150	 lr:0.091815 loss:1.612881
[Train] epoch:63	batch id:160	 lr:0.091815 loss:1.574883
[Train] epoch:63	batch id:170	 lr:0.091815 loss:1.617806
[Train] epoch:63	batch id:180	 lr:0.091815 loss:1.766556
[Train] epoch:63	batch id:190	 lr:0.091815 loss:1.584745
[Train] epoch:63	batch id:200	 lr:0.091815 loss:1.784017
[Train] epoch:63	batch id:210	 lr:0.091815 loss:1.629298
[Train] epoch:63	batch id:220	 lr:0.091815 loss:1.540756
[Train] epoch:63	batch id:230	 lr:0.091815 loss:1.627926
[Train] epoch:63	batch id:240	 lr:0.091815 loss:1.635349
[Train] epoch:63	batch id:250	 lr:0.091815 loss:1.548700
[Train] epoch:63	batch id:260	 lr:0.091815 loss:1.590252
[Train] epoch:63	batch id:270	 lr:0.091815 loss:1.641262
[Train] epoch:63	batch id:280	 lr:0.091815 loss:1.820725
[Train] epoch:63	batch id:290	 lr:0.091815 loss:1.777461
[Train] epoch:63	batch id:300	 lr:0.091815 loss:1.711829
[Train] 63, loss: 1.674514, train acc: 0.858306, 
[Test] epoch:63	batch id:0	 loss:1.344085
[Test] epoch:63	batch id:10	 loss:1.561130
[Test] epoch:63	batch id:20	 loss:1.517161
[Test] epoch:63	batch id:30	 loss:1.564717
[Test] epoch:63	batch id:40	 loss:1.498764
[Test] epoch:63	batch id:50	 loss:1.483821
[Test] epoch:63	batch id:60	 loss:1.332828
[Test] epoch:63	batch id:70	 loss:1.520890
[Test] epoch:63	batch id:80	 loss:1.762988
[Test] epoch:63	batch id:90	 loss:1.549058
[Test] epoch:63	batch id:100	 loss:1.912141
[Test] epoch:63	batch id:110	 loss:1.456163
[Test] epoch:63	batch id:120	 loss:1.409860
[Test] epoch:63	batch id:130	 loss:1.484758
[Test] epoch:63	batch id:140	 loss:1.411208
[Test] epoch:63	batch id:150	 loss:1.922620
[Test] 63, loss: 1.557378, test acc: 0.897083,
Max Acc:0.897083
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:64	batch id:0	 lr:0.091569 loss:1.632353
[Train] epoch:64	batch id:10	 lr:0.091569 loss:1.529948
[Train] epoch:64	batch id:20	 lr:0.091569 loss:1.765681
[Train] epoch:64	batch id:30	 lr:0.091569 loss:1.474531
[Train] epoch:64	batch id:40	 lr:0.091569 loss:1.473368
[Train] epoch:64	batch id:50	 lr:0.091569 loss:1.606845
[Train] epoch:64	batch id:60	 lr:0.091569 loss:1.654378
[Train] epoch:64	batch id:70	 lr:0.091569 loss:1.675366
[Train] epoch:64	batch id:80	 lr:0.091569 loss:1.790293
[Train] epoch:64	batch id:90	 lr:0.091569 loss:1.583752
[Train] epoch:64	batch id:100	 lr:0.091569 loss:1.661628
[Train] epoch:64	batch id:110	 lr:0.091569 loss:1.710279
[Train] epoch:64	batch id:120	 lr:0.091569 loss:1.619384
[Train] epoch:64	batch id:130	 lr:0.091569 loss:1.612363
[Train] epoch:64	batch id:140	 lr:0.091569 loss:1.727081
[Train] epoch:64	batch id:150	 lr:0.091569 loss:1.746202
[Train] epoch:64	batch id:160	 lr:0.091569 loss:1.604436
[Train] epoch:64	batch id:170	 lr:0.091569 loss:1.788160
[Train] epoch:64	batch id:180	 lr:0.091569 loss:1.667877
[Train] epoch:64	batch id:190	 lr:0.091569 loss:1.691451
[Train] epoch:64	batch id:200	 lr:0.091569 loss:1.655483
[Train] epoch:64	batch id:210	 lr:0.091569 loss:1.482066
[Train] epoch:64	batch id:220	 lr:0.091569 loss:1.734028
[Train] epoch:64	batch id:230	 lr:0.091569 loss:1.535671
[Train] epoch:64	batch id:240	 lr:0.091569 loss:1.590007
[Train] epoch:64	batch id:250	 lr:0.091569 loss:1.603274
[Train] epoch:64	batch id:260	 lr:0.091569 loss:1.689828
[Train] epoch:64	batch id:270	 lr:0.091569 loss:1.587242
[Train] epoch:64	batch id:280	 lr:0.091569 loss:1.544761
[Train] epoch:64	batch id:290	 lr:0.091569 loss:1.775030
[Train] epoch:64	batch id:300	 lr:0.091569 loss:1.743063
[Train] 64, loss: 1.673788, train acc: 0.860953, 
[Test] epoch:64	batch id:0	 loss:1.447404
[Test] epoch:64	batch id:10	 loss:1.674187
[Test] epoch:64	batch id:20	 loss:1.480501
[Test] epoch:64	batch id:30	 loss:1.651385
[Test] epoch:64	batch id:40	 loss:1.620753
[Test] epoch:64	batch id:50	 loss:1.567183
[Test] epoch:64	batch id:60	 loss:1.361221
[Test] epoch:64	batch id:70	 loss:1.591623
[Test] epoch:64	batch id:80	 loss:1.682930
[Test] epoch:64	batch id:90	 loss:1.551391
[Test] epoch:64	batch id:100	 loss:1.813030
[Test] epoch:64	batch id:110	 loss:1.636810
[Test] epoch:64	batch id:120	 loss:1.463372
[Test] epoch:64	batch id:130	 loss:1.591648
[Test] epoch:64	batch id:140	 loss:1.561788
[Test] epoch:64	batch id:150	 loss:1.924336
[Test] 64, loss: 1.591968, test acc: 0.885737,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:65	batch id:0	 lr:0.091319 loss:1.582488
[Train] epoch:65	batch id:10	 lr:0.091319 loss:1.437664
[Train] epoch:65	batch id:20	 lr:0.091319 loss:1.545745
[Train] epoch:65	batch id:30	 lr:0.091319 loss:1.726843
[Train] epoch:65	batch id:40	 lr:0.091319 loss:1.643202
[Train] epoch:65	batch id:50	 lr:0.091319 loss:1.699096
[Train] epoch:65	batch id:60	 lr:0.091319 loss:1.598126
[Train] epoch:65	batch id:70	 lr:0.091319 loss:1.716152
[Train] epoch:65	batch id:80	 lr:0.091319 loss:1.588127
[Train] epoch:65	batch id:90	 lr:0.091319 loss:1.608252
[Train] epoch:65	batch id:100	 lr:0.091319 loss:1.632652
[Train] epoch:65	batch id:110	 lr:0.091319 loss:1.740796
[Train] epoch:65	batch id:120	 lr:0.091319 loss:1.645531
[Train] epoch:65	batch id:130	 lr:0.091319 loss:1.702852
[Train] epoch:65	batch id:140	 lr:0.091319 loss:1.640925
[Train] epoch:65	batch id:150	 lr:0.091319 loss:1.602359
[Train] epoch:65	batch id:160	 lr:0.091319 loss:1.476034
[Train] epoch:65	batch id:170	 lr:0.091319 loss:1.732389
[Train] epoch:65	batch id:180	 lr:0.091319 loss:1.869625
[Train] epoch:65	batch id:190	 lr:0.091319 loss:1.658416
[Train] epoch:65	batch id:200	 lr:0.091319 loss:1.761338
[Train] epoch:65	batch id:210	 lr:0.091319 loss:1.688112
[Train] epoch:65	batch id:220	 lr:0.091319 loss:1.661755
[Train] epoch:65	batch id:230	 lr:0.091319 loss:1.601361
[Train] epoch:65	batch id:240	 lr:0.091319 loss:1.669806
[Train] epoch:65	batch id:250	 lr:0.091319 loss:1.624490
[Train] epoch:65	batch id:260	 lr:0.091319 loss:1.928654
[Train] epoch:65	batch id:270	 lr:0.091319 loss:1.907526
[Train] epoch:65	batch id:280	 lr:0.091319 loss:1.818569
[Train] epoch:65	batch id:290	 lr:0.091319 loss:1.680828
[Train] epoch:65	batch id:300	 lr:0.091319 loss:1.648032
[Train] 65, loss: 1.668253, train acc: 0.862785, 
[Test] epoch:65	batch id:0	 loss:1.383415
[Test] epoch:65	batch id:10	 loss:1.598449
[Test] epoch:65	batch id:20	 loss:1.470550
[Test] epoch:65	batch id:30	 loss:1.558986
[Test] epoch:65	batch id:40	 loss:1.533509
[Test] epoch:65	batch id:50	 loss:1.486115
[Test] epoch:65	batch id:60	 loss:1.333489
[Test] epoch:65	batch id:70	 loss:1.517539
[Test] epoch:65	batch id:80	 loss:1.707485
[Test] epoch:65	batch id:90	 loss:1.628666
[Test] epoch:65	batch id:100	 loss:1.852463
[Test] epoch:65	batch id:110	 loss:1.435020
[Test] epoch:65	batch id:120	 loss:1.509643
[Test] epoch:65	batch id:130	 loss:1.517331
[Test] epoch:65	batch id:140	 loss:1.482927
[Test] epoch:65	batch id:150	 loss:1.953784
[Test] 65, loss: 1.579658, test acc: 0.877634,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:66	batch id:0	 lr:0.091066 loss:1.528615
[Train] epoch:66	batch id:10	 lr:0.091066 loss:1.620313
[Train] epoch:66	batch id:20	 lr:0.091066 loss:1.640005
[Train] epoch:66	batch id:30	 lr:0.091066 loss:1.616620
[Train] epoch:66	batch id:40	 lr:0.091066 loss:1.637828
[Train] epoch:66	batch id:50	 lr:0.091066 loss:1.585689
[Train] epoch:66	batch id:60	 lr:0.091066 loss:1.644059
[Train] epoch:66	batch id:70	 lr:0.091066 loss:1.610828
[Train] epoch:66	batch id:80	 lr:0.091066 loss:1.640500
[Train] epoch:66	batch id:90	 lr:0.091066 loss:1.794348
[Train] epoch:66	batch id:100	 lr:0.091066 loss:1.795568
[Train] epoch:66	batch id:110	 lr:0.091066 loss:1.491556
[Train] epoch:66	batch id:120	 lr:0.091066 loss:1.547722
[Train] epoch:66	batch id:130	 lr:0.091066 loss:1.532959
[Train] epoch:66	batch id:140	 lr:0.091066 loss:1.652249
[Train] epoch:66	batch id:150	 lr:0.091066 loss:1.653473
[Train] epoch:66	batch id:160	 lr:0.091066 loss:1.604631
[Train] epoch:66	batch id:170	 lr:0.091066 loss:1.535579
[Train] epoch:66	batch id:180	 lr:0.091066 loss:1.722901
[Train] epoch:66	batch id:190	 lr:0.091066 loss:1.632447
[Train] epoch:66	batch id:200	 lr:0.091066 loss:1.721147
[Train] epoch:66	batch id:210	 lr:0.091066 loss:1.672761
[Train] epoch:66	batch id:220	 lr:0.091066 loss:1.591956
[Train] epoch:66	batch id:230	 lr:0.091066 loss:1.627733
[Train] epoch:66	batch id:240	 lr:0.091066 loss:1.492024
[Train] epoch:66	batch id:250	 lr:0.091066 loss:1.646309
[Train] epoch:66	batch id:260	 lr:0.091066 loss:1.475638
[Train] epoch:66	batch id:270	 lr:0.091066 loss:1.725957
[Train] epoch:66	batch id:280	 lr:0.091066 loss:1.713534
[Train] epoch:66	batch id:290	 lr:0.091066 loss:1.752631
[Train] epoch:66	batch id:300	 lr:0.091066 loss:1.637419
[Train] 66, loss: 1.657425, train acc: 0.864312, 
[Test] epoch:66	batch id:0	 loss:1.403265
[Test] epoch:66	batch id:10	 loss:1.617961
[Test] epoch:66	batch id:20	 loss:1.476555
[Test] epoch:66	batch id:30	 loss:1.418327
[Test] epoch:66	batch id:40	 loss:1.545014
[Test] epoch:66	batch id:50	 loss:1.479606
[Test] epoch:66	batch id:60	 loss:1.308178
[Test] epoch:66	batch id:70	 loss:1.488637
[Test] epoch:66	batch id:80	 loss:1.637247
[Test] epoch:66	batch id:90	 loss:1.601626
[Test] epoch:66	batch id:100	 loss:1.975939
[Test] epoch:66	batch id:110	 loss:1.480002
[Test] epoch:66	batch id:120	 loss:1.636245
[Test] epoch:66	batch id:130	 loss:1.577088
[Test] epoch:66	batch id:140	 loss:1.537517
[Test] epoch:66	batch id:150	 loss:1.800766
[Test] 66, loss: 1.602057, test acc: 0.873987,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:67	batch id:0	 lr:0.090810 loss:1.706069
[Train] epoch:67	batch id:10	 lr:0.090810 loss:1.514632
[Train] epoch:67	batch id:20	 lr:0.090810 loss:1.743384
[Train] epoch:67	batch id:30	 lr:0.090810 loss:1.696076
[Train] epoch:67	batch id:40	 lr:0.090810 loss:1.553759
[Train] epoch:67	batch id:50	 lr:0.090810 loss:1.533133
[Train] epoch:67	batch id:60	 lr:0.090810 loss:1.578516
[Train] epoch:67	batch id:70	 lr:0.090810 loss:1.665989
[Train] epoch:67	batch id:80	 lr:0.090810 loss:1.756617
[Train] epoch:67	batch id:90	 lr:0.090810 loss:1.793144
[Train] epoch:67	batch id:100	 lr:0.090810 loss:1.783019
[Train] epoch:67	batch id:110	 lr:0.090810 loss:1.518966
[Train] epoch:67	batch id:120	 lr:0.090810 loss:1.758632
[Train] epoch:67	batch id:130	 lr:0.090810 loss:1.579453
[Train] epoch:67	batch id:140	 lr:0.090810 loss:1.655249
[Train] epoch:67	batch id:150	 lr:0.090810 loss:1.906657
[Train] epoch:67	batch id:160	 lr:0.090810 loss:1.743390
[Train] epoch:67	batch id:170	 lr:0.090810 loss:1.621331
[Train] epoch:67	batch id:180	 lr:0.090810 loss:1.545032
[Train] epoch:67	batch id:190	 lr:0.090810 loss:1.557484
[Train] epoch:67	batch id:200	 lr:0.090810 loss:1.613190
[Train] epoch:67	batch id:210	 lr:0.090810 loss:1.674763
[Train] epoch:67	batch id:220	 lr:0.090810 loss:1.656735
[Train] epoch:67	batch id:230	 lr:0.090810 loss:1.718231
[Train] epoch:67	batch id:240	 lr:0.090810 loss:1.697348
[Train] epoch:67	batch id:250	 lr:0.090810 loss:1.613621
[Train] epoch:67	batch id:260	 lr:0.090810 loss:1.475797
[Train] epoch:67	batch id:270	 lr:0.090810 loss:1.524144
[Train] epoch:67	batch id:280	 lr:0.090810 loss:1.478687
[Train] epoch:67	batch id:290	 lr:0.090810 loss:1.624604
[Train] epoch:67	batch id:300	 lr:0.090810 loss:1.511009
[Train] 67, loss: 1.667262, train acc: 0.860953, 
[Test] epoch:67	batch id:0	 loss:1.401097
[Test] epoch:67	batch id:10	 loss:1.498893
[Test] epoch:67	batch id:20	 loss:1.466717
[Test] epoch:67	batch id:30	 loss:1.418797
[Test] epoch:67	batch id:40	 loss:1.478545
[Test] epoch:67	batch id:50	 loss:1.450714
[Test] epoch:67	batch id:60	 loss:1.352838
[Test] epoch:67	batch id:70	 loss:1.540117
[Test] epoch:67	batch id:80	 loss:1.555480
[Test] epoch:67	batch id:90	 loss:1.465142
[Test] epoch:67	batch id:100	 loss:1.860476
[Test] epoch:67	batch id:110	 loss:1.429959
[Test] epoch:67	batch id:120	 loss:1.418960
[Test] epoch:67	batch id:130	 loss:1.475743
[Test] epoch:67	batch id:140	 loss:1.493264
[Test] epoch:67	batch id:150	 loss:1.788761
[Test] 67, loss: 1.539573, test acc: 0.902350,
Max Acc:0.902350
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:68	batch id:0	 lr:0.090550 loss:1.650827
[Train] epoch:68	batch id:10	 lr:0.090550 loss:1.672629
[Train] epoch:68	batch id:20	 lr:0.090550 loss:1.750669
[Train] epoch:68	batch id:30	 lr:0.090550 loss:1.671283
[Train] epoch:68	batch id:40	 lr:0.090550 loss:1.787300
[Train] epoch:68	batch id:50	 lr:0.090550 loss:1.767933
[Train] epoch:68	batch id:60	 lr:0.090550 loss:1.558842
[Train] epoch:68	batch id:70	 lr:0.090550 loss:1.592191
[Train] epoch:68	batch id:80	 lr:0.090550 loss:1.607129
[Train] epoch:68	batch id:90	 lr:0.090550 loss:1.644152
[Train] epoch:68	batch id:100	 lr:0.090550 loss:1.545058
[Train] epoch:68	batch id:110	 lr:0.090550 loss:1.754292
[Train] epoch:68	batch id:120	 lr:0.090550 loss:1.719939
[Train] epoch:68	batch id:130	 lr:0.090550 loss:1.551146
[Train] epoch:68	batch id:140	 lr:0.090550 loss:1.808359
[Train] epoch:68	batch id:150	 lr:0.090550 loss:1.824464
[Train] epoch:68	batch id:160	 lr:0.090550 loss:1.506146
[Train] epoch:68	batch id:170	 lr:0.090550 loss:1.641024
[Train] epoch:68	batch id:180	 lr:0.090550 loss:1.631194
[Train] epoch:68	batch id:190	 lr:0.090550 loss:1.671113
[Train] epoch:68	batch id:200	 lr:0.090550 loss:1.830865
[Train] epoch:68	batch id:210	 lr:0.090550 loss:1.680433
[Train] epoch:68	batch id:220	 lr:0.090550 loss:1.590119
[Train] epoch:68	batch id:230	 lr:0.090550 loss:1.664397
[Train] epoch:68	batch id:240	 lr:0.090550 loss:1.687175
[Train] epoch:68	batch id:250	 lr:0.090550 loss:1.701759
[Train] epoch:68	batch id:260	 lr:0.090550 loss:1.666911
[Train] epoch:68	batch id:270	 lr:0.090550 loss:1.586120
[Train] epoch:68	batch id:280	 lr:0.090550 loss:1.675064
[Train] epoch:68	batch id:290	 lr:0.090550 loss:1.847721
[Train] epoch:68	batch id:300	 lr:0.090550 loss:1.573594
[Train] 68, loss: 1.665250, train acc: 0.863294, 
[Test] epoch:68	batch id:0	 loss:1.376511
[Test] epoch:68	batch id:10	 loss:1.495796
[Test] epoch:68	batch id:20	 loss:1.543211
[Test] epoch:68	batch id:30	 loss:1.452764
[Test] epoch:68	batch id:40	 loss:1.505778
[Test] epoch:68	batch id:50	 loss:1.509211
[Test] epoch:68	batch id:60	 loss:1.352682
[Test] epoch:68	batch id:70	 loss:1.522468
[Test] epoch:68	batch id:80	 loss:1.724609
[Test] epoch:68	batch id:90	 loss:1.600820
[Test] epoch:68	batch id:100	 loss:1.812280
[Test] epoch:68	batch id:110	 loss:1.463385
[Test] epoch:68	batch id:120	 loss:1.523372
[Test] epoch:68	batch id:130	 loss:1.545653
[Test] epoch:68	batch id:140	 loss:1.475874
[Test] epoch:68	batch id:150	 loss:1.865448
[Test] 68, loss: 1.562918, test acc: 0.898703,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:69	batch id:0	 lr:0.090288 loss:1.726933
[Train] epoch:69	batch id:10	 lr:0.090288 loss:1.774965
[Train] epoch:69	batch id:20	 lr:0.090288 loss:1.647373
[Train] epoch:69	batch id:30	 lr:0.090288 loss:1.582932
[Train] epoch:69	batch id:40	 lr:0.090288 loss:1.746167
[Train] epoch:69	batch id:50	 lr:0.090288 loss:1.691419
[Train] epoch:69	batch id:60	 lr:0.090288 loss:1.487426
[Train] epoch:69	batch id:70	 lr:0.090288 loss:1.743931
[Train] epoch:69	batch id:80	 lr:0.090288 loss:1.763348
[Train] epoch:69	batch id:90	 lr:0.090288 loss:1.769132
[Train] epoch:69	batch id:100	 lr:0.090288 loss:1.741442
[Train] epoch:69	batch id:110	 lr:0.090288 loss:1.680168
[Train] epoch:69	batch id:120	 lr:0.090288 loss:1.675957
[Train] epoch:69	batch id:130	 lr:0.090288 loss:1.507457
[Train] epoch:69	batch id:140	 lr:0.090288 loss:1.537060
[Train] epoch:69	batch id:150	 lr:0.090288 loss:1.566748
[Train] epoch:69	batch id:160	 lr:0.090288 loss:1.758279
[Train] epoch:69	batch id:170	 lr:0.090288 loss:1.781833
[Train] epoch:69	batch id:180	 lr:0.090288 loss:1.689138
[Train] epoch:69	batch id:190	 lr:0.090288 loss:1.675287
[Train] epoch:69	batch id:200	 lr:0.090288 loss:1.621974
[Train] epoch:69	batch id:210	 lr:0.090288 loss:1.593257
[Train] epoch:69	batch id:220	 lr:0.090288 loss:1.545055
[Train] epoch:69	batch id:230	 lr:0.090288 loss:1.669083
[Train] epoch:69	batch id:240	 lr:0.090288 loss:1.709458
[Train] epoch:69	batch id:250	 lr:0.090288 loss:1.602697
[Train] epoch:69	batch id:260	 lr:0.090288 loss:1.595042
[Train] epoch:69	batch id:270	 lr:0.090288 loss:1.698644
[Train] epoch:69	batch id:280	 lr:0.090288 loss:1.634875
[Train] epoch:69	batch id:290	 lr:0.090288 loss:1.669645
[Train] epoch:69	batch id:300	 lr:0.090288 loss:1.592232
[Train] 69, loss: 1.662180, train acc: 0.865126, 
[Test] epoch:69	batch id:0	 loss:1.327577
[Test] epoch:69	batch id:10	 loss:1.434842
[Test] epoch:69	batch id:20	 loss:1.456390
[Test] epoch:69	batch id:30	 loss:1.504192
[Test] epoch:69	batch id:40	 loss:1.471295
[Test] epoch:69	batch id:50	 loss:1.455157
[Test] epoch:69	batch id:60	 loss:1.323952
[Test] epoch:69	batch id:70	 loss:1.528237
[Test] epoch:69	batch id:80	 loss:1.708140
[Test] epoch:69	batch id:90	 loss:1.570016
[Test] epoch:69	batch id:100	 loss:1.895417
[Test] epoch:69	batch id:110	 loss:1.424116
[Test] epoch:69	batch id:120	 loss:1.398579
[Test] epoch:69	batch id:130	 loss:1.454375
[Test] epoch:69	batch id:140	 loss:1.416738
[Test] epoch:69	batch id:150	 loss:1.863497
[Test] 69, loss: 1.541169, test acc: 0.903160,
Max Acc:0.903160
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:70	batch id:0	 lr:0.090022 loss:1.685462
[Train] epoch:70	batch id:10	 lr:0.090022 loss:1.837898
[Train] epoch:70	batch id:20	 lr:0.090022 loss:1.581647
[Train] epoch:70	batch id:30	 lr:0.090022 loss:1.635051
[Train] epoch:70	batch id:40	 lr:0.090022 loss:1.668564
[Train] epoch:70	batch id:50	 lr:0.090022 loss:1.665757
[Train] epoch:70	batch id:60	 lr:0.090022 loss:1.595173
[Train] epoch:70	batch id:70	 lr:0.090022 loss:1.674979
[Train] epoch:70	batch id:80	 lr:0.090022 loss:1.594348
[Train] epoch:70	batch id:90	 lr:0.090022 loss:1.584154
[Train] epoch:70	batch id:100	 lr:0.090022 loss:1.635882
[Train] epoch:70	batch id:110	 lr:0.090022 loss:1.630560
[Train] epoch:70	batch id:120	 lr:0.090022 loss:1.670271
[Train] epoch:70	batch id:130	 lr:0.090022 loss:1.750907
[Train] epoch:70	batch id:140	 lr:0.090022 loss:1.526804
[Train] epoch:70	batch id:150	 lr:0.090022 loss:1.812059
[Train] epoch:70	batch id:160	 lr:0.090022 loss:1.664771
[Train] epoch:70	batch id:170	 lr:0.090022 loss:1.659572
[Train] epoch:70	batch id:180	 lr:0.090022 loss:1.644450
[Train] epoch:70	batch id:190	 lr:0.090022 loss:1.573422
[Train] epoch:70	batch id:200	 lr:0.090022 loss:1.625693
[Train] epoch:70	batch id:210	 lr:0.090022 loss:1.693851
[Train] epoch:70	batch id:220	 lr:0.090022 loss:1.636811
[Train] epoch:70	batch id:230	 lr:0.090022 loss:1.550456
[Train] epoch:70	batch id:240	 lr:0.090022 loss:1.728685
[Train] epoch:70	batch id:250	 lr:0.090022 loss:1.738926
[Train] epoch:70	batch id:260	 lr:0.090022 loss:1.867912
[Train] epoch:70	batch id:270	 lr:0.090022 loss:1.616595
[Train] epoch:70	batch id:280	 lr:0.090022 loss:1.629581
[Train] epoch:70	batch id:290	 lr:0.090022 loss:1.475556
[Train] epoch:70	batch id:300	 lr:0.090022 loss:1.708267
[Train] 70, loss: 1.662188, train acc: 0.862581, 
[Test] epoch:70	batch id:0	 loss:1.385972
[Test] epoch:70	batch id:10	 loss:1.500440
[Test] epoch:70	batch id:20	 loss:1.492578
[Test] epoch:70	batch id:30	 loss:1.614161
[Test] epoch:70	batch id:40	 loss:1.427708
[Test] epoch:70	batch id:50	 loss:1.523123
[Test] epoch:70	batch id:60	 loss:1.376801
[Test] epoch:70	batch id:70	 loss:1.519849
[Test] epoch:70	batch id:80	 loss:1.693731
[Test] epoch:70	batch id:90	 loss:1.428623
[Test] epoch:70	batch id:100	 loss:2.055651
[Test] epoch:70	batch id:110	 loss:1.443315
[Test] epoch:70	batch id:120	 loss:1.500334
[Test] epoch:70	batch id:130	 loss:1.421759
[Test] epoch:70	batch id:140	 loss:1.476981
[Test] epoch:70	batch id:150	 loss:1.920122
[Test] 70, loss: 1.550015, test acc: 0.897488,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:71	batch id:0	 lr:0.089752 loss:1.850330
[Train] epoch:71	batch id:10	 lr:0.089752 loss:1.738960
[Train] epoch:71	batch id:20	 lr:0.089752 loss:1.623795
[Train] epoch:71	batch id:30	 lr:0.089752 loss:1.659373
[Train] epoch:71	batch id:40	 lr:0.089752 loss:1.773663
[Train] epoch:71	batch id:50	 lr:0.089752 loss:1.739074
[Train] epoch:71	batch id:60	 lr:0.089752 loss:1.546713
[Train] epoch:71	batch id:70	 lr:0.089752 loss:1.754557
[Train] epoch:71	batch id:80	 lr:0.089752 loss:1.561548
[Train] epoch:71	batch id:90	 lr:0.089752 loss:1.722270
[Train] epoch:71	batch id:100	 lr:0.089752 loss:1.561231
[Train] epoch:71	batch id:110	 lr:0.089752 loss:1.765597
[Train] epoch:71	batch id:120	 lr:0.089752 loss:1.669461
[Train] epoch:71	batch id:130	 lr:0.089752 loss:1.683196
[Train] epoch:71	batch id:140	 lr:0.089752 loss:1.567327
[Train] epoch:71	batch id:150	 lr:0.089752 loss:1.683875
[Train] epoch:71	batch id:160	 lr:0.089752 loss:1.450225
[Train] epoch:71	batch id:170	 lr:0.089752 loss:1.668143
[Train] epoch:71	batch id:180	 lr:0.089752 loss:1.789753
[Train] epoch:71	batch id:190	 lr:0.089752 loss:1.643858
[Train] epoch:71	batch id:200	 lr:0.089752 loss:1.687573
[Train] epoch:71	batch id:210	 lr:0.089752 loss:1.661904
[Train] epoch:71	batch id:220	 lr:0.089752 loss:1.736367
[Train] epoch:71	batch id:230	 lr:0.089752 loss:1.551420
[Train] epoch:71	batch id:240	 lr:0.089752 loss:1.715089
[Train] epoch:71	batch id:250	 lr:0.089752 loss:1.711408
[Train] epoch:71	batch id:260	 lr:0.089752 loss:1.699385
[Train] epoch:71	batch id:270	 lr:0.089752 loss:1.704622
[Train] epoch:71	batch id:280	 lr:0.089752 loss:1.618790
[Train] epoch:71	batch id:290	 lr:0.089752 loss:1.512537
[Train] epoch:71	batch id:300	 lr:0.089752 loss:1.647144
[Train] 71, loss: 1.660195, train acc: 0.866958, 
[Test] epoch:71	batch id:0	 loss:1.466989
[Test] epoch:71	batch id:10	 loss:1.517703
[Test] epoch:71	batch id:20	 loss:1.471413
[Test] epoch:71	batch id:30	 loss:1.580468
[Test] epoch:71	batch id:40	 loss:1.517733
[Test] epoch:71	batch id:50	 loss:1.443444
[Test] epoch:71	batch id:60	 loss:1.340501
[Test] epoch:71	batch id:70	 loss:1.494865
[Test] epoch:71	batch id:80	 loss:1.698460
[Test] epoch:71	batch id:90	 loss:1.500565
[Test] epoch:71	batch id:100	 loss:1.906378
[Test] epoch:71	batch id:110	 loss:1.571965
[Test] epoch:71	batch id:120	 loss:1.502263
[Test] epoch:71	batch id:130	 loss:1.739107
[Test] epoch:71	batch id:140	 loss:1.535189
[Test] epoch:71	batch id:150	 loss:1.809532
[Test] 71, loss: 1.559178, test acc: 0.892626,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:72	batch id:0	 lr:0.089480 loss:1.727983
[Train] epoch:72	batch id:10	 lr:0.089480 loss:1.508707
[Train] epoch:72	batch id:20	 lr:0.089480 loss:1.526362
[Train] epoch:72	batch id:30	 lr:0.089480 loss:1.572542
[Train] epoch:72	batch id:40	 lr:0.089480 loss:1.685253
[Train] epoch:72	batch id:50	 lr:0.089480 loss:1.690509
[Train] epoch:72	batch id:60	 lr:0.089480 loss:1.614808
[Train] epoch:72	batch id:70	 lr:0.089480 loss:1.707526
[Train] epoch:72	batch id:80	 lr:0.089480 loss:1.674759
[Train] epoch:72	batch id:90	 lr:0.089480 loss:1.790532
[Train] epoch:72	batch id:100	 lr:0.089480 loss:1.604833
[Train] epoch:72	batch id:110	 lr:0.089480 loss:1.468058
[Train] epoch:72	batch id:120	 lr:0.089480 loss:1.527117
[Train] epoch:72	batch id:130	 lr:0.089480 loss:1.518556
[Train] epoch:72	batch id:140	 lr:0.089480 loss:1.731261
[Train] epoch:72	batch id:150	 lr:0.089480 loss:1.568280
[Train] epoch:72	batch id:160	 lr:0.089480 loss:1.535628
[Train] epoch:72	batch id:170	 lr:0.089480 loss:1.689391
[Train] epoch:72	batch id:180	 lr:0.089480 loss:1.598272
[Train] epoch:72	batch id:190	 lr:0.089480 loss:1.714725
[Train] epoch:72	batch id:200	 lr:0.089480 loss:1.605648
[Train] epoch:72	batch id:210	 lr:0.089480 loss:1.613005
[Train] epoch:72	batch id:220	 lr:0.089480 loss:1.642379
[Train] epoch:72	batch id:230	 lr:0.089480 loss:1.665787
[Train] epoch:72	batch id:240	 lr:0.089480 loss:1.646305
[Train] epoch:72	batch id:250	 lr:0.089480 loss:1.679783
[Train] epoch:72	batch id:260	 lr:0.089480 loss:1.905245
[Train] epoch:72	batch id:270	 lr:0.089480 loss:1.736932
[Train] epoch:72	batch id:280	 lr:0.089480 loss:1.607023
[Train] epoch:72	batch id:290	 lr:0.089480 loss:1.848577
[Train] epoch:72	batch id:300	 lr:0.089480 loss:1.661792
[Train] 72, loss: 1.652207, train acc: 0.869503, 
[Test] epoch:72	batch id:0	 loss:1.372046
[Test] epoch:72	batch id:10	 loss:1.538487
[Test] epoch:72	batch id:20	 loss:1.571536
[Test] epoch:72	batch id:30	 loss:1.551722
[Test] epoch:72	batch id:40	 loss:1.484631
[Test] epoch:72	batch id:50	 loss:1.386498
[Test] epoch:72	batch id:60	 loss:1.309753
[Test] epoch:72	batch id:70	 loss:1.464584
[Test] epoch:72	batch id:80	 loss:1.639861
[Test] epoch:72	batch id:90	 loss:1.542942
[Test] epoch:72	batch id:100	 loss:1.857715
[Test] epoch:72	batch id:110	 loss:1.482612
[Test] epoch:72	batch id:120	 loss:1.424659
[Test] epoch:72	batch id:130	 loss:1.513600
[Test] epoch:72	batch id:140	 loss:1.468039
[Test] epoch:72	batch id:150	 loss:1.963523
[Test] 72, loss: 1.540170, test acc: 0.902755,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:73	batch id:0	 lr:0.089205 loss:1.586036
[Train] epoch:73	batch id:10	 lr:0.089205 loss:1.612776
[Train] epoch:73	batch id:20	 lr:0.089205 loss:1.457935
[Train] epoch:73	batch id:30	 lr:0.089205 loss:1.709080
[Train] epoch:73	batch id:40	 lr:0.089205 loss:1.512861
[Train] epoch:73	batch id:50	 lr:0.089205 loss:1.598406
[Train] epoch:73	batch id:60	 lr:0.089205 loss:1.588382
[Train] epoch:73	batch id:70	 lr:0.089205 loss:1.777709
[Train] epoch:73	batch id:80	 lr:0.089205 loss:1.637589
[Train] epoch:73	batch id:90	 lr:0.089205 loss:1.807085
[Train] epoch:73	batch id:100	 lr:0.089205 loss:1.776188
[Train] epoch:73	batch id:110	 lr:0.089205 loss:1.917449
[Train] epoch:73	batch id:120	 lr:0.089205 loss:1.849890
[Train] epoch:73	batch id:130	 lr:0.089205 loss:1.628775
[Train] epoch:73	batch id:140	 lr:0.089205 loss:1.609731
[Train] epoch:73	batch id:150	 lr:0.089205 loss:1.578254
[Train] epoch:73	batch id:160	 lr:0.089205 loss:1.767083
[Train] epoch:73	batch id:170	 lr:0.089205 loss:1.596287
[Train] epoch:73	batch id:180	 lr:0.089205 loss:1.720119
[Train] epoch:73	batch id:190	 lr:0.089205 loss:1.680507
[Train] epoch:73	batch id:200	 lr:0.089205 loss:1.712735
[Train] epoch:73	batch id:210	 lr:0.089205 loss:1.690401
[Train] epoch:73	batch id:220	 lr:0.089205 loss:1.591994
[Train] epoch:73	batch id:230	 lr:0.089205 loss:1.592155
[Train] epoch:73	batch id:240	 lr:0.089205 loss:1.841141
[Train] epoch:73	batch id:250	 lr:0.089205 loss:1.750028
[Train] epoch:73	batch id:260	 lr:0.089205 loss:1.893097
[Train] epoch:73	batch id:270	 lr:0.089205 loss:1.615851
[Train] epoch:73	batch id:280	 lr:0.089205 loss:1.754820
[Train] epoch:73	batch id:290	 lr:0.089205 loss:1.646044
[Train] epoch:73	batch id:300	 lr:0.089205 loss:1.694861
[Train] 73, loss: 1.646560, train acc: 0.870318, 
[Test] epoch:73	batch id:0	 loss:1.345079
[Test] epoch:73	batch id:10	 loss:1.467042
[Test] epoch:73	batch id:20	 loss:1.446779
[Test] epoch:73	batch id:30	 loss:1.391986
[Test] epoch:73	batch id:40	 loss:1.399160
[Test] epoch:73	batch id:50	 loss:1.414675
[Test] epoch:73	batch id:60	 loss:1.298681
[Test] epoch:73	batch id:70	 loss:1.482845
[Test] epoch:73	batch id:80	 loss:1.642411
[Test] epoch:73	batch id:90	 loss:1.446869
[Test] epoch:73	batch id:100	 loss:1.940031
[Test] epoch:73	batch id:110	 loss:1.404276
[Test] epoch:73	batch id:120	 loss:1.480806
[Test] epoch:73	batch id:130	 loss:1.445316
[Test] epoch:73	batch id:140	 loss:1.385053
[Test] epoch:73	batch id:150	 loss:1.829131
[Test] 73, loss: 1.525802, test acc: 0.911669,
Max Acc:0.911669
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:74	batch id:0	 lr:0.088926 loss:1.615918
[Train] epoch:74	batch id:10	 lr:0.088926 loss:1.629425
[Train] epoch:74	batch id:20	 lr:0.088926 loss:1.803749
[Train] epoch:74	batch id:30	 lr:0.088926 loss:1.642258
[Train] epoch:74	batch id:40	 lr:0.088926 loss:1.556679
[Train] epoch:74	batch id:50	 lr:0.088926 loss:1.587205
[Train] epoch:74	batch id:60	 lr:0.088926 loss:1.588139
[Train] epoch:74	batch id:70	 lr:0.088926 loss:1.554880
[Train] epoch:74	batch id:80	 lr:0.088926 loss:1.599983
[Train] epoch:74	batch id:90	 lr:0.088926 loss:1.650388
[Train] epoch:74	batch id:100	 lr:0.088926 loss:1.845009
[Train] epoch:74	batch id:110	 lr:0.088926 loss:1.718334
[Train] epoch:74	batch id:120	 lr:0.088926 loss:1.752624
[Train] epoch:74	batch id:130	 lr:0.088926 loss:1.755138
[Train] epoch:74	batch id:140	 lr:0.088926 loss:1.650072
[Train] epoch:74	batch id:150	 lr:0.088926 loss:1.805825
[Train] epoch:74	batch id:160	 lr:0.088926 loss:1.716668
[Train] epoch:74	batch id:170	 lr:0.088926 loss:1.563430
[Train] epoch:74	batch id:180	 lr:0.088926 loss:1.644146
[Train] epoch:74	batch id:190	 lr:0.088926 loss:1.756206
[Train] epoch:74	batch id:200	 lr:0.088926 loss:1.649569
[Train] epoch:74	batch id:210	 lr:0.088926 loss:1.466038
[Train] epoch:74	batch id:220	 lr:0.088926 loss:1.675041
[Train] epoch:74	batch id:230	 lr:0.088926 loss:1.701587
[Train] epoch:74	batch id:240	 lr:0.088926 loss:1.722618
[Train] epoch:74	batch id:250	 lr:0.088926 loss:1.696951
[Train] epoch:74	batch id:260	 lr:0.088926 loss:1.620163
[Train] epoch:74	batch id:270	 lr:0.088926 loss:1.499398
[Train] epoch:74	batch id:280	 lr:0.088926 loss:1.607723
[Train] epoch:74	batch id:290	 lr:0.088926 loss:1.621009
[Train] epoch:74	batch id:300	 lr:0.088926 loss:1.558200
[Train] 74, loss: 1.657129, train acc: 0.867976, 
[Test] epoch:74	batch id:0	 loss:1.438115
[Test] epoch:74	batch id:10	 loss:1.488332
[Test] epoch:74	batch id:20	 loss:1.481226
[Test] epoch:74	batch id:30	 loss:1.732974
[Test] epoch:74	batch id:40	 loss:1.543632
[Test] epoch:74	batch id:50	 loss:1.491439
[Test] epoch:74	batch id:60	 loss:1.367807
[Test] epoch:74	batch id:70	 loss:1.603021
[Test] epoch:74	batch id:80	 loss:1.735893
[Test] epoch:74	batch id:90	 loss:1.493321
[Test] epoch:74	batch id:100	 loss:1.864864
[Test] epoch:74	batch id:110	 loss:1.473297
[Test] epoch:74	batch id:120	 loss:1.473775
[Test] epoch:74	batch id:130	 loss:1.495598
[Test] epoch:74	batch id:140	 loss:1.483877
[Test] epoch:74	batch id:150	 loss:1.810922
[Test] 74, loss: 1.572435, test acc: 0.890194,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:75	batch id:0	 lr:0.088644 loss:1.547133
[Train] epoch:75	batch id:10	 lr:0.088644 loss:1.882058
[Train] epoch:75	batch id:20	 lr:0.088644 loss:1.765616
[Train] epoch:75	batch id:30	 lr:0.088644 loss:1.633422
[Train] epoch:75	batch id:40	 lr:0.088644 loss:1.694419
[Train] epoch:75	batch id:50	 lr:0.088644 loss:1.651267
[Train] epoch:75	batch id:60	 lr:0.088644 loss:1.601465
[Train] epoch:75	batch id:70	 lr:0.088644 loss:1.577258
[Train] epoch:75	batch id:80	 lr:0.088644 loss:1.494421
[Train] epoch:75	batch id:90	 lr:0.088644 loss:1.640660
[Train] epoch:75	batch id:100	 lr:0.088644 loss:1.626710
[Train] epoch:75	batch id:110	 lr:0.088644 loss:1.435340
[Train] epoch:75	batch id:120	 lr:0.088644 loss:1.607721
[Train] epoch:75	batch id:130	 lr:0.088644 loss:1.549974
[Train] epoch:75	batch id:140	 lr:0.088644 loss:1.524942
[Train] epoch:75	batch id:150	 lr:0.088644 loss:1.691409
[Train] epoch:75	batch id:160	 lr:0.088644 loss:1.693437
[Train] epoch:75	batch id:170	 lr:0.088644 loss:1.607316
[Train] epoch:75	batch id:180	 lr:0.088644 loss:1.517650
[Train] epoch:75	batch id:190	 lr:0.088644 loss:1.472649
[Train] epoch:75	batch id:200	 lr:0.088644 loss:1.650091
[Train] epoch:75	batch id:210	 lr:0.088644 loss:1.542765
[Train] epoch:75	batch id:220	 lr:0.088644 loss:1.606250
[Train] epoch:75	batch id:230	 lr:0.088644 loss:1.847463
[Train] epoch:75	batch id:240	 lr:0.088644 loss:1.459373
[Train] epoch:75	batch id:250	 lr:0.088644 loss:1.610609
[Train] epoch:75	batch id:260	 lr:0.088644 loss:1.637332
[Train] epoch:75	batch id:270	 lr:0.088644 loss:1.848648
[Train] epoch:75	batch id:280	 lr:0.088644 loss:1.597767
[Train] epoch:75	batch id:290	 lr:0.088644 loss:1.530091
[Train] epoch:75	batch id:300	 lr:0.088644 loss:1.788254
[Train] 75, loss: 1.636813, train acc: 0.876018, 
[Test] epoch:75	batch id:0	 loss:1.307114
[Test] epoch:75	batch id:10	 loss:1.553859
[Test] epoch:75	batch id:20	 loss:1.505128
[Test] epoch:75	batch id:30	 loss:1.459763
[Test] epoch:75	batch id:40	 loss:1.443201
[Test] epoch:75	batch id:50	 loss:1.456515
[Test] epoch:75	batch id:60	 loss:1.318059
[Test] epoch:75	batch id:70	 loss:1.441273
[Test] epoch:75	batch id:80	 loss:1.684767
[Test] epoch:75	batch id:90	 loss:1.432156
[Test] epoch:75	batch id:100	 loss:2.080368
[Test] epoch:75	batch id:110	 loss:1.479964
[Test] epoch:75	batch id:120	 loss:1.495545
[Test] epoch:75	batch id:130	 loss:1.385028
[Test] epoch:75	batch id:140	 loss:1.425642
[Test] epoch:75	batch id:150	 loss:1.876074
[Test] 75, loss: 1.541828, test acc: 0.893436,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:76	batch id:0	 lr:0.088360 loss:1.596358
[Train] epoch:76	batch id:10	 lr:0.088360 loss:1.712581
[Train] epoch:76	batch id:20	 lr:0.088360 loss:1.531774
[Train] epoch:76	batch id:30	 lr:0.088360 loss:1.488172
[Train] epoch:76	batch id:40	 lr:0.088360 loss:1.544850
[Train] epoch:76	batch id:50	 lr:0.088360 loss:1.604456
[Train] epoch:76	batch id:60	 lr:0.088360 loss:1.629547
[Train] epoch:76	batch id:70	 lr:0.088360 loss:1.658870
[Train] epoch:76	batch id:80	 lr:0.088360 loss:1.668254
[Train] epoch:76	batch id:90	 lr:0.088360 loss:1.604594
[Train] epoch:76	batch id:100	 lr:0.088360 loss:1.571349
[Train] epoch:76	batch id:110	 lr:0.088360 loss:1.564009
[Train] epoch:76	batch id:120	 lr:0.088360 loss:1.564245
[Train] epoch:76	batch id:130	 lr:0.088360 loss:1.497921
[Train] epoch:76	batch id:140	 lr:0.088360 loss:1.830300
[Train] epoch:76	batch id:150	 lr:0.088360 loss:1.719261
[Train] epoch:76	batch id:160	 lr:0.088360 loss:1.602495
[Train] epoch:76	batch id:170	 lr:0.088360 loss:1.541100
[Train] epoch:76	batch id:180	 lr:0.088360 loss:1.669066
[Train] epoch:76	batch id:190	 lr:0.088360 loss:1.568364
[Train] epoch:76	batch id:200	 lr:0.088360 loss:1.578241
[Train] epoch:76	batch id:210	 lr:0.088360 loss:1.739679
[Train] epoch:76	batch id:220	 lr:0.088360 loss:1.485915
[Train] epoch:76	batch id:230	 lr:0.088360 loss:1.749242
[Train] epoch:76	batch id:240	 lr:0.088360 loss:1.577764
[Train] epoch:76	batch id:250	 lr:0.088360 loss:1.659591
[Train] epoch:76	batch id:260	 lr:0.088360 loss:1.667453
[Train] epoch:76	batch id:270	 lr:0.088360 loss:1.484065
[Train] epoch:76	batch id:280	 lr:0.088360 loss:1.577855
[Train] epoch:76	batch id:290	 lr:0.088360 loss:1.674655
[Train] epoch:76	batch id:300	 lr:0.088360 loss:1.718846
[Train] 76, loss: 1.644563, train acc: 0.874186, 
[Test] epoch:76	batch id:0	 loss:1.385700
[Test] epoch:76	batch id:10	 loss:1.466970
[Test] epoch:76	batch id:20	 loss:1.469303
[Test] epoch:76	batch id:30	 loss:1.451644
[Test] epoch:76	batch id:40	 loss:1.453114
[Test] epoch:76	batch id:50	 loss:1.486705
[Test] epoch:76	batch id:60	 loss:1.309432
[Test] epoch:76	batch id:70	 loss:1.439963
[Test] epoch:76	batch id:80	 loss:1.660801
[Test] epoch:76	batch id:90	 loss:1.522877
[Test] epoch:76	batch id:100	 loss:1.889368
[Test] epoch:76	batch id:110	 loss:1.397493
[Test] epoch:76	batch id:120	 loss:1.437549
[Test] epoch:76	batch id:130	 loss:1.546386
[Test] epoch:76	batch id:140	 loss:1.529894
[Test] epoch:76	batch id:150	 loss:1.844343
[Test] 76, loss: 1.544621, test acc: 0.901135,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:77	batch id:0	 lr:0.088072 loss:1.785805
[Train] epoch:77	batch id:10	 lr:0.088072 loss:1.630212
[Train] epoch:77	batch id:20	 lr:0.088072 loss:1.509483
[Train] epoch:77	batch id:30	 lr:0.088072 loss:1.687304
[Train] epoch:77	batch id:40	 lr:0.088072 loss:1.573271
[Train] epoch:77	batch id:50	 lr:0.088072 loss:1.761949
[Train] epoch:77	batch id:60	 lr:0.088072 loss:1.586619
[Train] epoch:77	batch id:70	 lr:0.088072 loss:1.525687
[Train] epoch:77	batch id:80	 lr:0.088072 loss:1.583587
[Train] epoch:77	batch id:90	 lr:0.088072 loss:1.591368
[Train] epoch:77	batch id:100	 lr:0.088072 loss:1.476887
[Train] epoch:77	batch id:110	 lr:0.088072 loss:1.597936
[Train] epoch:77	batch id:120	 lr:0.088072 loss:1.711081
[Train] epoch:77	batch id:130	 lr:0.088072 loss:1.678775
[Train] epoch:77	batch id:140	 lr:0.088072 loss:1.471473
[Train] epoch:77	batch id:150	 lr:0.088072 loss:1.530540
[Train] epoch:77	batch id:160	 lr:0.088072 loss:1.663927
[Train] epoch:77	batch id:170	 lr:0.088072 loss:1.681897
[Train] epoch:77	batch id:180	 lr:0.088072 loss:1.562811
[Train] epoch:77	batch id:190	 lr:0.088072 loss:1.562684
[Train] epoch:77	batch id:200	 lr:0.088072 loss:1.718469
[Train] epoch:77	batch id:210	 lr:0.088072 loss:1.578364
[Train] epoch:77	batch id:220	 lr:0.088072 loss:1.587387
[Train] epoch:77	batch id:230	 lr:0.088072 loss:1.555278
[Train] epoch:77	batch id:240	 lr:0.088072 loss:1.732242
[Train] epoch:77	batch id:250	 lr:0.088072 loss:1.424974
[Train] epoch:77	batch id:260	 lr:0.088072 loss:1.489843
[Train] epoch:77	batch id:270	 lr:0.088072 loss:1.575584
[Train] epoch:77	batch id:280	 lr:0.088072 loss:1.579728
[Train] epoch:77	batch id:290	 lr:0.088072 loss:1.442616
[Train] epoch:77	batch id:300	 lr:0.088072 loss:1.974048
[Train] 77, loss: 1.637976, train acc: 0.880090, 
[Test] epoch:77	batch id:0	 loss:1.332908
[Test] epoch:77	batch id:10	 loss:1.451976
[Test] epoch:77	batch id:20	 loss:1.456241
[Test] epoch:77	batch id:30	 loss:1.549909
[Test] epoch:77	batch id:40	 loss:1.472486
[Test] epoch:77	batch id:50	 loss:1.480965
[Test] epoch:77	batch id:60	 loss:1.335074
[Test] epoch:77	batch id:70	 loss:1.449341
[Test] epoch:77	batch id:80	 loss:1.654323
[Test] epoch:77	batch id:90	 loss:1.710269
[Test] epoch:77	batch id:100	 loss:1.853078
[Test] epoch:77	batch id:110	 loss:1.429608
[Test] epoch:77	batch id:120	 loss:1.429699
[Test] epoch:77	batch id:130	 loss:1.468963
[Test] epoch:77	batch id:140	 loss:1.435632
[Test] epoch:77	batch id:150	 loss:1.791714
[Test] 77, loss: 1.546945, test acc: 0.903160,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:78	batch id:0	 lr:0.087781 loss:1.640888
[Train] epoch:78	batch id:10	 lr:0.087781 loss:1.730628
[Train] epoch:78	batch id:20	 lr:0.087781 loss:1.460490
[Train] epoch:78	batch id:30	 lr:0.087781 loss:1.582767
[Train] epoch:78	batch id:40	 lr:0.087781 loss:1.579690
[Train] epoch:78	batch id:50	 lr:0.087781 loss:1.680774
[Train] epoch:78	batch id:60	 lr:0.087781 loss:1.555904
[Train] epoch:78	batch id:70	 lr:0.087781 loss:1.601042
[Train] epoch:78	batch id:80	 lr:0.087781 loss:1.594155
[Train] epoch:78	batch id:90	 lr:0.087781 loss:1.639868
[Train] epoch:78	batch id:100	 lr:0.087781 loss:1.621245
[Train] epoch:78	batch id:110	 lr:0.087781 loss:1.478690
[Train] epoch:78	batch id:120	 lr:0.087781 loss:1.889844
[Train] epoch:78	batch id:130	 lr:0.087781 loss:1.642558
[Train] epoch:78	batch id:140	 lr:0.087781 loss:1.667305
[Train] epoch:78	batch id:150	 lr:0.087781 loss:1.672962
[Train] epoch:78	batch id:160	 lr:0.087781 loss:1.754581
[Train] epoch:78	batch id:170	 lr:0.087781 loss:1.666691
[Train] epoch:78	batch id:180	 lr:0.087781 loss:1.612896
[Train] epoch:78	batch id:190	 lr:0.087781 loss:1.481177
[Train] epoch:78	batch id:200	 lr:0.087781 loss:1.516915
[Train] epoch:78	batch id:210	 lr:0.087781 loss:1.588170
[Train] epoch:78	batch id:220	 lr:0.087781 loss:1.512176
[Train] epoch:78	batch id:230	 lr:0.087781 loss:1.645782
[Train] epoch:78	batch id:240	 lr:0.087781 loss:1.673347
[Train] epoch:78	batch id:250	 lr:0.087781 loss:1.681054
[Train] epoch:78	batch id:260	 lr:0.087781 loss:1.670928
[Train] epoch:78	batch id:270	 lr:0.087781 loss:1.582798
[Train] epoch:78	batch id:280	 lr:0.087781 loss:1.923369
[Train] epoch:78	batch id:290	 lr:0.087781 loss:1.573246
[Train] epoch:78	batch id:300	 lr:0.087781 loss:1.824358
[Train] 78, loss: 1.640878, train acc: 0.873371, 
[Test] epoch:78	batch id:0	 loss:1.373827
[Test] epoch:78	batch id:10	 loss:1.481684
[Test] epoch:78	batch id:20	 loss:1.448892
[Test] epoch:78	batch id:30	 loss:1.553347
[Test] epoch:78	batch id:40	 loss:1.459659
[Test] epoch:78	batch id:50	 loss:1.600559
[Test] epoch:78	batch id:60	 loss:1.318230
[Test] epoch:78	batch id:70	 loss:1.569916
[Test] epoch:78	batch id:80	 loss:1.694031
[Test] epoch:78	batch id:90	 loss:1.595333
[Test] epoch:78	batch id:100	 loss:1.893464
[Test] epoch:78	batch id:110	 loss:1.440790
[Test] epoch:78	batch id:120	 loss:1.470805
[Test] epoch:78	batch id:130	 loss:1.469174
[Test] epoch:78	batch id:140	 loss:1.427155
[Test] epoch:78	batch id:150	 loss:2.075520
[Test] 78, loss: 1.567834, test acc: 0.880470,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:79	batch id:0	 lr:0.087487 loss:1.602358
[Train] epoch:79	batch id:10	 lr:0.087487 loss:1.694730
[Train] epoch:79	batch id:20	 lr:0.087487 loss:1.634221
[Train] epoch:79	batch id:30	 lr:0.087487 loss:1.747924
[Train] epoch:79	batch id:40	 lr:0.087487 loss:1.668584
[Train] epoch:79	batch id:50	 lr:0.087487 loss:1.603173
[Train] epoch:79	batch id:60	 lr:0.087487 loss:1.577598
[Train] epoch:79	batch id:70	 lr:0.087487 loss:1.555298
[Train] epoch:79	batch id:80	 lr:0.087487 loss:1.480352
[Train] epoch:79	batch id:90	 lr:0.087487 loss:1.564466
[Train] epoch:79	batch id:100	 lr:0.087487 loss:1.674377
[Train] epoch:79	batch id:110	 lr:0.087487 loss:1.699665
[Train] epoch:79	batch id:120	 lr:0.087487 loss:1.619241
[Train] epoch:79	batch id:130	 lr:0.087487 loss:1.606174
[Train] epoch:79	batch id:140	 lr:0.087487 loss:1.733257
[Train] epoch:79	batch id:150	 lr:0.087487 loss:1.710743
[Train] epoch:79	batch id:160	 lr:0.087487 loss:1.553924
[Train] epoch:79	batch id:170	 lr:0.087487 loss:1.713000
[Train] epoch:79	batch id:180	 lr:0.087487 loss:1.744800
[Train] epoch:79	batch id:190	 lr:0.087487 loss:1.731251
[Train] epoch:79	batch id:200	 lr:0.087487 loss:1.555889
[Train] epoch:79	batch id:210	 lr:0.087487 loss:1.625920
[Train] epoch:79	batch id:220	 lr:0.087487 loss:1.590520
[Train] epoch:79	batch id:230	 lr:0.087487 loss:1.577757
[Train] epoch:79	batch id:240	 lr:0.087487 loss:1.655414
[Train] epoch:79	batch id:250	 lr:0.087487 loss:1.775925
[Train] epoch:79	batch id:260	 lr:0.087487 loss:1.688325
[Train] epoch:79	batch id:270	 lr:0.087487 loss:1.689849
[Train] epoch:79	batch id:280	 lr:0.087487 loss:1.497478
[Train] epoch:79	batch id:290	 lr:0.087487 loss:1.649507
[Train] epoch:79	batch id:300	 lr:0.087487 loss:1.476087
[Train] 79, loss: 1.639044, train acc: 0.875509, 
[Test] epoch:79	batch id:0	 loss:1.458955
[Test] epoch:79	batch id:10	 loss:1.534071
[Test] epoch:79	batch id:20	 loss:1.462683
[Test] epoch:79	batch id:30	 loss:1.504373
[Test] epoch:79	batch id:40	 loss:1.481137
[Test] epoch:79	batch id:50	 loss:1.505051
[Test] epoch:79	batch id:60	 loss:1.342110
[Test] epoch:79	batch id:70	 loss:1.599652
[Test] epoch:79	batch id:80	 loss:1.737273
[Test] epoch:79	batch id:90	 loss:1.633869
[Test] epoch:79	batch id:100	 loss:1.911108
[Test] epoch:79	batch id:110	 loss:1.501883
[Test] epoch:79	batch id:120	 loss:1.436339
[Test] epoch:79	batch id:130	 loss:1.594180
[Test] epoch:79	batch id:140	 loss:1.514483
[Test] epoch:79	batch id:150	 loss:1.850660
[Test] 79, loss: 1.583025, test acc: 0.883306,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:80	batch id:0	 lr:0.087190 loss:1.611256
[Train] epoch:80	batch id:10	 lr:0.087190 loss:1.706464
[Train] epoch:80	batch id:20	 lr:0.087190 loss:1.717763
[Train] epoch:80	batch id:30	 lr:0.087190 loss:1.585832
[Train] epoch:80	batch id:40	 lr:0.087190 loss:1.887784
[Train] epoch:80	batch id:50	 lr:0.087190 loss:1.561634
[Train] epoch:80	batch id:60	 lr:0.087190 loss:1.700259
[Train] epoch:80	batch id:70	 lr:0.087190 loss:1.736537
[Train] epoch:80	batch id:80	 lr:0.087190 loss:1.631907
[Train] epoch:80	batch id:90	 lr:0.087190 loss:1.817927
[Train] epoch:80	batch id:100	 lr:0.087190 loss:1.513047
[Train] epoch:80	batch id:110	 lr:0.087190 loss:1.585462
[Train] epoch:80	batch id:120	 lr:0.087190 loss:1.744773
[Train] epoch:80	batch id:130	 lr:0.087190 loss:1.567335
[Train] epoch:80	batch id:140	 lr:0.087190 loss:1.579147
[Train] epoch:80	batch id:150	 lr:0.087190 loss:1.462278
[Train] epoch:80	batch id:160	 lr:0.087190 loss:1.620378
[Train] epoch:80	batch id:170	 lr:0.087190 loss:1.864780
[Train] epoch:80	batch id:180	 lr:0.087190 loss:1.589969
[Train] epoch:80	batch id:190	 lr:0.087190 loss:1.821960
[Train] epoch:80	batch id:200	 lr:0.087190 loss:1.587052
[Train] epoch:80	batch id:210	 lr:0.087190 loss:1.670512
[Train] epoch:80	batch id:220	 lr:0.087190 loss:1.745969
[Train] epoch:80	batch id:230	 lr:0.087190 loss:1.545781
[Train] epoch:80	batch id:240	 lr:0.087190 loss:1.626926
[Train] epoch:80	batch id:250	 lr:0.087190 loss:1.497252
[Train] epoch:80	batch id:260	 lr:0.087190 loss:1.670731
[Train] epoch:80	batch id:270	 lr:0.087190 loss:1.465553
[Train] epoch:80	batch id:280	 lr:0.087190 loss:1.659261
[Train] epoch:80	batch id:290	 lr:0.087190 loss:1.511339
[Train] epoch:80	batch id:300	 lr:0.087190 loss:1.583003
[Train] 80, loss: 1.640605, train acc: 0.873168, 
[Test] epoch:80	batch id:0	 loss:1.484438
[Test] epoch:80	batch id:10	 loss:1.573997
[Test] epoch:80	batch id:20	 loss:1.488503
[Test] epoch:80	batch id:30	 loss:1.607945
[Test] epoch:80	batch id:40	 loss:1.480289
[Test] epoch:80	batch id:50	 loss:1.447605
[Test] epoch:80	batch id:60	 loss:1.343227
[Test] epoch:80	batch id:70	 loss:1.538772
[Test] epoch:80	batch id:80	 loss:1.649230
[Test] epoch:80	batch id:90	 loss:1.549170
[Test] epoch:80	batch id:100	 loss:2.022745
[Test] epoch:80	batch id:110	 loss:1.557802
[Test] epoch:80	batch id:120	 loss:1.469643
[Test] epoch:80	batch id:130	 loss:1.444354
[Test] epoch:80	batch id:140	 loss:1.433208
[Test] epoch:80	batch id:150	 loss:1.810557
[Test] 80, loss: 1.563785, test acc: 0.890194,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:81	batch id:0	 lr:0.086891 loss:1.576207
[Train] epoch:81	batch id:10	 lr:0.086891 loss:1.691676
[Train] epoch:81	batch id:20	 lr:0.086891 loss:1.630009
[Train] epoch:81	batch id:30	 lr:0.086891 loss:1.776310
[Train] epoch:81	batch id:40	 lr:0.086891 loss:1.692345
[Train] epoch:81	batch id:50	 lr:0.086891 loss:1.575372
[Train] epoch:81	batch id:60	 lr:0.086891 loss:1.709787
[Train] epoch:81	batch id:70	 lr:0.086891 loss:1.589785
[Train] epoch:81	batch id:80	 lr:0.086891 loss:1.654223
[Train] epoch:81	batch id:90	 lr:0.086891 loss:1.588567
[Train] epoch:81	batch id:100	 lr:0.086891 loss:1.523088
[Train] epoch:81	batch id:110	 lr:0.086891 loss:1.496807
[Train] epoch:81	batch id:120	 lr:0.086891 loss:1.576583
[Train] epoch:81	batch id:130	 lr:0.086891 loss:1.591653
[Train] epoch:81	batch id:140	 lr:0.086891 loss:1.627236
[Train] epoch:81	batch id:150	 lr:0.086891 loss:1.634441
[Train] epoch:81	batch id:160	 lr:0.086891 loss:1.685868
[Train] epoch:81	batch id:170	 lr:0.086891 loss:1.715094
[Train] epoch:81	batch id:180	 lr:0.086891 loss:1.513465
[Train] epoch:81	batch id:190	 lr:0.086891 loss:1.392924
[Train] epoch:81	batch id:200	 lr:0.086891 loss:1.535208
[Train] epoch:81	batch id:210	 lr:0.086891 loss:1.672073
[Train] epoch:81	batch id:220	 lr:0.086891 loss:1.895239
[Train] epoch:81	batch id:230	 lr:0.086891 loss:1.821552
[Train] epoch:81	batch id:240	 lr:0.086891 loss:1.534592
[Train] epoch:81	batch id:250	 lr:0.086891 loss:1.675124
[Train] epoch:81	batch id:260	 lr:0.086891 loss:1.567817
[Train] epoch:81	batch id:270	 lr:0.086891 loss:1.511648
[Train] epoch:81	batch id:280	 lr:0.086891 loss:1.630238
[Train] epoch:81	batch id:290	 lr:0.086891 loss:1.528807
[Train] epoch:81	batch id:300	 lr:0.086891 loss:1.604435
[Train] 81, loss: 1.633166, train acc: 0.873066, 
[Test] epoch:81	batch id:0	 loss:1.442521
[Test] epoch:81	batch id:10	 loss:1.554301
[Test] epoch:81	batch id:20	 loss:1.447420
[Test] epoch:81	batch id:30	 loss:1.522897
[Test] epoch:81	batch id:40	 loss:1.410600
[Test] epoch:81	batch id:50	 loss:1.520865
[Test] epoch:81	batch id:60	 loss:1.329488
[Test] epoch:81	batch id:70	 loss:1.448670
[Test] epoch:81	batch id:80	 loss:1.690664
[Test] epoch:81	batch id:90	 loss:1.581068
[Test] epoch:81	batch id:100	 loss:1.927240
[Test] epoch:81	batch id:110	 loss:1.424240
[Test] epoch:81	batch id:120	 loss:1.461765
[Test] epoch:81	batch id:130	 loss:1.598573
[Test] epoch:81	batch id:140	 loss:1.395750
[Test] epoch:81	batch id:150	 loss:1.948382
[Test] 81, loss: 1.542769, test acc: 0.909643,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:82	batch id:0	 lr:0.086588 loss:1.665152
[Train] epoch:82	batch id:10	 lr:0.086588 loss:1.747545
[Train] epoch:82	batch id:20	 lr:0.086588 loss:1.549555
[Train] epoch:82	batch id:30	 lr:0.086588 loss:1.736592
[Train] epoch:82	batch id:40	 lr:0.086588 loss:1.552255
[Train] epoch:82	batch id:50	 lr:0.086588 loss:1.671411
[Train] epoch:82	batch id:60	 lr:0.086588 loss:1.563766
[Train] epoch:82	batch id:70	 lr:0.086588 loss:1.497852
[Train] epoch:82	batch id:80	 lr:0.086588 loss:1.694564
[Train] epoch:82	batch id:90	 lr:0.086588 loss:1.656558
[Train] epoch:82	batch id:100	 lr:0.086588 loss:1.458301
[Train] epoch:82	batch id:110	 lr:0.086588 loss:1.645823
[Train] epoch:82	batch id:120	 lr:0.086588 loss:1.577894
[Train] epoch:82	batch id:130	 lr:0.086588 loss:1.711347
[Train] epoch:82	batch id:140	 lr:0.086588 loss:1.654903
[Train] epoch:82	batch id:150	 lr:0.086588 loss:1.570993
[Train] epoch:82	batch id:160	 lr:0.086588 loss:1.688080
[Train] epoch:82	batch id:170	 lr:0.086588 loss:1.952276
[Train] epoch:82	batch id:180	 lr:0.086588 loss:1.683486
[Train] epoch:82	batch id:190	 lr:0.086588 loss:1.687195
[Train] epoch:82	batch id:200	 lr:0.086588 loss:1.611532
[Train] epoch:82	batch id:210	 lr:0.086588 loss:1.730406
[Train] epoch:82	batch id:220	 lr:0.086588 loss:1.575036
[Train] epoch:82	batch id:230	 lr:0.086588 loss:1.654651
[Train] epoch:82	batch id:240	 lr:0.086588 loss:1.518663
[Train] epoch:82	batch id:250	 lr:0.086588 loss:1.589016
[Train] epoch:82	batch id:260	 lr:0.086588 loss:1.889629
[Train] epoch:82	batch id:270	 lr:0.086588 loss:1.661462
[Train] epoch:82	batch id:280	 lr:0.086588 loss:1.745835
[Train] epoch:82	batch id:290	 lr:0.086588 loss:1.575217
[Train] epoch:82	batch id:300	 lr:0.086588 loss:1.771075
[Train] 82, loss: 1.633739, train acc: 0.874287, 
[Test] epoch:82	batch id:0	 loss:1.433094
[Test] epoch:82	batch id:10	 loss:1.500777
[Test] epoch:82	batch id:20	 loss:1.383548
[Test] epoch:82	batch id:30	 loss:1.622924
[Test] epoch:82	batch id:40	 loss:1.533648
[Test] epoch:82	batch id:50	 loss:1.442085
[Test] epoch:82	batch id:60	 loss:1.363912
[Test] epoch:82	batch id:70	 loss:1.452251
[Test] epoch:82	batch id:80	 loss:1.780447
[Test] epoch:82	batch id:90	 loss:1.569264
[Test] epoch:82	batch id:100	 loss:1.840864
[Test] epoch:82	batch id:110	 loss:1.429855
[Test] epoch:82	batch id:120	 loss:1.467225
[Test] epoch:82	batch id:130	 loss:1.515901
[Test] epoch:82	batch id:140	 loss:1.495347
[Test] epoch:82	batch id:150	 loss:2.074154
[Test] 82, loss: 1.574384, test acc: 0.897893,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:83	batch id:0	 lr:0.086282 loss:1.660829
[Train] epoch:83	batch id:10	 lr:0.086282 loss:1.466920
[Train] epoch:83	batch id:20	 lr:0.086282 loss:1.844337
[Train] epoch:83	batch id:30	 lr:0.086282 loss:1.667337
[Train] epoch:83	batch id:40	 lr:0.086282 loss:1.708659
[Train] epoch:83	batch id:50	 lr:0.086282 loss:1.595217
[Train] epoch:83	batch id:60	 lr:0.086282 loss:1.634466
[Train] epoch:83	batch id:70	 lr:0.086282 loss:1.617098
[Train] epoch:83	batch id:80	 lr:0.086282 loss:1.671669
[Train] epoch:83	batch id:90	 lr:0.086282 loss:1.779684
[Train] epoch:83	batch id:100	 lr:0.086282 loss:1.591012
[Train] epoch:83	batch id:110	 lr:0.086282 loss:1.568040
[Train] epoch:83	batch id:120	 lr:0.086282 loss:1.459560
[Train] epoch:83	batch id:130	 lr:0.086282 loss:1.630013
[Train] epoch:83	batch id:140	 lr:0.086282 loss:1.662512
[Train] epoch:83	batch id:150	 lr:0.086282 loss:1.668473
[Train] epoch:83	batch id:160	 lr:0.086282 loss:1.753050
[Train] epoch:83	batch id:170	 lr:0.086282 loss:1.473996
[Train] epoch:83	batch id:180	 lr:0.086282 loss:1.540600
[Train] epoch:83	batch id:190	 lr:0.086282 loss:1.605074
[Train] epoch:83	batch id:200	 lr:0.086282 loss:1.518982
[Train] epoch:83	batch id:210	 lr:0.086282 loss:1.435885
[Train] epoch:83	batch id:220	 lr:0.086282 loss:1.651339
[Train] epoch:83	batch id:230	 lr:0.086282 loss:1.611972
[Train] epoch:83	batch id:240	 lr:0.086282 loss:1.525378
[Train] epoch:83	batch id:250	 lr:0.086282 loss:1.602579
[Train] epoch:83	batch id:260	 lr:0.086282 loss:1.594273
[Train] epoch:83	batch id:270	 lr:0.086282 loss:1.639974
[Train] epoch:83	batch id:280	 lr:0.086282 loss:1.575259
[Train] epoch:83	batch id:290	 lr:0.086282 loss:1.654796
[Train] epoch:83	batch id:300	 lr:0.086282 loss:1.642781
[Train] 83, loss: 1.622995, train acc: 0.882024, 
[Test] epoch:83	batch id:0	 loss:1.370697
[Test] epoch:83	batch id:10	 loss:1.448541
[Test] epoch:83	batch id:20	 loss:1.521455
[Test] epoch:83	batch id:30	 loss:1.476119
[Test] epoch:83	batch id:40	 loss:1.490573
[Test] epoch:83	batch id:50	 loss:1.479754
[Test] epoch:83	batch id:60	 loss:1.356503
[Test] epoch:83	batch id:70	 loss:1.495928
[Test] epoch:83	batch id:80	 loss:1.764657
[Test] epoch:83	batch id:90	 loss:1.570610
[Test] epoch:83	batch id:100	 loss:1.772649
[Test] epoch:83	batch id:110	 loss:1.490108
[Test] epoch:83	batch id:120	 loss:1.502012
[Test] epoch:83	batch id:130	 loss:1.556989
[Test] epoch:83	batch id:140	 loss:1.531068
[Test] epoch:83	batch id:150	 loss:1.960262
[Test] 83, loss: 1.535055, test acc: 0.907618,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:84	batch id:0	 lr:0.085974 loss:1.680455
[Train] epoch:84	batch id:10	 lr:0.085974 loss:1.580467
[Train] epoch:84	batch id:20	 lr:0.085974 loss:1.482532
[Train] epoch:84	batch id:30	 lr:0.085974 loss:1.589263
[Train] epoch:84	batch id:40	 lr:0.085974 loss:1.537419
[Train] epoch:84	batch id:50	 lr:0.085974 loss:1.624559
[Train] epoch:84	batch id:60	 lr:0.085974 loss:1.561299
[Train] epoch:84	batch id:70	 lr:0.085974 loss:1.788922
[Train] epoch:84	batch id:80	 lr:0.085974 loss:1.746153
[Train] epoch:84	batch id:90	 lr:0.085974 loss:1.629290
[Train] epoch:84	batch id:100	 lr:0.085974 loss:1.827568
[Train] epoch:84	batch id:110	 lr:0.085974 loss:1.495888
[Train] epoch:84	batch id:120	 lr:0.085974 loss:1.712261
[Train] epoch:84	batch id:130	 lr:0.085974 loss:1.759098
[Train] epoch:84	batch id:140	 lr:0.085974 loss:1.777380
[Train] epoch:84	batch id:150	 lr:0.085974 loss:1.727606
[Train] epoch:84	batch id:160	 lr:0.085974 loss:1.625229
[Train] epoch:84	batch id:170	 lr:0.085974 loss:1.841041
[Train] epoch:84	batch id:180	 lr:0.085974 loss:1.758802
[Train] epoch:84	batch id:190	 lr:0.085974 loss:1.447677
[Train] epoch:84	batch id:200	 lr:0.085974 loss:1.435769
[Train] epoch:84	batch id:210	 lr:0.085974 loss:1.573026
[Train] epoch:84	batch id:220	 lr:0.085974 loss:1.693008
[Train] epoch:84	batch id:230	 lr:0.085974 loss:1.524171
[Train] epoch:84	batch id:240	 lr:0.085974 loss:1.697726
[Train] epoch:84	batch id:250	 lr:0.085974 loss:1.599168
[Train] epoch:84	batch id:260	 lr:0.085974 loss:1.678831
[Train] epoch:84	batch id:270	 lr:0.085974 loss:1.516692
[Train] epoch:84	batch id:280	 lr:0.085974 loss:1.526888
[Train] epoch:84	batch id:290	 lr:0.085974 loss:1.740093
[Train] epoch:84	batch id:300	 lr:0.085974 loss:1.680715
[Train] 84, loss: 1.630128, train acc: 0.883652, 
[Test] epoch:84	batch id:0	 loss:1.399757
[Test] epoch:84	batch id:10	 loss:1.475452
[Test] epoch:84	batch id:20	 loss:1.444591
[Test] epoch:84	batch id:30	 loss:1.612043
[Test] epoch:84	batch id:40	 loss:1.390375
[Test] epoch:84	batch id:50	 loss:1.523506
[Test] epoch:84	batch id:60	 loss:1.304028
[Test] epoch:84	batch id:70	 loss:1.468151
[Test] epoch:84	batch id:80	 loss:1.734077
[Test] epoch:84	batch id:90	 loss:1.526903
[Test] epoch:84	batch id:100	 loss:1.958638
[Test] epoch:84	batch id:110	 loss:1.381256
[Test] epoch:84	batch id:120	 loss:1.392612
[Test] epoch:84	batch id:130	 loss:1.439145
[Test] epoch:84	batch id:140	 loss:1.497977
[Test] epoch:84	batch id:150	 loss:1.778316
[Test] 84, loss: 1.547473, test acc: 0.889789,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:85	batch id:0	 lr:0.085662 loss:1.731821
[Train] epoch:85	batch id:10	 lr:0.085662 loss:1.468373
[Train] epoch:85	batch id:20	 lr:0.085662 loss:1.598185
[Train] epoch:85	batch id:30	 lr:0.085662 loss:1.558677
[Train] epoch:85	batch id:40	 lr:0.085662 loss:1.599162
[Train] epoch:85	batch id:50	 lr:0.085662 loss:1.573724
[Train] epoch:85	batch id:60	 lr:0.085662 loss:1.664851
[Train] epoch:85	batch id:70	 lr:0.085662 loss:1.791010
[Train] epoch:85	batch id:80	 lr:0.085662 loss:1.771326
[Train] epoch:85	batch id:90	 lr:0.085662 loss:1.537473
[Train] epoch:85	batch id:100	 lr:0.085662 loss:1.529459
[Train] epoch:85	batch id:110	 lr:0.085662 loss:1.618089
[Train] epoch:85	batch id:120	 lr:0.085662 loss:1.785474
[Train] epoch:85	batch id:130	 lr:0.085662 loss:1.594139
[Train] epoch:85	batch id:140	 lr:0.085662 loss:1.480656
[Train] epoch:85	batch id:150	 lr:0.085662 loss:1.438930
[Train] epoch:85	batch id:160	 lr:0.085662 loss:1.579396
[Train] epoch:85	batch id:170	 lr:0.085662 loss:1.627858
[Train] epoch:85	batch id:180	 lr:0.085662 loss:1.760602
[Train] epoch:85	batch id:190	 lr:0.085662 loss:1.651912
[Train] epoch:85	batch id:200	 lr:0.085662 loss:1.495659
[Train] epoch:85	batch id:210	 lr:0.085662 loss:1.854803
[Train] epoch:85	batch id:220	 lr:0.085662 loss:1.606165
[Train] epoch:85	batch id:230	 lr:0.085662 loss:1.680821
[Train] epoch:85	batch id:240	 lr:0.085662 loss:1.610436
[Train] epoch:85	batch id:250	 lr:0.085662 loss:1.491373
[Train] epoch:85	batch id:260	 lr:0.085662 loss:1.651380
[Train] epoch:85	batch id:270	 lr:0.085662 loss:1.656351
[Train] epoch:85	batch id:280	 lr:0.085662 loss:1.550645
[Train] epoch:85	batch id:290	 lr:0.085662 loss:1.615738
[Train] epoch:85	batch id:300	 lr:0.085662 loss:1.624180
[Train] 85, loss: 1.625327, train acc: 0.883754, 
[Test] epoch:85	batch id:0	 loss:1.356689
[Test] epoch:85	batch id:10	 loss:1.494248
[Test] epoch:85	batch id:20	 loss:1.494510
[Test] epoch:85	batch id:30	 loss:1.519813
[Test] epoch:85	batch id:40	 loss:1.450295
[Test] epoch:85	batch id:50	 loss:1.573404
[Test] epoch:85	batch id:60	 loss:1.294250
[Test] epoch:85	batch id:70	 loss:1.511446
[Test] epoch:85	batch id:80	 loss:1.731521
[Test] epoch:85	batch id:90	 loss:1.552486
[Test] epoch:85	batch id:100	 loss:2.052210
[Test] epoch:85	batch id:110	 loss:1.522075
[Test] epoch:85	batch id:120	 loss:1.471834
[Test] epoch:85	batch id:130	 loss:1.422237
[Test] epoch:85	batch id:140	 loss:1.521316
[Test] epoch:85	batch id:150	 loss:1.802125
[Test] 85, loss: 1.545244, test acc: 0.897893,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:86	batch id:0	 lr:0.085348 loss:1.572155
[Train] epoch:86	batch id:10	 lr:0.085348 loss:1.696411
[Train] epoch:86	batch id:20	 lr:0.085348 loss:1.546295
[Train] epoch:86	batch id:30	 lr:0.085348 loss:1.457917
[Train] epoch:86	batch id:40	 lr:0.085348 loss:1.516019
[Train] epoch:86	batch id:50	 lr:0.085348 loss:1.753179
[Train] epoch:86	batch id:60	 lr:0.085348 loss:1.513161
[Train] epoch:86	batch id:70	 lr:0.085348 loss:1.593838
[Train] epoch:86	batch id:80	 lr:0.085348 loss:1.586243
[Train] epoch:86	batch id:90	 lr:0.085348 loss:1.677527
[Train] epoch:86	batch id:100	 lr:0.085348 loss:1.737926
[Train] epoch:86	batch id:110	 lr:0.085348 loss:1.621228
[Train] epoch:86	batch id:120	 lr:0.085348 loss:1.581527
[Train] epoch:86	batch id:130	 lr:0.085348 loss:1.663336
[Train] epoch:86	batch id:140	 lr:0.085348 loss:1.515560
[Train] epoch:86	batch id:150	 lr:0.085348 loss:1.623197
[Train] epoch:86	batch id:160	 lr:0.085348 loss:1.637965
[Train] epoch:86	batch id:170	 lr:0.085348 loss:1.748846
[Train] epoch:86	batch id:180	 lr:0.085348 loss:1.602078
[Train] epoch:86	batch id:190	 lr:0.085348 loss:1.515456
[Train] epoch:86	batch id:200	 lr:0.085348 loss:1.738262
[Train] epoch:86	batch id:210	 lr:0.085348 loss:1.555629
[Train] epoch:86	batch id:220	 lr:0.085348 loss:1.648213
[Train] epoch:86	batch id:230	 lr:0.085348 loss:1.573302
[Train] epoch:86	batch id:240	 lr:0.085348 loss:1.481727
[Train] epoch:86	batch id:250	 lr:0.085348 loss:1.487017
[Train] epoch:86	batch id:260	 lr:0.085348 loss:1.628707
[Train] epoch:86	batch id:270	 lr:0.085348 loss:1.634401
[Train] epoch:86	batch id:280	 lr:0.085348 loss:1.519588
[Train] epoch:86	batch id:290	 lr:0.085348 loss:1.635076
[Train] epoch:86	batch id:300	 lr:0.085348 loss:1.540487
[Train] 86, loss: 1.634685, train acc: 0.879275, 
[Test] epoch:86	batch id:0	 loss:1.390372
[Test] epoch:86	batch id:10	 loss:1.492581
[Test] epoch:86	batch id:20	 loss:1.388973
[Test] epoch:86	batch id:30	 loss:1.529864
[Test] epoch:86	batch id:40	 loss:1.408546
[Test] epoch:86	batch id:50	 loss:1.451181
[Test] epoch:86	batch id:60	 loss:1.318875
[Test] epoch:86	batch id:70	 loss:1.498357
[Test] epoch:86	batch id:80	 loss:1.670691
[Test] epoch:86	batch id:90	 loss:1.562345
[Test] epoch:86	batch id:100	 loss:1.829764
[Test] epoch:86	batch id:110	 loss:1.478777
[Test] epoch:86	batch id:120	 loss:1.451880
[Test] epoch:86	batch id:130	 loss:1.551678
[Test] epoch:86	batch id:140	 loss:1.549921
[Test] epoch:86	batch id:150	 loss:1.841686
[Test] 86, loss: 1.537792, test acc: 0.897488,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:87	batch id:0	 lr:0.085031 loss:1.645391
[Train] epoch:87	batch id:10	 lr:0.085031 loss:1.483112
[Train] epoch:87	batch id:20	 lr:0.085031 loss:1.696633
[Train] epoch:87	batch id:30	 lr:0.085031 loss:1.478825
[Train] epoch:87	batch id:40	 lr:0.085031 loss:1.741064
[Train] epoch:87	batch id:50	 lr:0.085031 loss:1.726004
[Train] epoch:87	batch id:60	 lr:0.085031 loss:1.611874
[Train] epoch:87	batch id:70	 lr:0.085031 loss:1.616005
[Train] epoch:87	batch id:80	 lr:0.085031 loss:1.771174
[Train] epoch:87	batch id:90	 lr:0.085031 loss:1.667181
[Train] epoch:87	batch id:100	 lr:0.085031 loss:1.592659
[Train] epoch:87	batch id:110	 lr:0.085031 loss:1.539133
[Train] epoch:87	batch id:120	 lr:0.085031 loss:1.491021
[Train] epoch:87	batch id:130	 lr:0.085031 loss:1.569141
[Train] epoch:87	batch id:140	 lr:0.085031 loss:1.496990
[Train] epoch:87	batch id:150	 lr:0.085031 loss:1.682816
[Train] epoch:87	batch id:160	 lr:0.085031 loss:1.648753
[Train] epoch:87	batch id:170	 lr:0.085031 loss:1.605061
[Train] epoch:87	batch id:180	 lr:0.085031 loss:1.790793
[Train] epoch:87	batch id:190	 lr:0.085031 loss:1.619048
[Train] epoch:87	batch id:200	 lr:0.085031 loss:1.722283
[Train] epoch:87	batch id:210	 lr:0.085031 loss:1.643248
[Train] epoch:87	batch id:220	 lr:0.085031 loss:1.552457
[Train] epoch:87	batch id:230	 lr:0.085031 loss:1.718238
[Train] epoch:87	batch id:240	 lr:0.085031 loss:1.625249
[Train] epoch:87	batch id:250	 lr:0.085031 loss:1.644595
[Train] epoch:87	batch id:260	 lr:0.085031 loss:1.613142
[Train] epoch:87	batch id:270	 lr:0.085031 loss:1.629697
[Train] epoch:87	batch id:280	 lr:0.085031 loss:1.594211
[Train] epoch:87	batch id:290	 lr:0.085031 loss:1.616648
[Train] epoch:87	batch id:300	 lr:0.085031 loss:1.486739
[Train] 87, loss: 1.625688, train acc: 0.878868, 
[Test] epoch:87	batch id:0	 loss:1.353905
[Test] epoch:87	batch id:10	 loss:1.396727
[Test] epoch:87	batch id:20	 loss:1.458101
[Test] epoch:87	batch id:30	 loss:1.393455
[Test] epoch:87	batch id:40	 loss:1.424438
[Test] epoch:87	batch id:50	 loss:1.412003
[Test] epoch:87	batch id:60	 loss:1.335721
[Test] epoch:87	batch id:70	 loss:1.525646
[Test] epoch:87	batch id:80	 loss:1.598970
[Test] epoch:87	batch id:90	 loss:1.471351
[Test] epoch:87	batch id:100	 loss:2.031057
[Test] epoch:87	batch id:110	 loss:1.431963
[Test] epoch:87	batch id:120	 loss:1.406364
[Test] epoch:87	batch id:130	 loss:1.498197
[Test] epoch:87	batch id:140	 loss:1.491710
[Test] epoch:87	batch id:150	 loss:1.728519
[Test] 87, loss: 1.521015, test acc: 0.911669,
Max Acc:0.911669
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:88	batch id:0	 lr:0.084712 loss:1.499676
[Train] epoch:88	batch id:10	 lr:0.084712 loss:1.794799
[Train] epoch:88	batch id:20	 lr:0.084712 loss:1.743553
[Train] epoch:88	batch id:30	 lr:0.084712 loss:1.626501
[Train] epoch:88	batch id:40	 lr:0.084712 loss:1.627348
[Train] epoch:88	batch id:50	 lr:0.084712 loss:1.610819
[Train] epoch:88	batch id:60	 lr:0.084712 loss:1.504044
[Train] epoch:88	batch id:70	 lr:0.084712 loss:1.729330
[Train] epoch:88	batch id:80	 lr:0.084712 loss:1.721295
[Train] epoch:88	batch id:90	 lr:0.084712 loss:1.456715
[Train] epoch:88	batch id:100	 lr:0.084712 loss:1.787661
[Train] epoch:88	batch id:110	 lr:0.084712 loss:1.607693
[Train] epoch:88	batch id:120	 lr:0.084712 loss:1.652633
[Train] epoch:88	batch id:130	 lr:0.084712 loss:1.521728
[Train] epoch:88	batch id:140	 lr:0.084712 loss:1.694294
[Train] epoch:88	batch id:150	 lr:0.084712 loss:1.841946
[Train] epoch:88	batch id:160	 lr:0.084712 loss:1.724576
[Train] epoch:88	batch id:170	 lr:0.084712 loss:1.555633
[Train] epoch:88	batch id:180	 lr:0.084712 loss:1.606428
[Train] epoch:88	batch id:190	 lr:0.084712 loss:1.547197
[Train] epoch:88	batch id:200	 lr:0.084712 loss:1.648115
[Train] epoch:88	batch id:210	 lr:0.084712 loss:1.541794
[Train] epoch:88	batch id:220	 lr:0.084712 loss:1.541680
[Train] epoch:88	batch id:230	 lr:0.084712 loss:1.687572
[Train] epoch:88	batch id:240	 lr:0.084712 loss:1.589058
[Train] epoch:88	batch id:250	 lr:0.084712 loss:1.648472
[Train] epoch:88	batch id:260	 lr:0.084712 loss:1.701366
[Train] epoch:88	batch id:270	 lr:0.084712 loss:1.641408
[Train] epoch:88	batch id:280	 lr:0.084712 loss:1.523328
[Train] epoch:88	batch id:290	 lr:0.084712 loss:1.524286
[Train] epoch:88	batch id:300	 lr:0.084712 loss:1.665398
[Train] 88, loss: 1.630823, train acc: 0.877138, 
[Test] epoch:88	batch id:0	 loss:1.401425
[Test] epoch:88	batch id:10	 loss:1.455257
[Test] epoch:88	batch id:20	 loss:1.413223
[Test] epoch:88	batch id:30	 loss:1.413617
[Test] epoch:88	batch id:40	 loss:1.469735
[Test] epoch:88	batch id:50	 loss:1.530376
[Test] epoch:88	batch id:60	 loss:1.303428
[Test] epoch:88	batch id:70	 loss:1.477796
[Test] epoch:88	batch id:80	 loss:1.651769
[Test] epoch:88	batch id:90	 loss:1.719444
[Test] epoch:88	batch id:100	 loss:1.840156
[Test] epoch:88	batch id:110	 loss:1.486199
[Test] epoch:88	batch id:120	 loss:1.492795
[Test] epoch:88	batch id:130	 loss:1.573336
[Test] epoch:88	batch id:140	 loss:1.402126
[Test] epoch:88	batch id:150	 loss:1.973634
[Test] 88, loss: 1.542501, test acc: 0.896272,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:89	batch id:0	 lr:0.084389 loss:1.675174
[Train] epoch:89	batch id:10	 lr:0.084389 loss:1.445311
[Train] epoch:89	batch id:20	 lr:0.084389 loss:1.550797
[Train] epoch:89	batch id:30	 lr:0.084389 loss:1.603447
[Train] epoch:89	batch id:40	 lr:0.084389 loss:1.659018
[Train] epoch:89	batch id:50	 lr:0.084389 loss:1.548999
[Train] epoch:89	batch id:60	 lr:0.084389 loss:1.663232
[Train] epoch:89	batch id:70	 lr:0.084389 loss:1.459912
[Train] epoch:89	batch id:80	 lr:0.084389 loss:1.537606
[Train] epoch:89	batch id:90	 lr:0.084389 loss:1.461861
[Train] epoch:89	batch id:100	 lr:0.084389 loss:1.797127
[Train] epoch:89	batch id:110	 lr:0.084389 loss:1.693015
[Train] epoch:89	batch id:120	 lr:0.084389 loss:1.505020
[Train] epoch:89	batch id:130	 lr:0.084389 loss:1.615320
[Train] epoch:89	batch id:140	 lr:0.084389 loss:1.729029
[Train] epoch:89	batch id:150	 lr:0.084389 loss:1.506689
[Train] epoch:89	batch id:160	 lr:0.084389 loss:1.578452
[Train] epoch:89	batch id:170	 lr:0.084389 loss:1.699565
[Train] epoch:89	batch id:180	 lr:0.084389 loss:1.713113
[Train] epoch:89	batch id:190	 lr:0.084389 loss:1.606539
[Train] epoch:89	batch id:200	 lr:0.084389 loss:1.688695
[Train] epoch:89	batch id:210	 lr:0.084389 loss:1.605538
[Train] epoch:89	batch id:220	 lr:0.084389 loss:1.710748
[Train] epoch:89	batch id:230	 lr:0.084389 loss:1.486672
[Train] epoch:89	batch id:240	 lr:0.084389 loss:1.809719
[Train] epoch:89	batch id:250	 lr:0.084389 loss:1.917322
[Train] epoch:89	batch id:260	 lr:0.084389 loss:1.855040
[Train] epoch:89	batch id:270	 lr:0.084389 loss:1.717921
[Train] epoch:89	batch id:280	 lr:0.084389 loss:1.647111
[Train] epoch:89	batch id:290	 lr:0.084389 loss:1.500449
[Train] epoch:89	batch id:300	 lr:0.084389 loss:1.541586
[Train] 89, loss: 1.621891, train acc: 0.883042, 
[Test] epoch:89	batch id:0	 loss:1.365547
[Test] epoch:89	batch id:10	 loss:1.517351
[Test] epoch:89	batch id:20	 loss:1.446859
[Test] epoch:89	batch id:30	 loss:1.499644
[Test] epoch:89	batch id:40	 loss:1.446327
[Test] epoch:89	batch id:50	 loss:1.497728
[Test] epoch:89	batch id:60	 loss:1.313210
[Test] epoch:89	batch id:70	 loss:1.484663
[Test] epoch:89	batch id:80	 loss:1.586192
[Test] epoch:89	batch id:90	 loss:1.635843
[Test] epoch:89	batch id:100	 loss:1.999584
[Test] epoch:89	batch id:110	 loss:1.429141
[Test] epoch:89	batch id:120	 loss:1.500321
[Test] epoch:89	batch id:130	 loss:1.563391
[Test] epoch:89	batch id:140	 loss:1.446745
[Test] epoch:89	batch id:150	 loss:1.878201
[Test] 89, loss: 1.546362, test acc: 0.903160,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:90	batch id:0	 lr:0.084064 loss:1.604232
[Train] epoch:90	batch id:10	 lr:0.084064 loss:1.667549
[Train] epoch:90	batch id:20	 lr:0.084064 loss:1.657610
[Train] epoch:90	batch id:30	 lr:0.084064 loss:1.639718
[Train] epoch:90	batch id:40	 lr:0.084064 loss:1.475050
[Train] epoch:90	batch id:50	 lr:0.084064 loss:1.658392
[Train] epoch:90	batch id:60	 lr:0.084064 loss:1.762273
[Train] epoch:90	batch id:70	 lr:0.084064 loss:1.680562
[Train] epoch:90	batch id:80	 lr:0.084064 loss:1.609422
[Train] epoch:90	batch id:90	 lr:0.084064 loss:1.467961
[Train] epoch:90	batch id:100	 lr:0.084064 loss:1.568891
[Train] epoch:90	batch id:110	 lr:0.084064 loss:1.428164
[Train] epoch:90	batch id:120	 lr:0.084064 loss:1.803881
[Train] epoch:90	batch id:130	 lr:0.084064 loss:1.560417
[Train] epoch:90	batch id:140	 lr:0.084064 loss:1.650843
[Train] epoch:90	batch id:150	 lr:0.084064 loss:1.740441
[Train] epoch:90	batch id:160	 lr:0.084064 loss:1.656241
[Train] epoch:90	batch id:170	 lr:0.084064 loss:1.726311
[Train] epoch:90	batch id:180	 lr:0.084064 loss:1.761515
[Train] epoch:90	batch id:190	 lr:0.084064 loss:1.551736
[Train] epoch:90	batch id:200	 lr:0.084064 loss:1.514870
[Train] epoch:90	batch id:210	 lr:0.084064 loss:1.844931
[Train] epoch:90	batch id:220	 lr:0.084064 loss:1.461004
[Train] epoch:90	batch id:230	 lr:0.084064 loss:1.510668
[Train] epoch:90	batch id:240	 lr:0.084064 loss:1.699909
[Train] epoch:90	batch id:250	 lr:0.084064 loss:1.608313
[Train] epoch:90	batch id:260	 lr:0.084064 loss:1.550498
[Train] epoch:90	batch id:270	 lr:0.084064 loss:1.684282
[Train] epoch:90	batch id:280	 lr:0.084064 loss:1.756571
[Train] epoch:90	batch id:290	 lr:0.084064 loss:1.486217
[Train] epoch:90	batch id:300	 lr:0.084064 loss:1.517134
[Train] 90, loss: 1.619958, train acc: 0.885281, 
[Test] epoch:90	batch id:0	 loss:1.406192
[Test] epoch:90	batch id:10	 loss:1.533431
[Test] epoch:90	batch id:20	 loss:1.391737
[Test] epoch:90	batch id:30	 loss:1.491627
[Test] epoch:90	batch id:40	 loss:1.524261
[Test] epoch:90	batch id:50	 loss:1.387074
[Test] epoch:90	batch id:60	 loss:1.331160
[Test] epoch:90	batch id:70	 loss:1.449600
[Test] epoch:90	batch id:80	 loss:1.609886
[Test] epoch:90	batch id:90	 loss:1.539227
[Test] epoch:90	batch id:100	 loss:1.964535
[Test] epoch:90	batch id:110	 loss:1.388270
[Test] epoch:90	batch id:120	 loss:1.366602
[Test] epoch:90	batch id:130	 loss:1.481881
[Test] epoch:90	batch id:140	 loss:1.490974
[Test] epoch:90	batch id:150	 loss:1.896707
[Test] 90, loss: 1.547835, test acc: 0.899514,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:91	batch id:0	 lr:0.083736 loss:1.777006
[Train] epoch:91	batch id:10	 lr:0.083736 loss:1.560063
[Train] epoch:91	batch id:20	 lr:0.083736 loss:1.613614
[Train] epoch:91	batch id:30	 lr:0.083736 loss:1.443199
[Train] epoch:91	batch id:40	 lr:0.083736 loss:1.663380
[Train] epoch:91	batch id:50	 lr:0.083736 loss:1.576585
[Train] epoch:91	batch id:60	 lr:0.083736 loss:1.587247
[Train] epoch:91	batch id:70	 lr:0.083736 loss:1.458807
[Train] epoch:91	batch id:80	 lr:0.083736 loss:1.623363
[Train] epoch:91	batch id:90	 lr:0.083736 loss:1.668337
[Train] epoch:91	batch id:100	 lr:0.083736 loss:1.638474
[Train] epoch:91	batch id:110	 lr:0.083736 loss:1.712312
[Train] epoch:91	batch id:120	 lr:0.083736 loss:1.640695
[Train] epoch:91	batch id:130	 lr:0.083736 loss:1.566995
[Train] epoch:91	batch id:140	 lr:0.083736 loss:1.717175
[Train] epoch:91	batch id:150	 lr:0.083736 loss:1.575492
[Train] epoch:91	batch id:160	 lr:0.083736 loss:1.700040
[Train] epoch:91	batch id:170	 lr:0.083736 loss:1.545364
[Train] epoch:91	batch id:180	 lr:0.083736 loss:1.633849
[Train] epoch:91	batch id:190	 lr:0.083736 loss:1.536674
[Train] epoch:91	batch id:200	 lr:0.083736 loss:1.728158
[Train] epoch:91	batch id:210	 lr:0.083736 loss:1.493252
[Train] epoch:91	batch id:220	 lr:0.083736 loss:1.515393
[Train] epoch:91	batch id:230	 lr:0.083736 loss:1.516410
[Train] epoch:91	batch id:240	 lr:0.083736 loss:1.719491
[Train] epoch:91	batch id:250	 lr:0.083736 loss:1.598801
[Train] epoch:91	batch id:260	 lr:0.083736 loss:1.463551
[Train] epoch:91	batch id:270	 lr:0.083736 loss:1.620809
[Train] epoch:91	batch id:280	 lr:0.083736 loss:1.526130
[Train] epoch:91	batch id:290	 lr:0.083736 loss:1.561836
[Train] epoch:91	batch id:300	 lr:0.083736 loss:1.579891
[Train] 91, loss: 1.619788, train acc: 0.881107, 
[Test] epoch:91	batch id:0	 loss:1.376557
[Test] epoch:91	batch id:10	 loss:1.536931
[Test] epoch:91	batch id:20	 loss:1.403307
[Test] epoch:91	batch id:30	 loss:1.425234
[Test] epoch:91	batch id:40	 loss:1.400713
[Test] epoch:91	batch id:50	 loss:1.532529
[Test] epoch:91	batch id:60	 loss:1.337860
[Test] epoch:91	batch id:70	 loss:1.453790
[Test] epoch:91	batch id:80	 loss:1.786258
[Test] epoch:91	batch id:90	 loss:1.598562
[Test] epoch:91	batch id:100	 loss:1.955752
[Test] epoch:91	batch id:110	 loss:1.527569
[Test] epoch:91	batch id:120	 loss:1.494544
[Test] epoch:91	batch id:130	 loss:1.600986
[Test] epoch:91	batch id:140	 loss:1.460524
[Test] epoch:91	batch id:150	 loss:1.720394
[Test] 91, loss: 1.555679, test acc: 0.901135,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:92	batch id:0	 lr:0.083405 loss:1.526916
[Train] epoch:92	batch id:10	 lr:0.083405 loss:1.624774
[Train] epoch:92	batch id:20	 lr:0.083405 loss:1.514982
[Train] epoch:92	batch id:30	 lr:0.083405 loss:1.613442
[Train] epoch:92	batch id:40	 lr:0.083405 loss:1.701497
[Train] epoch:92	batch id:50	 lr:0.083405 loss:1.547457
[Train] epoch:92	batch id:60	 lr:0.083405 loss:1.567315
[Train] epoch:92	batch id:70	 lr:0.083405 loss:1.589543
[Train] epoch:92	batch id:80	 lr:0.083405 loss:1.490004
[Train] epoch:92	batch id:90	 lr:0.083405 loss:1.544841
[Train] epoch:92	batch id:100	 lr:0.083405 loss:1.637413
[Train] epoch:92	batch id:110	 lr:0.083405 loss:1.602171
[Train] epoch:92	batch id:120	 lr:0.083405 loss:1.559352
[Train] epoch:92	batch id:130	 lr:0.083405 loss:1.617015
[Train] epoch:92	batch id:140	 lr:0.083405 loss:1.461293
[Train] epoch:92	batch id:150	 lr:0.083405 loss:1.543022
[Train] epoch:92	batch id:160	 lr:0.083405 loss:1.632972
[Train] epoch:92	batch id:170	 lr:0.083405 loss:1.689323
[Train] epoch:92	batch id:180	 lr:0.083405 loss:1.750960
[Train] epoch:92	batch id:190	 lr:0.083405 loss:1.564894
[Train] epoch:92	batch id:200	 lr:0.083405 loss:1.536038
[Train] epoch:92	batch id:210	 lr:0.083405 loss:1.708783
[Train] epoch:92	batch id:220	 lr:0.083405 loss:1.760685
[Train] epoch:92	batch id:230	 lr:0.083405 loss:1.512418
[Train] epoch:92	batch id:240	 lr:0.083405 loss:1.586332
[Train] epoch:92	batch id:250	 lr:0.083405 loss:1.601905
[Train] epoch:92	batch id:260	 lr:0.083405 loss:1.518445
[Train] epoch:92	batch id:270	 lr:0.083405 loss:1.659912
[Train] epoch:92	batch id:280	 lr:0.083405 loss:1.679336
[Train] epoch:92	batch id:290	 lr:0.083405 loss:1.498996
[Train] epoch:92	batch id:300	 lr:0.083405 loss:1.543500
[Train] 92, loss: 1.618952, train acc: 0.883347, 
[Test] epoch:92	batch id:0	 loss:1.385978
[Test] epoch:92	batch id:10	 loss:1.450346
[Test] epoch:92	batch id:20	 loss:1.473026
[Test] epoch:92	batch id:30	 loss:1.438795
[Test] epoch:92	batch id:40	 loss:1.417845
[Test] epoch:92	batch id:50	 loss:1.445396
[Test] epoch:92	batch id:60	 loss:1.330943
[Test] epoch:92	batch id:70	 loss:1.495629
[Test] epoch:92	batch id:80	 loss:1.749432
[Test] epoch:92	batch id:90	 loss:1.491613
[Test] epoch:92	batch id:100	 loss:1.920437
[Test] epoch:92	batch id:110	 loss:1.433581
[Test] epoch:92	batch id:120	 loss:1.524739
[Test] epoch:92	batch id:130	 loss:1.501976
[Test] epoch:92	batch id:140	 loss:1.484538
[Test] epoch:92	batch id:150	 loss:1.733721
[Test] 92, loss: 1.537273, test acc: 0.896272,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:93	batch id:0	 lr:0.083072 loss:1.662967
[Train] epoch:93	batch id:10	 lr:0.083072 loss:1.528516
[Train] epoch:93	batch id:20	 lr:0.083072 loss:1.841585
[Train] epoch:93	batch id:30	 lr:0.083072 loss:1.683389
[Train] epoch:93	batch id:40	 lr:0.083072 loss:1.679528
[Train] epoch:93	batch id:50	 lr:0.083072 loss:1.948434
[Train] epoch:93	batch id:60	 lr:0.083072 loss:1.597697
[Train] epoch:93	batch id:70	 lr:0.083072 loss:1.548105
[Train] epoch:93	batch id:80	 lr:0.083072 loss:1.904486
[Train] epoch:93	batch id:90	 lr:0.083072 loss:1.457193
[Train] epoch:93	batch id:100	 lr:0.083072 loss:1.771168
[Train] epoch:93	batch id:110	 lr:0.083072 loss:1.560084
[Train] epoch:93	batch id:120	 lr:0.083072 loss:1.615338
[Train] epoch:93	batch id:130	 lr:0.083072 loss:1.573600
[Train] epoch:93	batch id:140	 lr:0.083072 loss:1.719379
[Train] epoch:93	batch id:150	 lr:0.083072 loss:1.742655
[Train] epoch:93	batch id:160	 lr:0.083072 loss:1.535050
[Train] epoch:93	batch id:170	 lr:0.083072 loss:1.646447
[Train] epoch:93	batch id:180	 lr:0.083072 loss:1.600978
[Train] epoch:93	batch id:190	 lr:0.083072 loss:1.728199
[Train] epoch:93	batch id:200	 lr:0.083072 loss:1.591525
[Train] epoch:93	batch id:210	 lr:0.083072 loss:1.623715
[Train] epoch:93	batch id:220	 lr:0.083072 loss:1.673418
[Train] epoch:93	batch id:230	 lr:0.083072 loss:1.578887
[Train] epoch:93	batch id:240	 lr:0.083072 loss:1.580352
[Train] epoch:93	batch id:250	 lr:0.083072 loss:1.749865
[Train] epoch:93	batch id:260	 lr:0.083072 loss:1.728880
[Train] epoch:93	batch id:270	 lr:0.083072 loss:1.608314
[Train] epoch:93	batch id:280	 lr:0.083072 loss:1.714241
[Train] epoch:93	batch id:290	 lr:0.083072 loss:1.709037
[Train] epoch:93	batch id:300	 lr:0.083072 loss:1.979210
[Train] 93, loss: 1.615020, train acc: 0.887419, 
[Test] epoch:93	batch id:0	 loss:1.339756
[Test] epoch:93	batch id:10	 loss:1.411699
[Test] epoch:93	batch id:20	 loss:1.441113
[Test] epoch:93	batch id:30	 loss:1.545540
[Test] epoch:93	batch id:40	 loss:1.445145
[Test] epoch:93	batch id:50	 loss:1.519914
[Test] epoch:93	batch id:60	 loss:1.310736
[Test] epoch:93	batch id:70	 loss:1.522755
[Test] epoch:93	batch id:80	 loss:1.650649
[Test] epoch:93	batch id:90	 loss:1.512574
[Test] epoch:93	batch id:100	 loss:1.925615
[Test] epoch:93	batch id:110	 loss:1.402247
[Test] epoch:93	batch id:120	 loss:1.391934
[Test] epoch:93	batch id:130	 loss:1.496971
[Test] epoch:93	batch id:140	 loss:1.386877
[Test] epoch:93	batch id:150	 loss:1.813065
[Test] 93, loss: 1.531998, test acc: 0.895057,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:94	batch id:0	 lr:0.082736 loss:1.468434
[Train] epoch:94	batch id:10	 lr:0.082736 loss:1.756130
[Train] epoch:94	batch id:20	 lr:0.082736 loss:1.605924
[Train] epoch:94	batch id:30	 lr:0.082736 loss:1.838539
[Train] epoch:94	batch id:40	 lr:0.082736 loss:1.531033
[Train] epoch:94	batch id:50	 lr:0.082736 loss:1.800411
[Train] epoch:94	batch id:60	 lr:0.082736 loss:1.513381
[Train] epoch:94	batch id:70	 lr:0.082736 loss:1.673515
[Train] epoch:94	batch id:80	 lr:0.082736 loss:1.507434
[Train] epoch:94	batch id:90	 lr:0.082736 loss:1.762101
[Train] epoch:94	batch id:100	 lr:0.082736 loss:1.683350
[Train] epoch:94	batch id:110	 lr:0.082736 loss:1.536661
[Train] epoch:94	batch id:120	 lr:0.082736 loss:1.468246
[Train] epoch:94	batch id:130	 lr:0.082736 loss:1.782423
[Train] epoch:94	batch id:140	 lr:0.082736 loss:1.704767
[Train] epoch:94	batch id:150	 lr:0.082736 loss:1.687583
[Train] epoch:94	batch id:160	 lr:0.082736 loss:1.747119
[Train] epoch:94	batch id:170	 lr:0.082736 loss:1.483319
[Train] epoch:94	batch id:180	 lr:0.082736 loss:1.729682
[Train] epoch:94	batch id:190	 lr:0.082736 loss:1.613007
[Train] epoch:94	batch id:200	 lr:0.082736 loss:1.705985
[Train] epoch:94	batch id:210	 lr:0.082736 loss:1.648666
[Train] epoch:94	batch id:220	 lr:0.082736 loss:1.555318
[Train] epoch:94	batch id:230	 lr:0.082736 loss:1.584460
[Train] epoch:94	batch id:240	 lr:0.082736 loss:1.630730
[Train] epoch:94	batch id:250	 lr:0.082736 loss:1.554639
[Train] epoch:94	batch id:260	 lr:0.082736 loss:1.654760
[Train] epoch:94	batch id:270	 lr:0.082736 loss:1.767203
[Train] epoch:94	batch id:280	 lr:0.082736 loss:1.572055
[Train] epoch:94	batch id:290	 lr:0.082736 loss:1.504838
[Train] epoch:94	batch id:300	 lr:0.082736 loss:1.507472
[Train] 94, loss: 1.617957, train acc: 0.885383, 
[Test] epoch:94	batch id:0	 loss:1.411568
[Test] epoch:94	batch id:10	 loss:1.520767
[Test] epoch:94	batch id:20	 loss:1.431579
[Test] epoch:94	batch id:30	 loss:1.488339
[Test] epoch:94	batch id:40	 loss:1.376960
[Test] epoch:94	batch id:50	 loss:1.465859
[Test] epoch:94	batch id:60	 loss:1.327686
[Test] epoch:94	batch id:70	 loss:1.578969
[Test] epoch:94	batch id:80	 loss:1.688929
[Test] epoch:94	batch id:90	 loss:1.459960
[Test] epoch:94	batch id:100	 loss:1.753861
[Test] epoch:94	batch id:110	 loss:1.537907
[Test] epoch:94	batch id:120	 loss:1.454295
[Test] epoch:94	batch id:130	 loss:1.713765
[Test] epoch:94	batch id:140	 loss:1.420116
[Test] epoch:94	batch id:150	 loss:1.891512
[Test] 94, loss: 1.558542, test acc: 0.896272,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:95	batch id:0	 lr:0.082398 loss:1.697914
[Train] epoch:95	batch id:10	 lr:0.082398 loss:1.851947
[Train] epoch:95	batch id:20	 lr:0.082398 loss:1.552772
[Train] epoch:95	batch id:30	 lr:0.082398 loss:1.613922
[Train] epoch:95	batch id:40	 lr:0.082398 loss:1.610820
[Train] epoch:95	batch id:50	 lr:0.082398 loss:1.628721
[Train] epoch:95	batch id:60	 lr:0.082398 loss:1.601347
[Train] epoch:95	batch id:70	 lr:0.082398 loss:1.808102
[Train] epoch:95	batch id:80	 lr:0.082398 loss:1.537012
[Train] epoch:95	batch id:90	 lr:0.082398 loss:1.707413
[Train] epoch:95	batch id:100	 lr:0.082398 loss:1.608981
[Train] epoch:95	batch id:110	 lr:0.082398 loss:1.547416
[Train] epoch:95	batch id:120	 lr:0.082398 loss:1.532915
[Train] epoch:95	batch id:130	 lr:0.082398 loss:1.564347
[Train] epoch:95	batch id:140	 lr:0.082398 loss:1.512755
[Train] epoch:95	batch id:150	 lr:0.082398 loss:1.661462
[Train] epoch:95	batch id:160	 lr:0.082398 loss:1.566455
[Train] epoch:95	batch id:170	 lr:0.082398 loss:1.631656
[Train] epoch:95	batch id:180	 lr:0.082398 loss:1.500345
[Train] epoch:95	batch id:190	 lr:0.082398 loss:1.592931
[Train] epoch:95	batch id:200	 lr:0.082398 loss:1.728590
[Train] epoch:95	batch id:210	 lr:0.082398 loss:1.581847
[Train] epoch:95	batch id:220	 lr:0.082398 loss:1.575136
[Train] epoch:95	batch id:230	 lr:0.082398 loss:1.731721
[Train] epoch:95	batch id:240	 lr:0.082398 loss:1.638591
[Train] epoch:95	batch id:250	 lr:0.082398 loss:1.872001
[Train] epoch:95	batch id:260	 lr:0.082398 loss:1.578121
[Train] epoch:95	batch id:270	 lr:0.082398 loss:1.784976
[Train] epoch:95	batch id:280	 lr:0.082398 loss:1.537009
[Train] epoch:95	batch id:290	 lr:0.082398 loss:1.580290
[Train] epoch:95	batch id:300	 lr:0.082398 loss:1.558133
[Train] 95, loss: 1.617516, train acc: 0.885993, 
[Test] epoch:95	batch id:0	 loss:1.329539
[Test] epoch:95	batch id:10	 loss:1.437212
[Test] epoch:95	batch id:20	 loss:1.486520
[Test] epoch:95	batch id:30	 loss:1.478320
[Test] epoch:95	batch id:40	 loss:1.421902
[Test] epoch:95	batch id:50	 loss:1.486982
[Test] epoch:95	batch id:60	 loss:1.317645
[Test] epoch:95	batch id:70	 loss:1.446106
[Test] epoch:95	batch id:80	 loss:1.676636
[Test] epoch:95	batch id:90	 loss:1.445997
[Test] epoch:95	batch id:100	 loss:2.078668
[Test] epoch:95	batch id:110	 loss:1.482431
[Test] epoch:95	batch id:120	 loss:1.398675
[Test] epoch:95	batch id:130	 loss:1.451306
[Test] epoch:95	batch id:140	 loss:1.392722
[Test] epoch:95	batch id:150	 loss:1.749967
[Test] 95, loss: 1.528674, test acc: 0.902755,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:96	batch id:0	 lr:0.082056 loss:1.558281
[Train] epoch:96	batch id:10	 lr:0.082056 loss:1.507293
[Train] epoch:96	batch id:20	 lr:0.082056 loss:1.622615
[Train] epoch:96	batch id:30	 lr:0.082056 loss:1.594803
[Train] epoch:96	batch id:40	 lr:0.082056 loss:1.546809
[Train] epoch:96	batch id:50	 lr:0.082056 loss:1.416394
[Train] epoch:96	batch id:60	 lr:0.082056 loss:1.563192
[Train] epoch:96	batch id:70	 lr:0.082056 loss:1.784232
[Train] epoch:96	batch id:80	 lr:0.082056 loss:1.552233
[Train] epoch:96	batch id:90	 lr:0.082056 loss:1.617818
[Train] epoch:96	batch id:100	 lr:0.082056 loss:1.543657
[Train] epoch:96	batch id:110	 lr:0.082056 loss:1.698341
[Train] epoch:96	batch id:120	 lr:0.082056 loss:1.773769
[Train] epoch:96	batch id:130	 lr:0.082056 loss:1.538929
[Train] epoch:96	batch id:140	 lr:0.082056 loss:1.515775
[Train] epoch:96	batch id:150	 lr:0.082056 loss:1.460791
[Train] epoch:96	batch id:160	 lr:0.082056 loss:1.615585
[Train] epoch:96	batch id:170	 lr:0.082056 loss:1.500270
[Train] epoch:96	batch id:180	 lr:0.082056 loss:1.635553
[Train] epoch:96	batch id:190	 lr:0.082056 loss:1.558456
[Train] epoch:96	batch id:200	 lr:0.082056 loss:1.577029
[Train] epoch:96	batch id:210	 lr:0.082056 loss:1.459828
[Train] epoch:96	batch id:220	 lr:0.082056 loss:1.813916
[Train] epoch:96	batch id:230	 lr:0.082056 loss:1.707405
[Train] epoch:96	batch id:240	 lr:0.082056 loss:1.570917
[Train] epoch:96	batch id:250	 lr:0.082056 loss:1.552452
[Train] epoch:96	batch id:260	 lr:0.082056 loss:1.540864
[Train] epoch:96	batch id:270	 lr:0.082056 loss:1.586579
[Train] epoch:96	batch id:280	 lr:0.082056 loss:1.739571
[Train] epoch:96	batch id:290	 lr:0.082056 loss:1.763708
[Train] epoch:96	batch id:300	 lr:0.082056 loss:1.677503
[Train] 96, loss: 1.606553, train acc: 0.889963, 
[Test] epoch:96	batch id:0	 loss:1.385974
[Test] epoch:96	batch id:10	 loss:1.464466
[Test] epoch:96	batch id:20	 loss:1.401025
[Test] epoch:96	batch id:30	 loss:1.454745
[Test] epoch:96	batch id:40	 loss:1.383338
[Test] epoch:96	batch id:50	 loss:1.620765
[Test] epoch:96	batch id:60	 loss:1.324224
[Test] epoch:96	batch id:70	 loss:1.507572
[Test] epoch:96	batch id:80	 loss:1.629915
[Test] epoch:96	batch id:90	 loss:1.534186
[Test] epoch:96	batch id:100	 loss:1.980315
[Test] epoch:96	batch id:110	 loss:1.413945
[Test] epoch:96	batch id:120	 loss:1.388809
[Test] epoch:96	batch id:130	 loss:1.472955
[Test] epoch:96	batch id:140	 loss:1.398049
[Test] epoch:96	batch id:150	 loss:1.844340
[Test] 96, loss: 1.537868, test acc: 0.895867,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:97	batch id:0	 lr:0.081713 loss:1.514473
[Train] epoch:97	batch id:10	 lr:0.081713 loss:1.610620
[Train] epoch:97	batch id:20	 lr:0.081713 loss:1.540355
[Train] epoch:97	batch id:30	 lr:0.081713 loss:1.645526
[Train] epoch:97	batch id:40	 lr:0.081713 loss:1.558518
[Train] epoch:97	batch id:50	 lr:0.081713 loss:1.492508
[Train] epoch:97	batch id:60	 lr:0.081713 loss:1.585270
[Train] epoch:97	batch id:70	 lr:0.081713 loss:1.732169
[Train] epoch:97	batch id:80	 lr:0.081713 loss:1.684106
[Train] epoch:97	batch id:90	 lr:0.081713 loss:1.404821
[Train] epoch:97	batch id:100	 lr:0.081713 loss:1.571628
[Train] epoch:97	batch id:110	 lr:0.081713 loss:1.646065
[Train] epoch:97	batch id:120	 lr:0.081713 loss:1.786838
[Train] epoch:97	batch id:130	 lr:0.081713 loss:1.634484
[Train] epoch:97	batch id:140	 lr:0.081713 loss:1.487773
[Train] epoch:97	batch id:150	 lr:0.081713 loss:1.633734
[Train] epoch:97	batch id:160	 lr:0.081713 loss:1.617741
[Train] epoch:97	batch id:170	 lr:0.081713 loss:1.513579
[Train] epoch:97	batch id:180	 lr:0.081713 loss:1.994809
[Train] epoch:97	batch id:190	 lr:0.081713 loss:1.560841
[Train] epoch:97	batch id:200	 lr:0.081713 loss:1.773952
[Train] epoch:97	batch id:210	 lr:0.081713 loss:1.559458
[Train] epoch:97	batch id:220	 lr:0.081713 loss:1.585864
[Train] epoch:97	batch id:230	 lr:0.081713 loss:2.077704
[Train] epoch:97	batch id:240	 lr:0.081713 loss:1.816610
[Train] epoch:97	batch id:250	 lr:0.081713 loss:1.671923
[Train] epoch:97	batch id:260	 lr:0.081713 loss:1.783183
[Train] epoch:97	batch id:270	 lr:0.081713 loss:1.520456
[Train] epoch:97	batch id:280	 lr:0.081713 loss:1.727824
[Train] epoch:97	batch id:290	 lr:0.081713 loss:1.536598
[Train] epoch:97	batch id:300	 lr:0.081713 loss:1.545158
[Train] 97, loss: 1.617058, train acc: 0.882736, 
[Test] epoch:97	batch id:0	 loss:1.390364
[Test] epoch:97	batch id:10	 loss:1.452290
[Test] epoch:97	batch id:20	 loss:1.412280
[Test] epoch:97	batch id:30	 loss:1.610195
[Test] epoch:97	batch id:40	 loss:1.492790
[Test] epoch:97	batch id:50	 loss:1.492234
[Test] epoch:97	batch id:60	 loss:1.351912
[Test] epoch:97	batch id:70	 loss:1.549681
[Test] epoch:97	batch id:80	 loss:1.680933
[Test] epoch:97	batch id:90	 loss:1.465708
[Test] epoch:97	batch id:100	 loss:1.918300
[Test] epoch:97	batch id:110	 loss:1.457648
[Test] epoch:97	batch id:120	 loss:1.528764
[Test] epoch:97	batch id:130	 loss:1.480570
[Test] epoch:97	batch id:140	 loss:1.378546
[Test] epoch:97	batch id:150	 loss:1.810872
[Test] 97, loss: 1.540786, test acc: 0.910454,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:98	batch id:0	 lr:0.081367 loss:1.677825
[Train] epoch:98	batch id:10	 lr:0.081367 loss:1.465620
[Train] epoch:98	batch id:20	 lr:0.081367 loss:1.441284
[Train] epoch:98	batch id:30	 lr:0.081367 loss:1.698550
[Train] epoch:98	batch id:40	 lr:0.081367 loss:1.470194
[Train] epoch:98	batch id:50	 lr:0.081367 loss:1.584307
[Train] epoch:98	batch id:60	 lr:0.081367 loss:1.499017
[Train] epoch:98	batch id:70	 lr:0.081367 loss:1.538941
[Train] epoch:98	batch id:80	 lr:0.081367 loss:1.550973
[Train] epoch:98	batch id:90	 lr:0.081367 loss:1.683406
[Train] epoch:98	batch id:100	 lr:0.081367 loss:1.489126
[Train] epoch:98	batch id:110	 lr:0.081367 loss:1.663114
[Train] epoch:98	batch id:120	 lr:0.081367 loss:1.719379
[Train] epoch:98	batch id:130	 lr:0.081367 loss:1.640357
[Train] epoch:98	batch id:140	 lr:0.081367 loss:1.699200
[Train] epoch:98	batch id:150	 lr:0.081367 loss:1.561388
[Train] epoch:98	batch id:160	 lr:0.081367 loss:1.442010
[Train] epoch:98	batch id:170	 lr:0.081367 loss:1.612515
[Train] epoch:98	batch id:180	 lr:0.081367 loss:1.473064
[Train] epoch:98	batch id:190	 lr:0.081367 loss:1.627754
[Train] epoch:98	batch id:200	 lr:0.081367 loss:1.546288
[Train] epoch:98	batch id:210	 lr:0.081367 loss:1.530463
[Train] epoch:98	batch id:220	 lr:0.081367 loss:1.658482
[Train] epoch:98	batch id:230	 lr:0.081367 loss:1.504439
[Train] epoch:98	batch id:240	 lr:0.081367 loss:1.610271
[Train] epoch:98	batch id:250	 lr:0.081367 loss:1.651793
[Train] epoch:98	batch id:260	 lr:0.081367 loss:1.589595
[Train] epoch:98	batch id:270	 lr:0.081367 loss:1.726527
[Train] epoch:98	batch id:280	 lr:0.081367 loss:1.609563
[Train] epoch:98	batch id:290	 lr:0.081367 loss:1.652648
[Train] epoch:98	batch id:300	 lr:0.081367 loss:1.716833
[Train] 98, loss: 1.593426, train acc: 0.892712, 
[Test] epoch:98	batch id:0	 loss:1.349138
[Test] epoch:98	batch id:10	 loss:1.495207
[Test] epoch:98	batch id:20	 loss:1.448334
[Test] epoch:98	batch id:30	 loss:1.559999
[Test] epoch:98	batch id:40	 loss:1.420382
[Test] epoch:98	batch id:50	 loss:1.394995
[Test] epoch:98	batch id:60	 loss:1.296769
[Test] epoch:98	batch id:70	 loss:1.505803
[Test] epoch:98	batch id:80	 loss:1.526974
[Test] epoch:98	batch id:90	 loss:1.453421
[Test] epoch:98	batch id:100	 loss:1.749348
[Test] epoch:98	batch id:110	 loss:1.439158
[Test] epoch:98	batch id:120	 loss:1.394762
[Test] epoch:98	batch id:130	 loss:1.449781
[Test] epoch:98	batch id:140	 loss:1.343626
[Test] epoch:98	batch id:150	 loss:1.801016
[Test] 98, loss: 1.511860, test acc: 0.905592,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:99	batch id:0	 lr:0.081018 loss:1.657825
[Train] epoch:99	batch id:10	 lr:0.081018 loss:1.555792
[Train] epoch:99	batch id:20	 lr:0.081018 loss:1.538327
[Train] epoch:99	batch id:30	 lr:0.081018 loss:1.548664
[Train] epoch:99	batch id:40	 lr:0.081018 loss:1.575990
[Train] epoch:99	batch id:50	 lr:0.081018 loss:1.683470
[Train] epoch:99	batch id:60	 lr:0.081018 loss:1.501418
[Train] epoch:99	batch id:70	 lr:0.081018 loss:1.558898
[Train] epoch:99	batch id:80	 lr:0.081018 loss:1.535249
[Train] epoch:99	batch id:90	 lr:0.081018 loss:1.675321
[Train] epoch:99	batch id:100	 lr:0.081018 loss:1.569157
[Train] epoch:99	batch id:110	 lr:0.081018 loss:1.489183
[Train] epoch:99	batch id:120	 lr:0.081018 loss:1.542753
[Train] epoch:99	batch id:130	 lr:0.081018 loss:1.637439
[Train] epoch:99	batch id:140	 lr:0.081018 loss:1.583444
[Train] epoch:99	batch id:150	 lr:0.081018 loss:1.601397
[Train] epoch:99	batch id:160	 lr:0.081018 loss:1.553596
[Train] epoch:99	batch id:170	 lr:0.081018 loss:1.590740
[Train] epoch:99	batch id:180	 lr:0.081018 loss:1.539439
[Train] epoch:99	batch id:190	 lr:0.081018 loss:1.589199
[Train] epoch:99	batch id:200	 lr:0.081018 loss:1.543573
[Train] epoch:99	batch id:210	 lr:0.081018 loss:1.742787
[Train] epoch:99	batch id:220	 lr:0.081018 loss:1.568158
[Train] epoch:99	batch id:230	 lr:0.081018 loss:1.514297
[Train] epoch:99	batch id:240	 lr:0.081018 loss:1.630489
[Train] epoch:99	batch id:250	 lr:0.081018 loss:1.672306
[Train] epoch:99	batch id:260	 lr:0.081018 loss:1.735334
[Train] epoch:99	batch id:270	 lr:0.081018 loss:1.576411
[Train] epoch:99	batch id:280	 lr:0.081018 loss:1.519961
[Train] epoch:99	batch id:290	 lr:0.081018 loss:1.479598
[Train] epoch:99	batch id:300	 lr:0.081018 loss:1.636917
[Train] 99, loss: 1.606549, train acc: 0.890371, 
[Test] epoch:99	batch id:0	 loss:1.374370
[Test] epoch:99	batch id:10	 loss:1.406263
[Test] epoch:99	batch id:20	 loss:1.418305
[Test] epoch:99	batch id:30	 loss:1.494550
[Test] epoch:99	batch id:40	 loss:1.442679
[Test] epoch:99	batch id:50	 loss:1.506218
[Test] epoch:99	batch id:60	 loss:1.293255
[Test] epoch:99	batch id:70	 loss:1.542276
[Test] epoch:99	batch id:80	 loss:1.662558
[Test] epoch:99	batch id:90	 loss:1.507998
[Test] epoch:99	batch id:100	 loss:1.874590
[Test] epoch:99	batch id:110	 loss:1.459005
[Test] epoch:99	batch id:120	 loss:1.416538
[Test] epoch:99	batch id:130	 loss:1.470341
[Test] epoch:99	batch id:140	 loss:1.338163
[Test] epoch:99	batch id:150	 loss:1.821900
[Test] 99, loss: 1.515114, test acc: 0.905186,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:100	batch id:0	 lr:0.080667 loss:1.539433
[Train] epoch:100	batch id:10	 lr:0.080667 loss:1.478007
[Train] epoch:100	batch id:20	 lr:0.080667 loss:1.678352
[Train] epoch:100	batch id:30	 lr:0.080667 loss:1.802342
[Train] epoch:100	batch id:40	 lr:0.080667 loss:1.449428
[Train] epoch:100	batch id:50	 lr:0.080667 loss:1.639318
[Train] epoch:100	batch id:60	 lr:0.080667 loss:1.589365
[Train] epoch:100	batch id:70	 lr:0.080667 loss:1.510489
[Train] epoch:100	batch id:80	 lr:0.080667 loss:1.575192
[Train] epoch:100	batch id:90	 lr:0.080667 loss:1.535746
[Train] epoch:100	batch id:100	 lr:0.080667 loss:1.643356
[Train] epoch:100	batch id:110	 lr:0.080667 loss:1.596180
[Train] epoch:100	batch id:120	 lr:0.080667 loss:1.482304
[Train] epoch:100	batch id:130	 lr:0.080667 loss:1.692907
[Train] epoch:100	batch id:140	 lr:0.080667 loss:1.481170
[Train] epoch:100	batch id:150	 lr:0.080667 loss:1.802617
[Train] epoch:100	batch id:160	 lr:0.080667 loss:1.680695
[Train] epoch:100	batch id:170	 lr:0.080667 loss:1.550554
[Train] epoch:100	batch id:180	 lr:0.080667 loss:1.585465
[Train] epoch:100	batch id:190	 lr:0.080667 loss:1.953097
[Train] epoch:100	batch id:200	 lr:0.080667 loss:1.584334
[Train] epoch:100	batch id:210	 lr:0.080667 loss:1.631432
[Train] epoch:100	batch id:220	 lr:0.080667 loss:1.508001
[Train] epoch:100	batch id:230	 lr:0.080667 loss:1.580680
[Train] epoch:100	batch id:240	 lr:0.080667 loss:1.602245
[Train] epoch:100	batch id:250	 lr:0.080667 loss:1.598591
[Train] epoch:100	batch id:260	 lr:0.080667 loss:1.621857
[Train] epoch:100	batch id:270	 lr:0.080667 loss:1.427243
[Train] epoch:100	batch id:280	 lr:0.080667 loss:1.661282
[Train] epoch:100	batch id:290	 lr:0.080667 loss:1.437157
[Train] epoch:100	batch id:300	 lr:0.080667 loss:1.645815
[Train] 100, loss: 1.603737, train acc: 0.889658, 
[Test] epoch:100	batch id:0	 loss:1.402043
[Test] epoch:100	batch id:10	 loss:1.398072
[Test] epoch:100	batch id:20	 loss:1.426753
[Test] epoch:100	batch id:30	 loss:1.472108
[Test] epoch:100	batch id:40	 loss:1.415924
[Test] epoch:100	batch id:50	 loss:1.611215
[Test] epoch:100	batch id:60	 loss:1.317081
[Test] epoch:100	batch id:70	 loss:1.524395
[Test] epoch:100	batch id:80	 loss:1.587202
[Test] epoch:100	batch id:90	 loss:1.512264
[Test] epoch:100	batch id:100	 loss:1.848703
[Test] epoch:100	batch id:110	 loss:1.442884
[Test] epoch:100	batch id:120	 loss:1.444133
[Test] epoch:100	batch id:130	 loss:1.528958
[Test] epoch:100	batch id:140	 loss:1.395705
[Test] epoch:100	batch id:150	 loss:1.822037
[Test] 100, loss: 1.510428, test acc: 0.904376,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:101	batch id:0	 lr:0.080314 loss:1.590200
[Train] epoch:101	batch id:10	 lr:0.080314 loss:1.506418
[Train] epoch:101	batch id:20	 lr:0.080314 loss:1.482785
[Train] epoch:101	batch id:30	 lr:0.080314 loss:1.717723
[Train] epoch:101	batch id:40	 lr:0.080314 loss:1.642137
[Train] epoch:101	batch id:50	 lr:0.080314 loss:1.476315
[Train] epoch:101	batch id:60	 lr:0.080314 loss:1.560777
[Train] epoch:101	batch id:70	 lr:0.080314 loss:1.457564
[Train] epoch:101	batch id:80	 lr:0.080314 loss:1.644842
[Train] epoch:101	batch id:90	 lr:0.080314 loss:1.736592
[Train] epoch:101	batch id:100	 lr:0.080314 loss:1.991419
[Train] epoch:101	batch id:110	 lr:0.080314 loss:1.476894
[Train] epoch:101	batch id:120	 lr:0.080314 loss:1.482212
[Train] epoch:101	batch id:130	 lr:0.080314 loss:1.506180
[Train] epoch:101	batch id:140	 lr:0.080314 loss:1.696216
[Train] epoch:101	batch id:150	 lr:0.080314 loss:1.712630
[Train] epoch:101	batch id:160	 lr:0.080314 loss:1.578069
[Train] epoch:101	batch id:170	 lr:0.080314 loss:1.668734
[Train] epoch:101	batch id:180	 lr:0.080314 loss:1.585529
[Train] epoch:101	batch id:190	 lr:0.080314 loss:1.504239
[Train] epoch:101	batch id:200	 lr:0.080314 loss:1.704037
[Train] epoch:101	batch id:210	 lr:0.080314 loss:1.599727
[Train] epoch:101	batch id:220	 lr:0.080314 loss:1.782836
[Train] epoch:101	batch id:230	 lr:0.080314 loss:1.779476
[Train] epoch:101	batch id:240	 lr:0.080314 loss:1.600305
[Train] epoch:101	batch id:250	 lr:0.080314 loss:1.606296
[Train] epoch:101	batch id:260	 lr:0.080314 loss:1.500189
[Train] epoch:101	batch id:270	 lr:0.080314 loss:1.572757
[Train] epoch:101	batch id:280	 lr:0.080314 loss:1.763731
[Train] epoch:101	batch id:290	 lr:0.080314 loss:1.646736
[Train] epoch:101	batch id:300	 lr:0.080314 loss:1.606730
[Train] 101, loss: 1.604834, train acc: 0.890472, 
[Test] epoch:101	batch id:0	 loss:1.352557
[Test] epoch:101	batch id:10	 loss:1.423386
[Test] epoch:101	batch id:20	 loss:1.414516
[Test] epoch:101	batch id:30	 loss:1.418528
[Test] epoch:101	batch id:40	 loss:1.400528
[Test] epoch:101	batch id:50	 loss:1.467196
[Test] epoch:101	batch id:60	 loss:1.298953
[Test] epoch:101	batch id:70	 loss:1.513399
[Test] epoch:101	batch id:80	 loss:1.680998
[Test] epoch:101	batch id:90	 loss:1.472998
[Test] epoch:101	batch id:100	 loss:1.863025
[Test] epoch:101	batch id:110	 loss:1.430392
[Test] epoch:101	batch id:120	 loss:1.429697
[Test] epoch:101	batch id:130	 loss:1.580134
[Test] epoch:101	batch id:140	 loss:1.354062
[Test] epoch:101	batch id:150	 loss:2.025099
[Test] 101, loss: 1.532408, test acc: 0.899514,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:102	batch id:0	 lr:0.079958 loss:1.479130
[Train] epoch:102	batch id:10	 lr:0.079958 loss:1.471927
[Train] epoch:102	batch id:20	 lr:0.079958 loss:1.570321
[Train] epoch:102	batch id:30	 lr:0.079958 loss:1.511951
[Train] epoch:102	batch id:40	 lr:0.079958 loss:1.526656
[Train] epoch:102	batch id:50	 lr:0.079958 loss:1.581794
[Train] epoch:102	batch id:60	 lr:0.079958 loss:1.563905
[Train] epoch:102	batch id:70	 lr:0.079958 loss:1.539656
[Train] epoch:102	batch id:80	 lr:0.079958 loss:1.639864
[Train] epoch:102	batch id:90	 lr:0.079958 loss:1.872009
[Train] epoch:102	batch id:100	 lr:0.079958 loss:1.505778
[Train] epoch:102	batch id:110	 lr:0.079958 loss:1.674215
[Train] epoch:102	batch id:120	 lr:0.079958 loss:1.730064
[Train] epoch:102	batch id:130	 lr:0.079958 loss:1.655453
[Train] epoch:102	batch id:140	 lr:0.079958 loss:1.736886
[Train] epoch:102	batch id:150	 lr:0.079958 loss:1.624467
[Train] epoch:102	batch id:160	 lr:0.079958 loss:1.594942
[Train] epoch:102	batch id:170	 lr:0.079958 loss:1.703429
[Train] epoch:102	batch id:180	 lr:0.079958 loss:1.760075
[Train] epoch:102	batch id:190	 lr:0.079958 loss:1.909633
[Train] epoch:102	batch id:200	 lr:0.079958 loss:1.550051
[Train] epoch:102	batch id:210	 lr:0.079958 loss:1.473166
[Train] epoch:102	batch id:220	 lr:0.079958 loss:1.677986
[Train] epoch:102	batch id:230	 lr:0.079958 loss:1.518168
[Train] epoch:102	batch id:240	 lr:0.079958 loss:1.566921
[Train] epoch:102	batch id:250	 lr:0.079958 loss:1.601398
[Train] epoch:102	batch id:260	 lr:0.079958 loss:1.559483
[Train] epoch:102	batch id:270	 lr:0.079958 loss:1.747999
[Train] epoch:102	batch id:280	 lr:0.079958 loss:1.627481
[Train] epoch:102	batch id:290	 lr:0.079958 loss:1.761379
[Train] epoch:102	batch id:300	 lr:0.079958 loss:1.567989
[Train] 102, loss: 1.595383, train acc: 0.894849, 
[Test] epoch:102	batch id:0	 loss:1.334049
[Test] epoch:102	batch id:10	 loss:1.441434
[Test] epoch:102	batch id:20	 loss:1.509822
[Test] epoch:102	batch id:30	 loss:1.409004
[Test] epoch:102	batch id:40	 loss:1.455638
[Test] epoch:102	batch id:50	 loss:1.426710
[Test] epoch:102	batch id:60	 loss:1.304437
[Test] epoch:102	batch id:70	 loss:1.503175
[Test] epoch:102	batch id:80	 loss:1.694993
[Test] epoch:102	batch id:90	 loss:1.501460
[Test] epoch:102	batch id:100	 loss:1.851662
[Test] epoch:102	batch id:110	 loss:1.377767
[Test] epoch:102	batch id:120	 loss:1.418400
[Test] epoch:102	batch id:130	 loss:1.467545
[Test] epoch:102	batch id:140	 loss:1.412976
[Test] epoch:102	batch id:150	 loss:1.933777
[Test] 102, loss: 1.516469, test acc: 0.901945,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:103	batch id:0	 lr:0.079599 loss:1.753404
[Train] epoch:103	batch id:10	 lr:0.079599 loss:1.646294
[Train] epoch:103	batch id:20	 lr:0.079599 loss:1.737492
[Train] epoch:103	batch id:30	 lr:0.079599 loss:1.674682
[Train] epoch:103	batch id:40	 lr:0.079599 loss:1.608526
[Train] epoch:103	batch id:50	 lr:0.079599 loss:1.646014
[Train] epoch:103	batch id:60	 lr:0.079599 loss:1.556602
[Train] epoch:103	batch id:70	 lr:0.079599 loss:1.517573
[Train] epoch:103	batch id:80	 lr:0.079599 loss:1.689382
[Train] epoch:103	batch id:90	 lr:0.079599 loss:1.647993
[Train] epoch:103	batch id:100	 lr:0.079599 loss:1.426195
[Train] epoch:103	batch id:110	 lr:0.079599 loss:1.821581
[Train] epoch:103	batch id:120	 lr:0.079599 loss:1.622364
[Train] epoch:103	batch id:130	 lr:0.079599 loss:1.820485
[Train] epoch:103	batch id:140	 lr:0.079599 loss:1.664588
[Train] epoch:103	batch id:150	 lr:0.079599 loss:1.639848
[Train] epoch:103	batch id:160	 lr:0.079599 loss:1.537377
[Train] epoch:103	batch id:170	 lr:0.079599 loss:1.479180
[Train] epoch:103	batch id:180	 lr:0.079599 loss:1.569464
[Train] epoch:103	batch id:190	 lr:0.079599 loss:1.518855
[Train] epoch:103	batch id:200	 lr:0.079599 loss:1.461642
[Train] epoch:103	batch id:210	 lr:0.079599 loss:1.550664
[Train] epoch:103	batch id:220	 lr:0.079599 loss:1.683890
[Train] epoch:103	batch id:230	 lr:0.079599 loss:1.461720
[Train] epoch:103	batch id:240	 lr:0.079599 loss:1.610613
[Train] epoch:103	batch id:250	 lr:0.079599 loss:1.577908
[Train] epoch:103	batch id:260	 lr:0.079599 loss:1.697996
[Train] epoch:103	batch id:270	 lr:0.079599 loss:1.601068
[Train] epoch:103	batch id:280	 lr:0.079599 loss:1.795018
[Train] epoch:103	batch id:290	 lr:0.079599 loss:1.581390
[Train] epoch:103	batch id:300	 lr:0.079599 loss:1.656823
[Train] 103, loss: 1.596009, train acc: 0.892915, 
[Test] epoch:103	batch id:0	 loss:1.349780
[Test] epoch:103	batch id:10	 loss:1.567837
[Test] epoch:103	batch id:20	 loss:1.486996
[Test] epoch:103	batch id:30	 loss:1.512452
[Test] epoch:103	batch id:40	 loss:1.462067
[Test] epoch:103	batch id:50	 loss:1.432225
[Test] epoch:103	batch id:60	 loss:1.317138
[Test] epoch:103	batch id:70	 loss:1.472181
[Test] epoch:103	batch id:80	 loss:1.724244
[Test] epoch:103	batch id:90	 loss:1.466534
[Test] epoch:103	batch id:100	 loss:2.002213
[Test] epoch:103	batch id:110	 loss:1.390763
[Test] epoch:103	batch id:120	 loss:1.479305
[Test] epoch:103	batch id:130	 loss:1.442234
[Test] epoch:103	batch id:140	 loss:1.404239
[Test] epoch:103	batch id:150	 loss:1.988495
[Test] 103, loss: 1.546361, test acc: 0.899109,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:104	batch id:0	 lr:0.079239 loss:1.601931
[Train] epoch:104	batch id:10	 lr:0.079239 loss:1.807055
[Train] epoch:104	batch id:20	 lr:0.079239 loss:1.638517
[Train] epoch:104	batch id:30	 lr:0.079239 loss:1.571616
[Train] epoch:104	batch id:40	 lr:0.079239 loss:1.515472
[Train] epoch:104	batch id:50	 lr:0.079239 loss:1.562481
[Train] epoch:104	batch id:60	 lr:0.079239 loss:1.610252
[Train] epoch:104	batch id:70	 lr:0.079239 loss:1.681891
[Train] epoch:104	batch id:80	 lr:0.079239 loss:1.563199
[Train] epoch:104	batch id:90	 lr:0.079239 loss:1.841776
[Train] epoch:104	batch id:100	 lr:0.079239 loss:1.761328
[Train] epoch:104	batch id:110	 lr:0.079239 loss:1.768317
[Train] epoch:104	batch id:120	 lr:0.079239 loss:1.483801
[Train] epoch:104	batch id:130	 lr:0.079239 loss:1.583575
[Train] epoch:104	batch id:140	 lr:0.079239 loss:1.775776
[Train] epoch:104	batch id:150	 lr:0.079239 loss:1.538354
[Train] epoch:104	batch id:160	 lr:0.079239 loss:1.768477
[Train] epoch:104	batch id:170	 lr:0.079239 loss:1.589279
[Train] epoch:104	batch id:180	 lr:0.079239 loss:1.561346
[Train] epoch:104	batch id:190	 lr:0.079239 loss:1.537659
[Train] epoch:104	batch id:200	 lr:0.079239 loss:1.496760
[Train] epoch:104	batch id:210	 lr:0.079239 loss:1.560309
[Train] epoch:104	batch id:220	 lr:0.079239 loss:1.562656
[Train] epoch:104	batch id:230	 lr:0.079239 loss:1.551131
[Train] epoch:104	batch id:240	 lr:0.079239 loss:1.549910
[Train] epoch:104	batch id:250	 lr:0.079239 loss:1.849517
[Train] epoch:104	batch id:260	 lr:0.079239 loss:1.479826
[Train] epoch:104	batch id:270	 lr:0.079239 loss:1.546269
[Train] epoch:104	batch id:280	 lr:0.079239 loss:1.576862
[Train] epoch:104	batch id:290	 lr:0.079239 loss:1.665176
[Train] epoch:104	batch id:300	 lr:0.079239 loss:1.578545
[Train] 104, loss: 1.593391, train acc: 0.894035, 
[Test] epoch:104	batch id:0	 loss:1.387535
[Test] epoch:104	batch id:10	 loss:1.522918
[Test] epoch:104	batch id:20	 loss:1.426164
[Test] epoch:104	batch id:30	 loss:1.542505
[Test] epoch:104	batch id:40	 loss:1.409463
[Test] epoch:104	batch id:50	 loss:1.417695
[Test] epoch:104	batch id:60	 loss:1.356337
[Test] epoch:104	batch id:70	 loss:1.530163
[Test] epoch:104	batch id:80	 loss:1.730788
[Test] epoch:104	batch id:90	 loss:1.497220
[Test] epoch:104	batch id:100	 loss:2.059235
[Test] epoch:104	batch id:110	 loss:1.431136
[Test] epoch:104	batch id:120	 loss:1.412330
[Test] epoch:104	batch id:130	 loss:1.406445
[Test] epoch:104	batch id:140	 loss:1.399883
[Test] epoch:104	batch id:150	 loss:1.971181
[Test] 104, loss: 1.540675, test acc: 0.900729,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:105	batch id:0	 lr:0.078876 loss:1.518614
[Train] epoch:105	batch id:10	 lr:0.078876 loss:1.665388
[Train] epoch:105	batch id:20	 lr:0.078876 loss:1.534002
[Train] epoch:105	batch id:30	 lr:0.078876 loss:1.541340
[Train] epoch:105	batch id:40	 lr:0.078876 loss:1.559201
[Train] epoch:105	batch id:50	 lr:0.078876 loss:1.701430
[Train] epoch:105	batch id:60	 lr:0.078876 loss:1.440718
[Train] epoch:105	batch id:70	 lr:0.078876 loss:1.725138
[Train] epoch:105	batch id:80	 lr:0.078876 loss:1.476660
[Train] epoch:105	batch id:90	 lr:0.078876 loss:1.476218
[Train] epoch:105	batch id:100	 lr:0.078876 loss:1.526153
[Train] epoch:105	batch id:110	 lr:0.078876 loss:1.611201
[Train] epoch:105	batch id:120	 lr:0.078876 loss:1.574538
[Train] epoch:105	batch id:130	 lr:0.078876 loss:1.630724
[Train] epoch:105	batch id:140	 lr:0.078876 loss:1.644098
[Train] epoch:105	batch id:150	 lr:0.078876 loss:1.616415
[Train] epoch:105	batch id:160	 lr:0.078876 loss:1.502731
[Train] epoch:105	batch id:170	 lr:0.078876 loss:1.589477
[Train] epoch:105	batch id:180	 lr:0.078876 loss:1.548635
[Train] epoch:105	batch id:190	 lr:0.078876 loss:1.664890
[Train] epoch:105	batch id:200	 lr:0.078876 loss:1.588218
[Train] epoch:105	batch id:210	 lr:0.078876 loss:1.493809
[Train] epoch:105	batch id:220	 lr:0.078876 loss:1.581356
[Train] epoch:105	batch id:230	 lr:0.078876 loss:1.545413
[Train] epoch:105	batch id:240	 lr:0.078876 loss:1.643408
[Train] epoch:105	batch id:250	 lr:0.078876 loss:1.507370
[Train] epoch:105	batch id:260	 lr:0.078876 loss:1.520033
[Train] epoch:105	batch id:270	 lr:0.078876 loss:1.591568
[Train] epoch:105	batch id:280	 lr:0.078876 loss:1.792587
[Train] epoch:105	batch id:290	 lr:0.078876 loss:1.657942
[Train] epoch:105	batch id:300	 lr:0.078876 loss:1.603256
[Train] 105, loss: 1.601491, train acc: 0.891083, 
[Test] epoch:105	batch id:0	 loss:1.400705
[Test] epoch:105	batch id:10	 loss:1.513672
[Test] epoch:105	batch id:20	 loss:1.458933
[Test] epoch:105	batch id:30	 loss:1.527249
[Test] epoch:105	batch id:40	 loss:1.331313
[Test] epoch:105	batch id:50	 loss:1.496173
[Test] epoch:105	batch id:60	 loss:1.342146
[Test] epoch:105	batch id:70	 loss:1.548376
[Test] epoch:105	batch id:80	 loss:1.665421
[Test] epoch:105	batch id:90	 loss:1.452998
[Test] epoch:105	batch id:100	 loss:2.106245
[Test] epoch:105	batch id:110	 loss:1.441216
[Test] epoch:105	batch id:120	 loss:1.401714
[Test] epoch:105	batch id:130	 loss:1.563478
[Test] epoch:105	batch id:140	 loss:1.400314
[Test] epoch:105	batch id:150	 loss:1.797550
[Test] 105, loss: 1.538119, test acc: 0.893031,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:106	batch id:0	 lr:0.078511 loss:1.449417
[Train] epoch:106	batch id:10	 lr:0.078511 loss:1.528942
[Train] epoch:106	batch id:20	 lr:0.078511 loss:1.534914
[Train] epoch:106	batch id:30	 lr:0.078511 loss:1.494329
[Train] epoch:106	batch id:40	 lr:0.078511 loss:1.707306
[Train] epoch:106	batch id:50	 lr:0.078511 loss:1.459191
[Train] epoch:106	batch id:60	 lr:0.078511 loss:1.684269
[Train] epoch:106	batch id:70	 lr:0.078511 loss:1.516309
[Train] epoch:106	batch id:80	 lr:0.078511 loss:1.480970
[Train] epoch:106	batch id:90	 lr:0.078511 loss:1.454770
[Train] epoch:106	batch id:100	 lr:0.078511 loss:1.431902
[Train] epoch:106	batch id:110	 lr:0.078511 loss:1.721231
[Train] epoch:106	batch id:120	 lr:0.078511 loss:1.592559
[Train] epoch:106	batch id:130	 lr:0.078511 loss:1.560562
[Train] epoch:106	batch id:140	 lr:0.078511 loss:1.467761
[Train] epoch:106	batch id:150	 lr:0.078511 loss:1.528603
[Train] epoch:106	batch id:160	 lr:0.078511 loss:1.722088
[Train] epoch:106	batch id:170	 lr:0.078511 loss:1.650980
[Train] epoch:106	batch id:180	 lr:0.078511 loss:1.686421
[Train] epoch:106	batch id:190	 lr:0.078511 loss:1.477351
[Train] epoch:106	batch id:200	 lr:0.078511 loss:1.609205
[Train] epoch:106	batch id:210	 lr:0.078511 loss:1.607476
[Train] epoch:106	batch id:220	 lr:0.078511 loss:1.514993
[Train] epoch:106	batch id:230	 lr:0.078511 loss:1.455909
[Train] epoch:106	batch id:240	 lr:0.078511 loss:1.707156
[Train] epoch:106	batch id:250	 lr:0.078511 loss:1.473156
[Train] epoch:106	batch id:260	 lr:0.078511 loss:1.575609
[Train] epoch:106	batch id:270	 lr:0.078511 loss:1.545642
[Train] epoch:106	batch id:280	 lr:0.078511 loss:1.569364
[Train] epoch:106	batch id:290	 lr:0.078511 loss:1.591500
[Train] epoch:106	batch id:300	 lr:0.078511 loss:1.502782
[Train] 106, loss: 1.591559, train acc: 0.895969, 
[Test] epoch:106	batch id:0	 loss:1.341097
[Test] epoch:106	batch id:10	 loss:1.452384
[Test] epoch:106	batch id:20	 loss:1.426723
[Test] epoch:106	batch id:30	 loss:1.488917
[Test] epoch:106	batch id:40	 loss:1.467379
[Test] epoch:106	batch id:50	 loss:1.386602
[Test] epoch:106	batch id:60	 loss:1.308059
[Test] epoch:106	batch id:70	 loss:1.400690
[Test] epoch:106	batch id:80	 loss:1.560105
[Test] epoch:106	batch id:90	 loss:1.509550
[Test] epoch:106	batch id:100	 loss:1.809203
[Test] epoch:106	batch id:110	 loss:1.505847
[Test] epoch:106	batch id:120	 loss:1.421866
[Test] epoch:106	batch id:130	 loss:1.502657
[Test] epoch:106	batch id:140	 loss:1.490034
[Test] epoch:106	batch id:150	 loss:1.781659
[Test] 106, loss: 1.522588, test acc: 0.903566,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:107	batch id:0	 lr:0.078143 loss:1.902886
[Train] epoch:107	batch id:10	 lr:0.078143 loss:1.610668
[Train] epoch:107	batch id:20	 lr:0.078143 loss:1.444217
[Train] epoch:107	batch id:30	 lr:0.078143 loss:1.501764
[Train] epoch:107	batch id:40	 lr:0.078143 loss:1.516143
[Train] epoch:107	batch id:50	 lr:0.078143 loss:1.491067
[Train] epoch:107	batch id:60	 lr:0.078143 loss:1.546617
[Train] epoch:107	batch id:70	 lr:0.078143 loss:1.608380
[Train] epoch:107	batch id:80	 lr:0.078143 loss:1.529803
[Train] epoch:107	batch id:90	 lr:0.078143 loss:1.447347
[Train] epoch:107	batch id:100	 lr:0.078143 loss:1.522184
[Train] epoch:107	batch id:110	 lr:0.078143 loss:1.507947
[Train] epoch:107	batch id:120	 lr:0.078143 loss:1.591599
[Train] epoch:107	batch id:130	 lr:0.078143 loss:1.691294
[Train] epoch:107	batch id:140	 lr:0.078143 loss:1.527895
[Train] epoch:107	batch id:150	 lr:0.078143 loss:1.497170
[Train] epoch:107	batch id:160	 lr:0.078143 loss:1.575002
[Train] epoch:107	batch id:170	 lr:0.078143 loss:1.497039
[Train] epoch:107	batch id:180	 lr:0.078143 loss:1.637060
[Train] epoch:107	batch id:190	 lr:0.078143 loss:1.432640
[Train] epoch:107	batch id:200	 lr:0.078143 loss:1.600121
[Train] epoch:107	batch id:210	 lr:0.078143 loss:1.515205
[Train] epoch:107	batch id:220	 lr:0.078143 loss:1.695027
[Train] epoch:107	batch id:230	 lr:0.078143 loss:1.777592
[Train] epoch:107	batch id:240	 lr:0.078143 loss:1.535069
[Train] epoch:107	batch id:250	 lr:0.078143 loss:1.831137
[Train] epoch:107	batch id:260	 lr:0.078143 loss:1.715257
[Train] epoch:107	batch id:270	 lr:0.078143 loss:1.490366
[Train] epoch:107	batch id:280	 lr:0.078143 loss:1.539891
[Train] epoch:107	batch id:290	 lr:0.078143 loss:1.589464
[Train] epoch:107	batch id:300	 lr:0.078143 loss:1.597753
[Train] 107, loss: 1.585208, train acc: 0.897394, 
[Test] epoch:107	batch id:0	 loss:1.378691
[Test] epoch:107	batch id:10	 loss:1.429375
[Test] epoch:107	batch id:20	 loss:1.449817
[Test] epoch:107	batch id:30	 loss:1.452796
[Test] epoch:107	batch id:40	 loss:1.438897
[Test] epoch:107	batch id:50	 loss:1.481570
[Test] epoch:107	batch id:60	 loss:1.321868
[Test] epoch:107	batch id:70	 loss:1.424874
[Test] epoch:107	batch id:80	 loss:1.692363
[Test] epoch:107	batch id:90	 loss:1.518061
[Test] epoch:107	batch id:100	 loss:1.945950
[Test] epoch:107	batch id:110	 loss:1.550684
[Test] epoch:107	batch id:120	 loss:1.381622
[Test] epoch:107	batch id:130	 loss:1.592249
[Test] epoch:107	batch id:140	 loss:1.522434
[Test] epoch:107	batch id:150	 loss:1.735002
[Test] 107, loss: 1.559423, test acc: 0.878039,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:108	batch id:0	 lr:0.077773 loss:1.475708
[Train] epoch:108	batch id:10	 lr:0.077773 loss:1.461544
[Train] epoch:108	batch id:20	 lr:0.077773 loss:1.523969
[Train] epoch:108	batch id:30	 lr:0.077773 loss:1.517963
[Train] epoch:108	batch id:40	 lr:0.077773 loss:1.467268
[Train] epoch:108	batch id:50	 lr:0.077773 loss:1.560501
[Train] epoch:108	batch id:60	 lr:0.077773 loss:1.537153
[Train] epoch:108	batch id:70	 lr:0.077773 loss:1.595757
[Train] epoch:108	batch id:80	 lr:0.077773 loss:1.507943
[Train] epoch:108	batch id:90	 lr:0.077773 loss:1.674934
[Train] epoch:108	batch id:100	 lr:0.077773 loss:1.539725
[Train] epoch:108	batch id:110	 lr:0.077773 loss:1.636841
[Train] epoch:108	batch id:120	 lr:0.077773 loss:1.593015
[Train] epoch:108	batch id:130	 lr:0.077773 loss:1.491765
[Train] epoch:108	batch id:140	 lr:0.077773 loss:1.597181
[Train] epoch:108	batch id:150	 lr:0.077773 loss:1.456397
[Train] epoch:108	batch id:160	 lr:0.077773 loss:1.408511
[Train] epoch:108	batch id:170	 lr:0.077773 loss:1.470678
[Train] epoch:108	batch id:180	 lr:0.077773 loss:1.553961
[Train] epoch:108	batch id:190	 lr:0.077773 loss:1.464804
[Train] epoch:108	batch id:200	 lr:0.077773 loss:1.648066
[Train] epoch:108	batch id:210	 lr:0.077773 loss:1.494111
[Train] epoch:108	batch id:220	 lr:0.077773 loss:1.529520
[Train] epoch:108	batch id:230	 lr:0.077773 loss:1.730837
[Train] epoch:108	batch id:240	 lr:0.077773 loss:1.530255
[Train] epoch:108	batch id:250	 lr:0.077773 loss:1.579072
[Train] epoch:108	batch id:260	 lr:0.077773 loss:1.575882
[Train] epoch:108	batch id:270	 lr:0.077773 loss:1.461023
[Train] epoch:108	batch id:280	 lr:0.077773 loss:1.493581
[Train] epoch:108	batch id:290	 lr:0.077773 loss:1.609500
[Train] epoch:108	batch id:300	 lr:0.077773 loss:1.548475
[Train] 108, loss: 1.585174, train acc: 0.898310, 
[Test] epoch:108	batch id:0	 loss:1.361369
[Test] epoch:108	batch id:10	 loss:1.404490
[Test] epoch:108	batch id:20	 loss:1.389364
[Test] epoch:108	batch id:30	 loss:1.511836
[Test] epoch:108	batch id:40	 loss:1.415904
[Test] epoch:108	batch id:50	 loss:1.516205
[Test] epoch:108	batch id:60	 loss:1.350189
[Test] epoch:108	batch id:70	 loss:1.513135
[Test] epoch:108	batch id:80	 loss:1.743395
[Test] epoch:108	batch id:90	 loss:1.538147
[Test] epoch:108	batch id:100	 loss:1.902876
[Test] epoch:108	batch id:110	 loss:1.452687
[Test] epoch:108	batch id:120	 loss:1.467535
[Test] epoch:108	batch id:130	 loss:1.526550
[Test] epoch:108	batch id:140	 loss:1.424882
[Test] epoch:108	batch id:150	 loss:1.987308
[Test] 108, loss: 1.525967, test acc: 0.904781,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:109	batch id:0	 lr:0.077401 loss:1.531451
[Train] epoch:109	batch id:10	 lr:0.077401 loss:1.663511
[Train] epoch:109	batch id:20	 lr:0.077401 loss:1.637994
[Train] epoch:109	batch id:30	 lr:0.077401 loss:1.457610
[Train] epoch:109	batch id:40	 lr:0.077401 loss:1.574857
[Train] epoch:109	batch id:50	 lr:0.077401 loss:1.505867
[Train] epoch:109	batch id:60	 lr:0.077401 loss:1.563908
[Train] epoch:109	batch id:70	 lr:0.077401 loss:1.470100
[Train] epoch:109	batch id:80	 lr:0.077401 loss:1.477146
[Train] epoch:109	batch id:90	 lr:0.077401 loss:1.768058
[Train] epoch:109	batch id:100	 lr:0.077401 loss:1.700550
[Train] epoch:109	batch id:110	 lr:0.077401 loss:1.584911
[Train] epoch:109	batch id:120	 lr:0.077401 loss:1.539181
[Train] epoch:109	batch id:130	 lr:0.077401 loss:1.658414
[Train] epoch:109	batch id:140	 lr:0.077401 loss:1.503799
[Train] epoch:109	batch id:150	 lr:0.077401 loss:1.709775
[Train] epoch:109	batch id:160	 lr:0.077401 loss:1.704483
[Train] epoch:109	batch id:170	 lr:0.077401 loss:1.565395
[Train] epoch:109	batch id:180	 lr:0.077401 loss:1.613205
[Train] epoch:109	batch id:190	 lr:0.077401 loss:1.607722
[Train] epoch:109	batch id:200	 lr:0.077401 loss:1.687812
[Train] epoch:109	batch id:210	 lr:0.077401 loss:1.578957
[Train] epoch:109	batch id:220	 lr:0.077401 loss:1.547062
[Train] epoch:109	batch id:230	 lr:0.077401 loss:1.798169
[Train] epoch:109	batch id:240	 lr:0.077401 loss:1.564872
[Train] epoch:109	batch id:250	 lr:0.077401 loss:1.673606
[Train] epoch:109	batch id:260	 lr:0.077401 loss:1.485432
[Train] epoch:109	batch id:270	 lr:0.077401 loss:1.706049
[Train] epoch:109	batch id:280	 lr:0.077401 loss:1.804725
[Train] epoch:109	batch id:290	 lr:0.077401 loss:1.681397
[Train] epoch:109	batch id:300	 lr:0.077401 loss:1.615236
[Train] 109, loss: 1.602902, train acc: 0.893119, 
[Test] epoch:109	batch id:0	 loss:1.468395
[Test] epoch:109	batch id:10	 loss:1.536513
[Test] epoch:109	batch id:20	 loss:1.411641
[Test] epoch:109	batch id:30	 loss:1.666962
[Test] epoch:109	batch id:40	 loss:1.517224
[Test] epoch:109	batch id:50	 loss:1.479817
[Test] epoch:109	batch id:60	 loss:1.346262
[Test] epoch:109	batch id:70	 loss:1.560169
[Test] epoch:109	batch id:80	 loss:1.647095
[Test] epoch:109	batch id:90	 loss:1.549339
[Test] epoch:109	batch id:100	 loss:2.021842
[Test] epoch:109	batch id:110	 loss:1.441424
[Test] epoch:109	batch id:120	 loss:1.467382
[Test] epoch:109	batch id:130	 loss:1.550894
[Test] epoch:109	batch id:140	 loss:1.348091
[Test] epoch:109	batch id:150	 loss:1.825977
[Test] 109, loss: 1.546203, test acc: 0.897488,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:110	batch id:0	 lr:0.077027 loss:1.548575
[Train] epoch:110	batch id:10	 lr:0.077027 loss:1.562104
[Train] epoch:110	batch id:20	 lr:0.077027 loss:1.590336
[Train] epoch:110	batch id:30	 lr:0.077027 loss:1.600692
[Train] epoch:110	batch id:40	 lr:0.077027 loss:1.498676
[Train] epoch:110	batch id:50	 lr:0.077027 loss:1.857159
[Train] epoch:110	batch id:60	 lr:0.077027 loss:1.485162
[Train] epoch:110	batch id:70	 lr:0.077027 loss:1.572779
[Train] epoch:110	batch id:80	 lr:0.077027 loss:1.633824
[Train] epoch:110	batch id:90	 lr:0.077027 loss:1.561096
[Train] epoch:110	batch id:100	 lr:0.077027 loss:1.711810
[Train] epoch:110	batch id:110	 lr:0.077027 loss:1.611540
[Train] epoch:110	batch id:120	 lr:0.077027 loss:1.555187
[Train] epoch:110	batch id:130	 lr:0.077027 loss:1.506390
[Train] epoch:110	batch id:140	 lr:0.077027 loss:1.561884
[Train] epoch:110	batch id:150	 lr:0.077027 loss:1.734586
[Train] epoch:110	batch id:160	 lr:0.077027 loss:1.647095
[Train] epoch:110	batch id:170	 lr:0.077027 loss:1.481707
[Train] epoch:110	batch id:180	 lr:0.077027 loss:1.530694
[Train] epoch:110	batch id:190	 lr:0.077027 loss:1.552488
[Train] epoch:110	batch id:200	 lr:0.077027 loss:1.798463
[Train] epoch:110	batch id:210	 lr:0.077027 loss:1.477704
[Train] epoch:110	batch id:220	 lr:0.077027 loss:1.710283
[Train] epoch:110	batch id:230	 lr:0.077027 loss:1.501660
[Train] epoch:110	batch id:240	 lr:0.077027 loss:1.507651
[Train] epoch:110	batch id:250	 lr:0.077027 loss:1.589006
[Train] epoch:110	batch id:260	 lr:0.077027 loss:1.575956
[Train] epoch:110	batch id:270	 lr:0.077027 loss:1.650256
[Train] epoch:110	batch id:280	 lr:0.077027 loss:1.520865
[Train] epoch:110	batch id:290	 lr:0.077027 loss:1.668072
[Train] epoch:110	batch id:300	 lr:0.077027 loss:1.752034
[Train] 110, loss: 1.592806, train acc: 0.895257, 
[Test] epoch:110	batch id:0	 loss:1.380283
[Test] epoch:110	batch id:10	 loss:1.525904
[Test] epoch:110	batch id:20	 loss:1.477661
[Test] epoch:110	batch id:30	 loss:1.527999
[Test] epoch:110	batch id:40	 loss:1.523703
[Test] epoch:110	batch id:50	 loss:1.505956
[Test] epoch:110	batch id:60	 loss:1.298041
[Test] epoch:110	batch id:70	 loss:1.485081
[Test] epoch:110	batch id:80	 loss:1.577555
[Test] epoch:110	batch id:90	 loss:1.502143
[Test] epoch:110	batch id:100	 loss:1.918817
[Test] epoch:110	batch id:110	 loss:1.444822
[Test] epoch:110	batch id:120	 loss:1.494607
[Test] epoch:110	batch id:130	 loss:1.554937
[Test] epoch:110	batch id:140	 loss:1.466053
[Test] epoch:110	batch id:150	 loss:1.785672
[Test] 110, loss: 1.539140, test acc: 0.892626,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:111	batch id:0	 lr:0.076651 loss:1.609772
[Train] epoch:111	batch id:10	 lr:0.076651 loss:1.549503
[Train] epoch:111	batch id:20	 lr:0.076651 loss:1.574759
[Train] epoch:111	batch id:30	 lr:0.076651 loss:1.422284
[Train] epoch:111	batch id:40	 lr:0.076651 loss:1.499040
[Train] epoch:111	batch id:50	 lr:0.076651 loss:1.544871
[Train] epoch:111	batch id:60	 lr:0.076651 loss:1.742604
[Train] epoch:111	batch id:70	 lr:0.076651 loss:1.559712
[Train] epoch:111	batch id:80	 lr:0.076651 loss:1.519089
[Train] epoch:111	batch id:90	 lr:0.076651 loss:1.558442
[Train] epoch:111	batch id:100	 lr:0.076651 loss:1.590354
[Train] epoch:111	batch id:110	 lr:0.076651 loss:1.475736
[Train] epoch:111	batch id:120	 lr:0.076651 loss:1.493775
[Train] epoch:111	batch id:130	 lr:0.076651 loss:1.481957
[Train] epoch:111	batch id:140	 lr:0.076651 loss:1.695036
[Train] epoch:111	batch id:150	 lr:0.076651 loss:1.957702
[Train] epoch:111	batch id:160	 lr:0.076651 loss:1.688651
[Train] epoch:111	batch id:170	 lr:0.076651 loss:1.652778
[Train] epoch:111	batch id:180	 lr:0.076651 loss:1.499343
[Train] epoch:111	batch id:190	 lr:0.076651 loss:1.672981
[Train] epoch:111	batch id:200	 lr:0.076651 loss:1.614279
[Train] epoch:111	batch id:210	 lr:0.076651 loss:1.572917
[Train] epoch:111	batch id:220	 lr:0.076651 loss:1.693572
[Train] epoch:111	batch id:230	 lr:0.076651 loss:1.594992
[Train] epoch:111	batch id:240	 lr:0.076651 loss:1.459100
[Train] epoch:111	batch id:250	 lr:0.076651 loss:1.542860
[Train] epoch:111	batch id:260	 lr:0.076651 loss:1.699468
[Train] epoch:111	batch id:270	 lr:0.076651 loss:1.745745
[Train] epoch:111	batch id:280	 lr:0.076651 loss:1.681292
[Train] epoch:111	batch id:290	 lr:0.076651 loss:1.511261
[Train] epoch:111	batch id:300	 lr:0.076651 loss:1.545443
[Train] 111, loss: 1.588280, train acc: 0.898107, 
[Test] epoch:111	batch id:0	 loss:1.378983
[Test] epoch:111	batch id:10	 loss:1.509780
[Test] epoch:111	batch id:20	 loss:1.433040
[Test] epoch:111	batch id:30	 loss:1.412089
[Test] epoch:111	batch id:40	 loss:1.380810
[Test] epoch:111	batch id:50	 loss:1.462794
[Test] epoch:111	batch id:60	 loss:1.303852
[Test] epoch:111	batch id:70	 loss:1.421755
[Test] epoch:111	batch id:80	 loss:1.740529
[Test] epoch:111	batch id:90	 loss:1.498553
[Test] epoch:111	batch id:100	 loss:1.889480
[Test] epoch:111	batch id:110	 loss:1.476576
[Test] epoch:111	batch id:120	 loss:1.492164
[Test] epoch:111	batch id:130	 loss:1.541163
[Test] epoch:111	batch id:140	 loss:1.433131
[Test] epoch:111	batch id:150	 loss:1.906623
[Test] 111, loss: 1.535711, test acc: 0.902350,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:112	batch id:0	 lr:0.076273 loss:1.686910
[Train] epoch:112	batch id:10	 lr:0.076273 loss:1.491704
[Train] epoch:112	batch id:20	 lr:0.076273 loss:1.707038
[Train] epoch:112	batch id:30	 lr:0.076273 loss:1.437647
[Train] epoch:112	batch id:40	 lr:0.076273 loss:1.462523
[Train] epoch:112	batch id:50	 lr:0.076273 loss:1.537781
[Train] epoch:112	batch id:60	 lr:0.076273 loss:1.590302
[Train] epoch:112	batch id:70	 lr:0.076273 loss:1.522481
[Train] epoch:112	batch id:80	 lr:0.076273 loss:1.486109
[Train] epoch:112	batch id:90	 lr:0.076273 loss:1.533672
[Train] epoch:112	batch id:100	 lr:0.076273 loss:1.442776
[Train] epoch:112	batch id:110	 lr:0.076273 loss:1.454290
[Train] epoch:112	batch id:120	 lr:0.076273 loss:1.523646
[Train] epoch:112	batch id:130	 lr:0.076273 loss:1.527574
[Train] epoch:112	batch id:140	 lr:0.076273 loss:1.590573
[Train] epoch:112	batch id:150	 lr:0.076273 loss:1.733308
[Train] epoch:112	batch id:160	 lr:0.076273 loss:1.875806
[Train] epoch:112	batch id:170	 lr:0.076273 loss:1.572589
[Train] epoch:112	batch id:180	 lr:0.076273 loss:1.516219
[Train] epoch:112	batch id:190	 lr:0.076273 loss:1.549566
[Train] epoch:112	batch id:200	 lr:0.076273 loss:1.549754
[Train] epoch:112	batch id:210	 lr:0.076273 loss:1.616452
[Train] epoch:112	batch id:220	 lr:0.076273 loss:1.713977
[Train] epoch:112	batch id:230	 lr:0.076273 loss:1.483839
[Train] epoch:112	batch id:240	 lr:0.076273 loss:1.484417
[Train] epoch:112	batch id:250	 lr:0.076273 loss:1.489243
[Train] epoch:112	batch id:260	 lr:0.076273 loss:1.487982
[Train] epoch:112	batch id:270	 lr:0.076273 loss:1.549348
[Train] epoch:112	batch id:280	 lr:0.076273 loss:1.570100
[Train] epoch:112	batch id:290	 lr:0.076273 loss:1.533629
[Train] epoch:112	batch id:300	 lr:0.076273 loss:1.631274
[Train] 112, loss: 1.577701, train acc: 0.899125, 
[Test] epoch:112	batch id:0	 loss:1.337733
[Test] epoch:112	batch id:10	 loss:1.421966
[Test] epoch:112	batch id:20	 loss:1.403709
[Test] epoch:112	batch id:30	 loss:1.446131
[Test] epoch:112	batch id:40	 loss:1.396026
[Test] epoch:112	batch id:50	 loss:1.429820
[Test] epoch:112	batch id:60	 loss:1.360558
[Test] epoch:112	batch id:70	 loss:1.487598
[Test] epoch:112	batch id:80	 loss:1.661142
[Test] epoch:112	batch id:90	 loss:1.640184
[Test] epoch:112	batch id:100	 loss:1.901856
[Test] epoch:112	batch id:110	 loss:1.471940
[Test] epoch:112	batch id:120	 loss:1.519806
[Test] epoch:112	batch id:130	 loss:1.528423
[Test] epoch:112	batch id:140	 loss:1.547201
[Test] epoch:112	batch id:150	 loss:1.774188
[Test] 112, loss: 1.516158, test acc: 0.914100,
Max Acc:0.914100
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:113	batch id:0	 lr:0.075892 loss:1.491851
[Train] epoch:113	batch id:10	 lr:0.075892 loss:1.623346
[Train] epoch:113	batch id:20	 lr:0.075892 loss:1.420352
[Train] epoch:113	batch id:30	 lr:0.075892 loss:1.594900
[Train] epoch:113	batch id:40	 lr:0.075892 loss:1.546543
[Train] epoch:113	batch id:50	 lr:0.075892 loss:1.885373
[Train] epoch:113	batch id:60	 lr:0.075892 loss:1.462154
[Train] epoch:113	batch id:70	 lr:0.075892 loss:1.516964
[Train] epoch:113	batch id:80	 lr:0.075892 loss:1.500335
[Train] epoch:113	batch id:90	 lr:0.075892 loss:1.553682
[Train] epoch:113	batch id:100	 lr:0.075892 loss:1.577825
[Train] epoch:113	batch id:110	 lr:0.075892 loss:1.613691
[Train] epoch:113	batch id:120	 lr:0.075892 loss:1.471911
[Train] epoch:113	batch id:130	 lr:0.075892 loss:1.658443
[Train] epoch:113	batch id:140	 lr:0.075892 loss:1.479466
[Train] epoch:113	batch id:150	 lr:0.075892 loss:1.534039
[Train] epoch:113	batch id:160	 lr:0.075892 loss:1.392364
[Train] epoch:113	batch id:170	 lr:0.075892 loss:1.558241
[Train] epoch:113	batch id:180	 lr:0.075892 loss:1.487399
[Train] epoch:113	batch id:190	 lr:0.075892 loss:1.654657
[Train] epoch:113	batch id:200	 lr:0.075892 loss:1.732435
[Train] epoch:113	batch id:210	 lr:0.075892 loss:1.559962
[Train] epoch:113	batch id:220	 lr:0.075892 loss:1.859687
[Train] epoch:113	batch id:230	 lr:0.075892 loss:1.484669
[Train] epoch:113	batch id:240	 lr:0.075892 loss:1.597902
[Train] epoch:113	batch id:250	 lr:0.075892 loss:1.559853
[Train] epoch:113	batch id:260	 lr:0.075892 loss:1.492654
[Train] epoch:113	batch id:270	 lr:0.075892 loss:1.698167
[Train] epoch:113	batch id:280	 lr:0.075892 loss:1.645252
[Train] epoch:113	batch id:290	 lr:0.075892 loss:1.572548
[Train] epoch:113	batch id:300	 lr:0.075892 loss:1.550237
[Train] 113, loss: 1.580196, train acc: 0.900550, 
[Test] epoch:113	batch id:0	 loss:1.435651
[Test] epoch:113	batch id:10	 loss:1.543381
[Test] epoch:113	batch id:20	 loss:1.426575
[Test] epoch:113	batch id:30	 loss:1.430717
[Test] epoch:113	batch id:40	 loss:1.390377
[Test] epoch:113	batch id:50	 loss:1.502120
[Test] epoch:113	batch id:60	 loss:1.302911
[Test] epoch:113	batch id:70	 loss:1.440494
[Test] epoch:113	batch id:80	 loss:1.614071
[Test] epoch:113	batch id:90	 loss:1.574403
[Test] epoch:113	batch id:100	 loss:1.740716
[Test] epoch:113	batch id:110	 loss:1.479624
[Test] epoch:113	batch id:120	 loss:1.396590
[Test] epoch:113	batch id:130	 loss:1.513911
[Test] epoch:113	batch id:140	 loss:1.391643
[Test] epoch:113	batch id:150	 loss:1.863996
[Test] 113, loss: 1.536623, test acc: 0.897083,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:114	batch id:0	 lr:0.075510 loss:1.620134
[Train] epoch:114	batch id:10	 lr:0.075510 loss:1.621428
[Train] epoch:114	batch id:20	 lr:0.075510 loss:1.615351
[Train] epoch:114	batch id:30	 lr:0.075510 loss:1.502324
[Train] epoch:114	batch id:40	 lr:0.075510 loss:1.566846
[Train] epoch:114	batch id:50	 lr:0.075510 loss:1.683735
[Train] epoch:114	batch id:60	 lr:0.075510 loss:1.593214
[Train] epoch:114	batch id:70	 lr:0.075510 loss:1.519840
[Train] epoch:114	batch id:80	 lr:0.075510 loss:1.573612
[Train] epoch:114	batch id:90	 lr:0.075510 loss:1.511164
[Train] epoch:114	batch id:100	 lr:0.075510 loss:1.517815
[Train] epoch:114	batch id:110	 lr:0.075510 loss:1.491215
[Train] epoch:114	batch id:120	 lr:0.075510 loss:1.680371
[Train] epoch:114	batch id:130	 lr:0.075510 loss:1.780287
[Train] epoch:114	batch id:140	 lr:0.075510 loss:1.487933
[Train] epoch:114	batch id:150	 lr:0.075510 loss:1.561272
[Train] epoch:114	batch id:160	 lr:0.075510 loss:1.473698
[Train] epoch:114	batch id:170	 lr:0.075510 loss:1.630591
[Train] epoch:114	batch id:180	 lr:0.075510 loss:1.644203
[Train] epoch:114	batch id:190	 lr:0.075510 loss:1.466929
[Train] epoch:114	batch id:200	 lr:0.075510 loss:1.610008
[Train] epoch:114	batch id:210	 lr:0.075510 loss:1.520401
[Train] epoch:114	batch id:220	 lr:0.075510 loss:1.649250
[Train] epoch:114	batch id:230	 lr:0.075510 loss:1.640053
[Train] epoch:114	batch id:240	 lr:0.075510 loss:1.513594
[Train] epoch:114	batch id:250	 lr:0.075510 loss:1.555721
[Train] epoch:114	batch id:260	 lr:0.075510 loss:1.694285
[Train] epoch:114	batch id:270	 lr:0.075510 loss:1.454548
[Train] epoch:114	batch id:280	 lr:0.075510 loss:1.588697
[Train] epoch:114	batch id:290	 lr:0.075510 loss:1.569375
[Train] epoch:114	batch id:300	 lr:0.075510 loss:1.668968
[Train] 114, loss: 1.579131, train acc: 0.902178, 
[Test] epoch:114	batch id:0	 loss:1.319610
[Test] epoch:114	batch id:10	 loss:1.385721
[Test] epoch:114	batch id:20	 loss:1.391391
[Test] epoch:114	batch id:30	 loss:1.441309
[Test] epoch:114	batch id:40	 loss:1.401460
[Test] epoch:114	batch id:50	 loss:1.464742
[Test] epoch:114	batch id:60	 loss:1.324505
[Test] epoch:114	batch id:70	 loss:1.497224
[Test] epoch:114	batch id:80	 loss:1.708087
[Test] epoch:114	batch id:90	 loss:1.468507
[Test] epoch:114	batch id:100	 loss:1.864632
[Test] epoch:114	batch id:110	 loss:1.399265
[Test] epoch:114	batch id:120	 loss:1.354585
[Test] epoch:114	batch id:130	 loss:1.450172
[Test] epoch:114	batch id:140	 loss:1.393832
[Test] epoch:114	batch id:150	 loss:1.742214
[Test] 114, loss: 1.496076, test acc: 0.910454,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:115	batch id:0	 lr:0.075126 loss:1.489939
[Train] epoch:115	batch id:10	 lr:0.075126 loss:1.522763
[Train] epoch:115	batch id:20	 lr:0.075126 loss:1.690853
[Train] epoch:115	batch id:30	 lr:0.075126 loss:1.469235
[Train] epoch:115	batch id:40	 lr:0.075126 loss:1.594543
[Train] epoch:115	batch id:50	 lr:0.075126 loss:1.610232
[Train] epoch:115	batch id:60	 lr:0.075126 loss:1.597149
[Train] epoch:115	batch id:70	 lr:0.075126 loss:1.550962
[Train] epoch:115	batch id:80	 lr:0.075126 loss:1.730279
[Train] epoch:115	batch id:90	 lr:0.075126 loss:1.545126
[Train] epoch:115	batch id:100	 lr:0.075126 loss:1.550429
[Train] epoch:115	batch id:110	 lr:0.075126 loss:1.530423
[Train] epoch:115	batch id:120	 lr:0.075126 loss:1.633533
[Train] epoch:115	batch id:130	 lr:0.075126 loss:1.544960
[Train] epoch:115	batch id:140	 lr:0.075126 loss:1.400360
[Train] epoch:115	batch id:150	 lr:0.075126 loss:1.502333
[Train] epoch:115	batch id:160	 lr:0.075126 loss:1.490023
[Train] epoch:115	batch id:170	 lr:0.075126 loss:1.439237
[Train] epoch:115	batch id:180	 lr:0.075126 loss:1.521194
[Train] epoch:115	batch id:190	 lr:0.075126 loss:1.554758
[Train] epoch:115	batch id:200	 lr:0.075126 loss:1.566150
[Train] epoch:115	batch id:210	 lr:0.075126 loss:1.434562
[Train] epoch:115	batch id:220	 lr:0.075126 loss:1.534178
[Train] epoch:115	batch id:230	 lr:0.075126 loss:1.627695
[Train] epoch:115	batch id:240	 lr:0.075126 loss:1.540433
[Train] epoch:115	batch id:250	 lr:0.075126 loss:1.646288
[Train] epoch:115	batch id:260	 lr:0.075126 loss:1.494910
[Train] epoch:115	batch id:270	 lr:0.075126 loss:1.579199
[Train] epoch:115	batch id:280	 lr:0.075126 loss:1.555292
[Train] epoch:115	batch id:290	 lr:0.075126 loss:1.550086
[Train] epoch:115	batch id:300	 lr:0.075126 loss:1.587258
[Train] 115, loss: 1.577765, train acc: 0.900550, 
[Test] epoch:115	batch id:0	 loss:1.325481
[Test] epoch:115	batch id:10	 loss:1.491171
[Test] epoch:115	batch id:20	 loss:1.393975
[Test] epoch:115	batch id:30	 loss:1.592172
[Test] epoch:115	batch id:40	 loss:1.469350
[Test] epoch:115	batch id:50	 loss:1.411236
[Test] epoch:115	batch id:60	 loss:1.274160
[Test] epoch:115	batch id:70	 loss:1.414264
[Test] epoch:115	batch id:80	 loss:1.697842
[Test] epoch:115	batch id:90	 loss:1.518372
[Test] epoch:115	batch id:100	 loss:2.065197
[Test] epoch:115	batch id:110	 loss:1.508815
[Test] epoch:115	batch id:120	 loss:1.414924
[Test] epoch:115	batch id:130	 loss:1.375723
[Test] epoch:115	batch id:140	 loss:1.536710
[Test] epoch:115	batch id:150	 loss:1.765565
[Test] 115, loss: 1.511523, test acc: 0.897893,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:116	batch id:0	 lr:0.074739 loss:1.549063
[Train] epoch:116	batch id:10	 lr:0.074739 loss:1.568056
[Train] epoch:116	batch id:20	 lr:0.074739 loss:1.895543
[Train] epoch:116	batch id:30	 lr:0.074739 loss:1.412599
[Train] epoch:116	batch id:40	 lr:0.074739 loss:1.572423
[Train] epoch:116	batch id:50	 lr:0.074739 loss:1.512317
[Train] epoch:116	batch id:60	 lr:0.074739 loss:1.502157
[Train] epoch:116	batch id:70	 lr:0.074739 loss:1.671828
[Train] epoch:116	batch id:80	 lr:0.074739 loss:1.517644
[Train] epoch:116	batch id:90	 lr:0.074739 loss:1.554600
[Train] epoch:116	batch id:100	 lr:0.074739 loss:1.438350
[Train] epoch:116	batch id:110	 lr:0.074739 loss:1.788180
[Train] epoch:116	batch id:120	 lr:0.074739 loss:1.634789
[Train] epoch:116	batch id:130	 lr:0.074739 loss:1.535597
[Train] epoch:116	batch id:140	 lr:0.074739 loss:1.528977
[Train] epoch:116	batch id:150	 lr:0.074739 loss:1.461167
[Train] epoch:116	batch id:160	 lr:0.074739 loss:1.440750
[Train] epoch:116	batch id:170	 lr:0.074739 loss:1.485712
[Train] epoch:116	batch id:180	 lr:0.074739 loss:1.561261
[Train] epoch:116	batch id:190	 lr:0.074739 loss:1.624267
[Train] epoch:116	batch id:200	 lr:0.074739 loss:1.570956
[Train] epoch:116	batch id:210	 lr:0.074739 loss:1.542834
[Train] epoch:116	batch id:220	 lr:0.074739 loss:1.651585
[Train] epoch:116	batch id:230	 lr:0.074739 loss:1.542863
[Train] epoch:116	batch id:240	 lr:0.074739 loss:1.539190
[Train] epoch:116	batch id:250	 lr:0.074739 loss:1.475645
[Train] epoch:116	batch id:260	 lr:0.074739 loss:1.470082
[Train] epoch:116	batch id:270	 lr:0.074739 loss:1.528729
[Train] epoch:116	batch id:280	 lr:0.074739 loss:1.568205
[Train] epoch:116	batch id:290	 lr:0.074739 loss:1.649949
[Train] epoch:116	batch id:300	 lr:0.074739 loss:1.601719
[Train] 116, loss: 1.577771, train acc: 0.901059, 
[Test] epoch:116	batch id:0	 loss:1.346780
[Test] epoch:116	batch id:10	 loss:1.512997
[Test] epoch:116	batch id:20	 loss:1.389007
[Test] epoch:116	batch id:30	 loss:1.419281
[Test] epoch:116	batch id:40	 loss:1.396616
[Test] epoch:116	batch id:50	 loss:1.490032
[Test] epoch:116	batch id:60	 loss:1.351653
[Test] epoch:116	batch id:70	 loss:1.422075
[Test] epoch:116	batch id:80	 loss:1.590762
[Test] epoch:116	batch id:90	 loss:1.612082
[Test] epoch:116	batch id:100	 loss:1.863485
[Test] epoch:116	batch id:110	 loss:1.483772
[Test] epoch:116	batch id:120	 loss:1.393135
[Test] epoch:116	batch id:130	 loss:1.439855
[Test] epoch:116	batch id:140	 loss:1.389027
[Test] epoch:116	batch id:150	 loss:1.869966
[Test] 116, loss: 1.529566, test acc: 0.908833,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:117	batch id:0	 lr:0.074351 loss:1.577730
[Train] epoch:117	batch id:10	 lr:0.074351 loss:1.741895
[Train] epoch:117	batch id:20	 lr:0.074351 loss:1.654818
[Train] epoch:117	batch id:30	 lr:0.074351 loss:1.538051
[Train] epoch:117	batch id:40	 lr:0.074351 loss:1.564185
[Train] epoch:117	batch id:50	 lr:0.074351 loss:1.519155
[Train] epoch:117	batch id:60	 lr:0.074351 loss:1.474685
[Train] epoch:117	batch id:70	 lr:0.074351 loss:1.717102
[Train] epoch:117	batch id:80	 lr:0.074351 loss:1.495224
[Train] epoch:117	batch id:90	 lr:0.074351 loss:1.439394
[Train] epoch:117	batch id:100	 lr:0.074351 loss:1.533898
[Train] epoch:117	batch id:110	 lr:0.074351 loss:1.608218
[Train] epoch:117	batch id:120	 lr:0.074351 loss:1.820362
[Train] epoch:117	batch id:130	 lr:0.074351 loss:1.574841
[Train] epoch:117	batch id:140	 lr:0.074351 loss:1.772713
[Train] epoch:117	batch id:150	 lr:0.074351 loss:1.579341
[Train] epoch:117	batch id:160	 lr:0.074351 loss:1.679958
[Train] epoch:117	batch id:170	 lr:0.074351 loss:1.535602
[Train] epoch:117	batch id:180	 lr:0.074351 loss:1.503421
[Train] epoch:117	batch id:190	 lr:0.074351 loss:1.484878
[Train] epoch:117	batch id:200	 lr:0.074351 loss:1.618814
[Train] epoch:117	batch id:210	 lr:0.074351 loss:1.549315
[Train] epoch:117	batch id:220	 lr:0.074351 loss:1.575522
[Train] epoch:117	batch id:230	 lr:0.074351 loss:1.627517
[Train] epoch:117	batch id:240	 lr:0.074351 loss:1.677593
[Train] epoch:117	batch id:250	 lr:0.074351 loss:1.582634
[Train] epoch:117	batch id:260	 lr:0.074351 loss:1.470120
[Train] epoch:117	batch id:270	 lr:0.074351 loss:1.622287
[Train] epoch:117	batch id:280	 lr:0.074351 loss:1.538304
[Train] epoch:117	batch id:290	 lr:0.074351 loss:1.622825
[Train] epoch:117	batch id:300	 lr:0.074351 loss:1.632135
[Train] 117, loss: 1.579011, train acc: 0.903705, 
[Test] epoch:117	batch id:0	 loss:1.351311
[Test] epoch:117	batch id:10	 loss:1.435766
[Test] epoch:117	batch id:20	 loss:1.478114
[Test] epoch:117	batch id:30	 loss:1.487656
[Test] epoch:117	batch id:40	 loss:1.425288
[Test] epoch:117	batch id:50	 loss:1.402184
[Test] epoch:117	batch id:60	 loss:1.287781
[Test] epoch:117	batch id:70	 loss:1.549353
[Test] epoch:117	batch id:80	 loss:1.665289
[Test] epoch:117	batch id:90	 loss:1.488058
[Test] epoch:117	batch id:100	 loss:1.715648
[Test] epoch:117	batch id:110	 loss:1.458835
[Test] epoch:117	batch id:120	 loss:1.378498
[Test] epoch:117	batch id:130	 loss:1.545688
[Test] epoch:117	batch id:140	 loss:1.413321
[Test] epoch:117	batch id:150	 loss:1.868347
[Test] 117, loss: 1.518930, test acc: 0.904781,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:118	batch id:0	 lr:0.073960 loss:1.586263
[Train] epoch:118	batch id:10	 lr:0.073960 loss:1.690137
[Train] epoch:118	batch id:20	 lr:0.073960 loss:1.543154
[Train] epoch:118	batch id:30	 lr:0.073960 loss:1.684773
[Train] epoch:118	batch id:40	 lr:0.073960 loss:1.599069
[Train] epoch:118	batch id:50	 lr:0.073960 loss:1.625714
[Train] epoch:118	batch id:60	 lr:0.073960 loss:1.554462
[Train] epoch:118	batch id:70	 lr:0.073960 loss:1.785093
[Train] epoch:118	batch id:80	 lr:0.073960 loss:1.569706
[Train] epoch:118	batch id:90	 lr:0.073960 loss:1.459704
[Train] epoch:118	batch id:100	 lr:0.073960 loss:1.771139
[Train] epoch:118	batch id:110	 lr:0.073960 loss:1.519484
[Train] epoch:118	batch id:120	 lr:0.073960 loss:1.627795
[Train] epoch:118	batch id:130	 lr:0.073960 loss:1.472260
[Train] epoch:118	batch id:140	 lr:0.073960 loss:1.506963
[Train] epoch:118	batch id:150	 lr:0.073960 loss:1.568581
[Train] epoch:118	batch id:160	 lr:0.073960 loss:1.712141
[Train] epoch:118	batch id:170	 lr:0.073960 loss:1.605757
[Train] epoch:118	batch id:180	 lr:0.073960 loss:1.489944
[Train] epoch:118	batch id:190	 lr:0.073960 loss:1.625890
[Train] epoch:118	batch id:200	 lr:0.073960 loss:1.696123
[Train] epoch:118	batch id:210	 lr:0.073960 loss:1.502103
[Train] epoch:118	batch id:220	 lr:0.073960 loss:1.562260
[Train] epoch:118	batch id:230	 lr:0.073960 loss:1.547447
[Train] epoch:118	batch id:240	 lr:0.073960 loss:1.649607
[Train] epoch:118	batch id:250	 lr:0.073960 loss:1.520617
[Train] epoch:118	batch id:260	 lr:0.073960 loss:1.487168
[Train] epoch:118	batch id:270	 lr:0.073960 loss:1.555331
[Train] epoch:118	batch id:280	 lr:0.073960 loss:1.592221
[Train] epoch:118	batch id:290	 lr:0.073960 loss:1.611622
[Train] epoch:118	batch id:300	 lr:0.073960 loss:1.592531
[Train] 118, loss: 1.580810, train acc: 0.902484, 
[Test] epoch:118	batch id:0	 loss:1.369283
[Test] epoch:118	batch id:10	 loss:1.371535
[Test] epoch:118	batch id:20	 loss:1.475356
[Test] epoch:118	batch id:30	 loss:1.437642
[Test] epoch:118	batch id:40	 loss:1.392787
[Test] epoch:118	batch id:50	 loss:1.483461
[Test] epoch:118	batch id:60	 loss:1.338324
[Test] epoch:118	batch id:70	 loss:1.526671
[Test] epoch:118	batch id:80	 loss:1.629033
[Test] epoch:118	batch id:90	 loss:1.550009
[Test] epoch:118	batch id:100	 loss:2.000534
[Test] epoch:118	batch id:110	 loss:1.462329
[Test] epoch:118	batch id:120	 loss:1.376787
[Test] epoch:118	batch id:130	 loss:1.454375
[Test] epoch:118	batch id:140	 loss:1.480897
[Test] epoch:118	batch id:150	 loss:1.781609
[Test] 118, loss: 1.514220, test acc: 0.909238,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:119	batch id:0	 lr:0.073568 loss:1.548527
[Train] epoch:119	batch id:10	 lr:0.073568 loss:1.577003
[Train] epoch:119	batch id:20	 lr:0.073568 loss:1.646478
[Train] epoch:119	batch id:30	 lr:0.073568 loss:1.538906
[Train] epoch:119	batch id:40	 lr:0.073568 loss:1.473657
[Train] epoch:119	batch id:50	 lr:0.073568 loss:1.529661
[Train] epoch:119	batch id:60	 lr:0.073568 loss:1.475673
[Train] epoch:119	batch id:70	 lr:0.073568 loss:1.701511
[Train] epoch:119	batch id:80	 lr:0.073568 loss:1.537604
[Train] epoch:119	batch id:90	 lr:0.073568 loss:1.426883
[Train] epoch:119	batch id:100	 lr:0.073568 loss:1.467459
[Train] epoch:119	batch id:110	 lr:0.073568 loss:1.731288
[Train] epoch:119	batch id:120	 lr:0.073568 loss:1.644949
[Train] epoch:119	batch id:130	 lr:0.073568 loss:1.534602
[Train] epoch:119	batch id:140	 lr:0.073568 loss:1.630199
[Train] epoch:119	batch id:150	 lr:0.073568 loss:1.393112
[Train] epoch:119	batch id:160	 lr:0.073568 loss:1.797630
[Train] epoch:119	batch id:170	 lr:0.073568 loss:1.580823
[Train] epoch:119	batch id:180	 lr:0.073568 loss:1.563458
[Train] epoch:119	batch id:190	 lr:0.073568 loss:1.486659
[Train] epoch:119	batch id:200	 lr:0.073568 loss:1.564584
[Train] epoch:119	batch id:210	 lr:0.073568 loss:1.494617
[Train] epoch:119	batch id:220	 lr:0.073568 loss:1.591429
[Train] epoch:119	batch id:230	 lr:0.073568 loss:1.542875
[Train] epoch:119	batch id:240	 lr:0.073568 loss:1.479124
[Train] epoch:119	batch id:250	 lr:0.073568 loss:1.699635
[Train] epoch:119	batch id:260	 lr:0.073568 loss:1.561622
[Train] epoch:119	batch id:270	 lr:0.073568 loss:1.547799
[Train] epoch:119	batch id:280	 lr:0.073568 loss:1.576647
[Train] epoch:119	batch id:290	 lr:0.073568 loss:1.662265
[Train] epoch:119	batch id:300	 lr:0.073568 loss:1.566911
[Train] 119, loss: 1.581470, train acc: 0.901466, 
[Test] epoch:119	batch id:0	 loss:1.352911
[Test] epoch:119	batch id:10	 loss:1.495826
[Test] epoch:119	batch id:20	 loss:1.494306
[Test] epoch:119	batch id:30	 loss:1.493815
[Test] epoch:119	batch id:40	 loss:1.525626
[Test] epoch:119	batch id:50	 loss:1.436546
[Test] epoch:119	batch id:60	 loss:1.323134
[Test] epoch:119	batch id:70	 loss:1.552127
[Test] epoch:119	batch id:80	 loss:1.779030
[Test] epoch:119	batch id:90	 loss:1.547911
[Test] epoch:119	batch id:100	 loss:1.878628
[Test] epoch:119	batch id:110	 loss:1.364692
[Test] epoch:119	batch id:120	 loss:1.372848
[Test] epoch:119	batch id:130	 loss:1.523302
[Test] epoch:119	batch id:140	 loss:1.442384
[Test] epoch:119	batch id:150	 loss:1.865888
[Test] 119, loss: 1.534454, test acc: 0.901540,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:120	batch id:0	 lr:0.073174 loss:1.596096
[Train] epoch:120	batch id:10	 lr:0.073174 loss:1.606138
[Train] epoch:120	batch id:20	 lr:0.073174 loss:1.585888
[Train] epoch:120	batch id:30	 lr:0.073174 loss:1.637564
[Train] epoch:120	batch id:40	 lr:0.073174 loss:1.522614
[Train] epoch:120	batch id:50	 lr:0.073174 loss:1.631201
[Train] epoch:120	batch id:60	 lr:0.073174 loss:1.442424
[Train] epoch:120	batch id:70	 lr:0.073174 loss:1.707427
[Train] epoch:120	batch id:80	 lr:0.073174 loss:1.568725
[Train] epoch:120	batch id:90	 lr:0.073174 loss:1.484456
[Train] epoch:120	batch id:100	 lr:0.073174 loss:1.528551
[Train] epoch:120	batch id:110	 lr:0.073174 loss:1.488276
[Train] epoch:120	batch id:120	 lr:0.073174 loss:1.573921
[Train] epoch:120	batch id:130	 lr:0.073174 loss:1.480219
[Train] epoch:120	batch id:140	 lr:0.073174 loss:1.454103
[Train] epoch:120	batch id:150	 lr:0.073174 loss:1.604923
[Train] epoch:120	batch id:160	 lr:0.073174 loss:1.607387
[Train] epoch:120	batch id:170	 lr:0.073174 loss:1.603440
[Train] epoch:120	batch id:180	 lr:0.073174 loss:1.510675
[Train] epoch:120	batch id:190	 lr:0.073174 loss:1.565641
[Train] epoch:120	batch id:200	 lr:0.073174 loss:1.526549
[Train] epoch:120	batch id:210	 lr:0.073174 loss:1.578825
[Train] epoch:120	batch id:220	 lr:0.073174 loss:1.588272
[Train] epoch:120	batch id:230	 lr:0.073174 loss:1.547420
[Train] epoch:120	batch id:240	 lr:0.073174 loss:1.516347
[Train] epoch:120	batch id:250	 lr:0.073174 loss:1.512307
[Train] epoch:120	batch id:260	 lr:0.073174 loss:1.719193
[Train] epoch:120	batch id:270	 lr:0.073174 loss:1.505526
[Train] epoch:120	batch id:280	 lr:0.073174 loss:1.513085
[Train] epoch:120	batch id:290	 lr:0.073174 loss:1.580717
[Train] epoch:120	batch id:300	 lr:0.073174 loss:1.490254
[Train] 120, loss: 1.575750, train acc: 0.902993, 
[Test] epoch:120	batch id:0	 loss:1.392099
[Test] epoch:120	batch id:10	 loss:1.488586
[Test] epoch:120	batch id:20	 loss:1.536022
[Test] epoch:120	batch id:30	 loss:1.563539
[Test] epoch:120	batch id:40	 loss:1.527575
[Test] epoch:120	batch id:50	 loss:1.465492
[Test] epoch:120	batch id:60	 loss:1.333406
[Test] epoch:120	batch id:70	 loss:1.550721
[Test] epoch:120	batch id:80	 loss:1.829737
[Test] epoch:120	batch id:90	 loss:1.544111
[Test] epoch:120	batch id:100	 loss:1.893350
[Test] epoch:120	batch id:110	 loss:1.416446
[Test] epoch:120	batch id:120	 loss:1.389912
[Test] epoch:120	batch id:130	 loss:1.528511
[Test] epoch:120	batch id:140	 loss:1.412020
[Test] epoch:120	batch id:150	 loss:1.900002
[Test] 120, loss: 1.545343, test acc: 0.899109,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:121	batch id:0	 lr:0.072778 loss:1.489671
[Train] epoch:121	batch id:10	 lr:0.072778 loss:1.519913
[Train] epoch:121	batch id:20	 lr:0.072778 loss:1.531825
[Train] epoch:121	batch id:30	 lr:0.072778 loss:1.686696
[Train] epoch:121	batch id:40	 lr:0.072778 loss:1.736044
[Train] epoch:121	batch id:50	 lr:0.072778 loss:1.487902
[Train] epoch:121	batch id:60	 lr:0.072778 loss:1.728450
[Train] epoch:121	batch id:70	 lr:0.072778 loss:1.586474
[Train] epoch:121	batch id:80	 lr:0.072778 loss:1.645696
[Train] epoch:121	batch id:90	 lr:0.072778 loss:1.530473
[Train] epoch:121	batch id:100	 lr:0.072778 loss:1.388935
[Train] epoch:121	batch id:110	 lr:0.072778 loss:1.457703
[Train] epoch:121	batch id:120	 lr:0.072778 loss:1.644250
[Train] epoch:121	batch id:130	 lr:0.072778 loss:1.523134
[Train] epoch:121	batch id:140	 lr:0.072778 loss:1.613766
[Train] epoch:121	batch id:150	 lr:0.072778 loss:1.520072
[Train] epoch:121	batch id:160	 lr:0.072778 loss:1.638035
[Train] epoch:121	batch id:170	 lr:0.072778 loss:1.452145
[Train] epoch:121	batch id:180	 lr:0.072778 loss:1.662482
[Train] epoch:121	batch id:190	 lr:0.072778 loss:1.472741
[Train] epoch:121	batch id:200	 lr:0.072778 loss:1.476881
[Train] epoch:121	batch id:210	 lr:0.072778 loss:1.771189
[Train] epoch:121	batch id:220	 lr:0.072778 loss:1.770227
[Train] epoch:121	batch id:230	 lr:0.072778 loss:1.558145
[Train] epoch:121	batch id:240	 lr:0.072778 loss:1.546464
[Train] epoch:121	batch id:250	 lr:0.072778 loss:1.650678
[Train] epoch:121	batch id:260	 lr:0.072778 loss:1.453182
[Train] epoch:121	batch id:270	 lr:0.072778 loss:1.542740
[Train] epoch:121	batch id:280	 lr:0.072778 loss:1.514527
[Train] epoch:121	batch id:290	 lr:0.072778 loss:1.759332
[Train] epoch:121	batch id:300	 lr:0.072778 loss:1.600677
[Train] 121, loss: 1.575278, train acc: 0.901262, 
[Test] epoch:121	batch id:0	 loss:1.516381
[Test] epoch:121	batch id:10	 loss:1.506076
[Test] epoch:121	batch id:20	 loss:1.526938
[Test] epoch:121	batch id:30	 loss:1.482781
[Test] epoch:121	batch id:40	 loss:1.481312
[Test] epoch:121	batch id:50	 loss:1.520856
[Test] epoch:121	batch id:60	 loss:1.343857
[Test] epoch:121	batch id:70	 loss:1.528224
[Test] epoch:121	batch id:80	 loss:1.687292
[Test] epoch:121	batch id:90	 loss:1.552803
[Test] epoch:121	batch id:100	 loss:1.817894
[Test] epoch:121	batch id:110	 loss:1.452625
[Test] epoch:121	batch id:120	 loss:1.414895
[Test] epoch:121	batch id:130	 loss:1.678691
[Test] epoch:121	batch id:140	 loss:1.474513
[Test] epoch:121	batch id:150	 loss:1.805863
[Test] 121, loss: 1.554994, test acc: 0.891005,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:122	batch id:0	 lr:0.072381 loss:1.494026
[Train] epoch:122	batch id:10	 lr:0.072381 loss:1.539368
[Train] epoch:122	batch id:20	 lr:0.072381 loss:1.577147
[Train] epoch:122	batch id:30	 lr:0.072381 loss:1.725620
[Train] epoch:122	batch id:40	 lr:0.072381 loss:1.664887
[Train] epoch:122	batch id:50	 lr:0.072381 loss:1.720474
[Train] epoch:122	batch id:60	 lr:0.072381 loss:1.484837
[Train] epoch:122	batch id:70	 lr:0.072381 loss:1.494096
[Train] epoch:122	batch id:80	 lr:0.072381 loss:1.499471
[Train] epoch:122	batch id:90	 lr:0.072381 loss:1.599566
[Train] epoch:122	batch id:100	 lr:0.072381 loss:1.738733
[Train] epoch:122	batch id:110	 lr:0.072381 loss:1.712694
[Train] epoch:122	batch id:120	 lr:0.072381 loss:1.577270
[Train] epoch:122	batch id:130	 lr:0.072381 loss:1.467224
[Train] epoch:122	batch id:140	 lr:0.072381 loss:1.578655
[Train] epoch:122	batch id:150	 lr:0.072381 loss:1.645027
[Train] epoch:122	batch id:160	 lr:0.072381 loss:1.526714
[Train] epoch:122	batch id:170	 lr:0.072381 loss:1.435564
[Train] epoch:122	batch id:180	 lr:0.072381 loss:1.577278
[Train] epoch:122	batch id:190	 lr:0.072381 loss:1.495191
[Train] epoch:122	batch id:200	 lr:0.072381 loss:1.527380
[Train] epoch:122	batch id:210	 lr:0.072381 loss:1.635257
[Train] epoch:122	batch id:220	 lr:0.072381 loss:1.647675
[Train] epoch:122	batch id:230	 lr:0.072381 loss:1.543050
[Train] epoch:122	batch id:240	 lr:0.072381 loss:1.426135
[Train] epoch:122	batch id:250	 lr:0.072381 loss:1.614204
[Train] epoch:122	batch id:260	 lr:0.072381 loss:1.518263
[Train] epoch:122	batch id:270	 lr:0.072381 loss:1.577882
[Train] epoch:122	batch id:280	 lr:0.072381 loss:1.758766
[Train] epoch:122	batch id:290	 lr:0.072381 loss:1.610667
[Train] epoch:122	batch id:300	 lr:0.072381 loss:1.601690
[Train] 122, loss: 1.576211, train acc: 0.902077, 
[Test] epoch:122	batch id:0	 loss:1.355528
[Test] epoch:122	batch id:10	 loss:1.450745
[Test] epoch:122	batch id:20	 loss:1.356029
[Test] epoch:122	batch id:30	 loss:1.479418
[Test] epoch:122	batch id:40	 loss:1.441261
[Test] epoch:122	batch id:50	 loss:1.494034
[Test] epoch:122	batch id:60	 loss:1.286426
[Test] epoch:122	batch id:70	 loss:1.430084
[Test] epoch:122	batch id:80	 loss:1.698164
[Test] epoch:122	batch id:90	 loss:1.595811
[Test] epoch:122	batch id:100	 loss:1.821495
[Test] epoch:122	batch id:110	 loss:1.483035
[Test] epoch:122	batch id:120	 loss:1.373258
[Test] epoch:122	batch id:130	 loss:1.486465
[Test] epoch:122	batch id:140	 loss:1.628859
[Test] epoch:122	batch id:150	 loss:1.716761
[Test] 122, loss: 1.524935, test acc: 0.905592,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:123	batch id:0	 lr:0.071981 loss:1.654636
[Train] epoch:123	batch id:10	 lr:0.071981 loss:1.495864
[Train] epoch:123	batch id:20	 lr:0.071981 loss:1.548609
[Train] epoch:123	batch id:30	 lr:0.071981 loss:1.529541
[Train] epoch:123	batch id:40	 lr:0.071981 loss:1.522798
[Train] epoch:123	batch id:50	 lr:0.071981 loss:1.605668
[Train] epoch:123	batch id:60	 lr:0.071981 loss:1.669201
[Train] epoch:123	batch id:70	 lr:0.071981 loss:1.498235
[Train] epoch:123	batch id:80	 lr:0.071981 loss:1.628535
[Train] epoch:123	batch id:90	 lr:0.071981 loss:1.500452
[Train] epoch:123	batch id:100	 lr:0.071981 loss:1.609217
[Train] epoch:123	batch id:110	 lr:0.071981 loss:1.446127
[Train] epoch:123	batch id:120	 lr:0.071981 loss:1.593381
[Train] epoch:123	batch id:130	 lr:0.071981 loss:1.530488
[Train] epoch:123	batch id:140	 lr:0.071981 loss:1.662019
[Train] epoch:123	batch id:150	 lr:0.071981 loss:1.562692
[Train] epoch:123	batch id:160	 lr:0.071981 loss:1.446221
[Train] epoch:123	batch id:170	 lr:0.071981 loss:1.552100
[Train] epoch:123	batch id:180	 lr:0.071981 loss:1.683553
[Train] epoch:123	batch id:190	 lr:0.071981 loss:1.567078
[Train] epoch:123	batch id:200	 lr:0.071981 loss:1.553034
[Train] epoch:123	batch id:210	 lr:0.071981 loss:1.649174
[Train] epoch:123	batch id:220	 lr:0.071981 loss:1.767554
[Train] epoch:123	batch id:230	 lr:0.071981 loss:1.607727
[Train] epoch:123	batch id:240	 lr:0.071981 loss:1.509951
[Train] epoch:123	batch id:250	 lr:0.071981 loss:1.531994
[Train] epoch:123	batch id:260	 lr:0.071981 loss:1.576197
[Train] epoch:123	batch id:270	 lr:0.071981 loss:1.458240
[Train] epoch:123	batch id:280	 lr:0.071981 loss:1.610213
[Train] epoch:123	batch id:290	 lr:0.071981 loss:1.592430
[Train] epoch:123	batch id:300	 lr:0.071981 loss:1.576990
[Train] 123, loss: 1.573754, train acc: 0.903298, 
[Test] epoch:123	batch id:0	 loss:1.367164
[Test] epoch:123	batch id:10	 loss:1.385979
[Test] epoch:123	batch id:20	 loss:1.389152
[Test] epoch:123	batch id:30	 loss:1.434705
[Test] epoch:123	batch id:40	 loss:1.471905
[Test] epoch:123	batch id:50	 loss:1.474972
[Test] epoch:123	batch id:60	 loss:1.321159
[Test] epoch:123	batch id:70	 loss:1.449567
[Test] epoch:123	batch id:80	 loss:1.592502
[Test] epoch:123	batch id:90	 loss:1.548116
[Test] epoch:123	batch id:100	 loss:1.909263
[Test] epoch:123	batch id:110	 loss:1.449749
[Test] epoch:123	batch id:120	 loss:1.424709
[Test] epoch:123	batch id:130	 loss:1.578353
[Test] epoch:123	batch id:140	 loss:1.355181
[Test] epoch:123	batch id:150	 loss:1.863763
[Test] 123, loss: 1.514592, test acc: 0.913695,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:124	batch id:0	 lr:0.071580 loss:1.591724
[Train] epoch:124	batch id:10	 lr:0.071580 loss:1.620384
[Train] epoch:124	batch id:20	 lr:0.071580 loss:1.567511
[Train] epoch:124	batch id:30	 lr:0.071580 loss:1.555520
[Train] epoch:124	batch id:40	 lr:0.071580 loss:1.618400
[Train] epoch:124	batch id:50	 lr:0.071580 loss:1.481834
[Train] epoch:124	batch id:60	 lr:0.071580 loss:1.563100
[Train] epoch:124	batch id:70	 lr:0.071580 loss:1.504381
[Train] epoch:124	batch id:80	 lr:0.071580 loss:1.622565
[Train] epoch:124	batch id:90	 lr:0.071580 loss:1.645596
[Train] epoch:124	batch id:100	 lr:0.071580 loss:1.568321
[Train] epoch:124	batch id:110	 lr:0.071580 loss:1.477744
[Train] epoch:124	batch id:120	 lr:0.071580 loss:1.725777
[Train] epoch:124	batch id:130	 lr:0.071580 loss:1.620329
[Train] epoch:124	batch id:140	 lr:0.071580 loss:1.559976
[Train] epoch:124	batch id:150	 lr:0.071580 loss:1.652390
[Train] epoch:124	batch id:160	 lr:0.071580 loss:1.651283
[Train] epoch:124	batch id:170	 lr:0.071580 loss:1.504735
[Train] epoch:124	batch id:180	 lr:0.071580 loss:1.615682
[Train] epoch:124	batch id:190	 lr:0.071580 loss:1.605524
[Train] epoch:124	batch id:200	 lr:0.071580 loss:1.676469
[Train] epoch:124	batch id:210	 lr:0.071580 loss:1.584319
[Train] epoch:124	batch id:220	 lr:0.071580 loss:1.593440
[Train] epoch:124	batch id:230	 lr:0.071580 loss:1.553765
[Train] epoch:124	batch id:240	 lr:0.071580 loss:1.573599
[Train] epoch:124	batch id:250	 lr:0.071580 loss:1.505937
[Train] epoch:124	batch id:260	 lr:0.071580 loss:1.628481
[Train] epoch:124	batch id:270	 lr:0.071580 loss:1.490610
[Train] epoch:124	batch id:280	 lr:0.071580 loss:1.458792
[Train] epoch:124	batch id:290	 lr:0.071580 loss:1.612272
[Train] epoch:124	batch id:300	 lr:0.071580 loss:1.465754
[Train] 124, loss: 1.567590, train acc: 0.908388, 
[Test] epoch:124	batch id:0	 loss:1.331487
[Test] epoch:124	batch id:10	 loss:1.401552
[Test] epoch:124	batch id:20	 loss:1.414472
[Test] epoch:124	batch id:30	 loss:1.409978
[Test] epoch:124	batch id:40	 loss:1.433990
[Test] epoch:124	batch id:50	 loss:1.496485
[Test] epoch:124	batch id:60	 loss:1.358503
[Test] epoch:124	batch id:70	 loss:1.480006
[Test] epoch:124	batch id:80	 loss:1.722564
[Test] epoch:124	batch id:90	 loss:1.555079
[Test] epoch:124	batch id:100	 loss:1.827424
[Test] epoch:124	batch id:110	 loss:1.420892
[Test] epoch:124	batch id:120	 loss:1.403403
[Test] epoch:124	batch id:130	 loss:1.446674
[Test] epoch:124	batch id:140	 loss:1.389005
[Test] epoch:124	batch id:150	 loss:1.844755
[Test] 124, loss: 1.523413, test acc: 0.899919,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:125	batch id:0	 lr:0.071177 loss:1.453521
[Train] epoch:125	batch id:10	 lr:0.071177 loss:1.540188
[Train] epoch:125	batch id:20	 lr:0.071177 loss:1.610122
[Train] epoch:125	batch id:30	 lr:0.071177 loss:1.536486
[Train] epoch:125	batch id:40	 lr:0.071177 loss:1.615752
[Train] epoch:125	batch id:50	 lr:0.071177 loss:1.547839
[Train] epoch:125	batch id:60	 lr:0.071177 loss:1.644338
[Train] epoch:125	batch id:70	 lr:0.071177 loss:1.497626
[Train] epoch:125	batch id:80	 lr:0.071177 loss:1.493681
[Train] epoch:125	batch id:90	 lr:0.071177 loss:1.648574
[Train] epoch:125	batch id:100	 lr:0.071177 loss:1.553584
[Train] epoch:125	batch id:110	 lr:0.071177 loss:1.488939
[Train] epoch:125	batch id:120	 lr:0.071177 loss:1.508629
[Train] epoch:125	batch id:130	 lr:0.071177 loss:1.688836
[Train] epoch:125	batch id:140	 lr:0.071177 loss:1.416819
[Train] epoch:125	batch id:150	 lr:0.071177 loss:1.517848
[Train] epoch:125	batch id:160	 lr:0.071177 loss:1.614807
[Train] epoch:125	batch id:170	 lr:0.071177 loss:1.531488
[Train] epoch:125	batch id:180	 lr:0.071177 loss:1.560178
[Train] epoch:125	batch id:190	 lr:0.071177 loss:1.709318
[Train] epoch:125	batch id:200	 lr:0.071177 loss:1.528217
[Train] epoch:125	batch id:210	 lr:0.071177 loss:1.527701
[Train] epoch:125	batch id:220	 lr:0.071177 loss:1.665926
[Train] epoch:125	batch id:230	 lr:0.071177 loss:1.503272
[Train] epoch:125	batch id:240	 lr:0.071177 loss:1.557646
[Train] epoch:125	batch id:250	 lr:0.071177 loss:1.618538
[Train] epoch:125	batch id:260	 lr:0.071177 loss:1.486881
[Train] epoch:125	batch id:270	 lr:0.071177 loss:1.435611
[Train] epoch:125	batch id:280	 lr:0.071177 loss:1.874495
[Train] epoch:125	batch id:290	 lr:0.071177 loss:1.569594
[Train] epoch:125	batch id:300	 lr:0.071177 loss:1.630614
[Train] 125, loss: 1.563848, train acc: 0.907166, 
[Test] epoch:125	batch id:0	 loss:1.332075
[Test] epoch:125	batch id:10	 loss:1.453757
[Test] epoch:125	batch id:20	 loss:1.460337
[Test] epoch:125	batch id:30	 loss:1.572382
[Test] epoch:125	batch id:40	 loss:1.401265
[Test] epoch:125	batch id:50	 loss:1.527006
[Test] epoch:125	batch id:60	 loss:1.281157
[Test] epoch:125	batch id:70	 loss:1.422215
[Test] epoch:125	batch id:80	 loss:1.745338
[Test] epoch:125	batch id:90	 loss:1.525889
[Test] epoch:125	batch id:100	 loss:1.941908
[Test] epoch:125	batch id:110	 loss:1.464016
[Test] epoch:125	batch id:120	 loss:1.419996
[Test] epoch:125	batch id:130	 loss:1.393921
[Test] epoch:125	batch id:140	 loss:1.380186
[Test] epoch:125	batch id:150	 loss:1.575300
[Test] 125, loss: 1.503254, test acc: 0.908428,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:126	batch id:0	 lr:0.070773 loss:1.554439
[Train] epoch:126	batch id:10	 lr:0.070773 loss:1.490374
[Train] epoch:126	batch id:20	 lr:0.070773 loss:1.513808
[Train] epoch:126	batch id:30	 lr:0.070773 loss:1.553190
[Train] epoch:126	batch id:40	 lr:0.070773 loss:1.506740
[Train] epoch:126	batch id:50	 lr:0.070773 loss:1.425561
[Train] epoch:126	batch id:60	 lr:0.070773 loss:1.629930
[Train] epoch:126	batch id:70	 lr:0.070773 loss:1.781125
[Train] epoch:126	batch id:80	 lr:0.070773 loss:1.536841
[Train] epoch:126	batch id:90	 lr:0.070773 loss:1.568746
[Train] epoch:126	batch id:100	 lr:0.070773 loss:1.631421
[Train] epoch:126	batch id:110	 lr:0.070773 loss:1.519779
[Train] epoch:126	batch id:120	 lr:0.070773 loss:1.592827
[Train] epoch:126	batch id:130	 lr:0.070773 loss:1.764448
[Train] epoch:126	batch id:140	 lr:0.070773 loss:1.679829
[Train] epoch:126	batch id:150	 lr:0.070773 loss:1.496211
[Train] epoch:126	batch id:160	 lr:0.070773 loss:1.593761
[Train] epoch:126	batch id:170	 lr:0.070773 loss:1.520792
[Train] epoch:126	batch id:180	 lr:0.070773 loss:1.530473
[Train] epoch:126	batch id:190	 lr:0.070773 loss:1.471962
[Train] epoch:126	batch id:200	 lr:0.070773 loss:1.641073
[Train] epoch:126	batch id:210	 lr:0.070773 loss:1.523133
[Train] epoch:126	batch id:220	 lr:0.070773 loss:1.549195
[Train] epoch:126	batch id:230	 lr:0.070773 loss:1.453188
[Train] epoch:126	batch id:240	 lr:0.070773 loss:1.413311
[Train] epoch:126	batch id:250	 lr:0.070773 loss:1.762382
[Train] epoch:126	batch id:260	 lr:0.070773 loss:1.522027
[Train] epoch:126	batch id:270	 lr:0.070773 loss:1.697737
[Train] epoch:126	batch id:280	 lr:0.070773 loss:1.611730
[Train] epoch:126	batch id:290	 lr:0.070773 loss:1.433436
[Train] epoch:126	batch id:300	 lr:0.070773 loss:1.547260
[Train] 126, loss: 1.567786, train acc: 0.905537, 
[Test] epoch:126	batch id:0	 loss:1.365197
[Test] epoch:126	batch id:10	 loss:1.487013
[Test] epoch:126	batch id:20	 loss:1.580041
[Test] epoch:126	batch id:30	 loss:1.385690
[Test] epoch:126	batch id:40	 loss:1.432502
[Test] epoch:126	batch id:50	 loss:1.414054
[Test] epoch:126	batch id:60	 loss:1.301799
[Test] epoch:126	batch id:70	 loss:1.478419
[Test] epoch:126	batch id:80	 loss:1.632231
[Test] epoch:126	batch id:90	 loss:1.443228
[Test] epoch:126	batch id:100	 loss:1.944840
[Test] epoch:126	batch id:110	 loss:1.366728
[Test] epoch:126	batch id:120	 loss:1.400976
[Test] epoch:126	batch id:130	 loss:1.498513
[Test] epoch:126	batch id:140	 loss:1.465526
[Test] epoch:126	batch id:150	 loss:1.851866
[Test] 126, loss: 1.516873, test acc: 0.905592,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:127	batch id:0	 lr:0.070366 loss:1.521126
[Train] epoch:127	batch id:10	 lr:0.070366 loss:1.542009
[Train] epoch:127	batch id:20	 lr:0.070366 loss:1.464929
[Train] epoch:127	batch id:30	 lr:0.070366 loss:1.468783
[Train] epoch:127	batch id:40	 lr:0.070366 loss:1.560391
[Train] epoch:127	batch id:50	 lr:0.070366 loss:1.616078
[Train] epoch:127	batch id:60	 lr:0.070366 loss:1.625019
[Train] epoch:127	batch id:70	 lr:0.070366 loss:1.546782
[Train] epoch:127	batch id:80	 lr:0.070366 loss:1.543303
[Train] epoch:127	batch id:90	 lr:0.070366 loss:1.458080
[Train] epoch:127	batch id:100	 lr:0.070366 loss:1.524889
[Train] epoch:127	batch id:110	 lr:0.070366 loss:1.577947
[Train] epoch:127	batch id:120	 lr:0.070366 loss:1.789773
[Train] epoch:127	batch id:130	 lr:0.070366 loss:1.409130
[Train] epoch:127	batch id:140	 lr:0.070366 loss:1.525027
[Train] epoch:127	batch id:150	 lr:0.070366 loss:1.719523
[Train] epoch:127	batch id:160	 lr:0.070366 loss:1.600329
[Train] epoch:127	batch id:170	 lr:0.070366 loss:1.659763
[Train] epoch:127	batch id:180	 lr:0.070366 loss:1.598515
[Train] epoch:127	batch id:190	 lr:0.070366 loss:1.542148
[Train] epoch:127	batch id:200	 lr:0.070366 loss:1.482198
[Train] epoch:127	batch id:210	 lr:0.070366 loss:1.674839
[Train] epoch:127	batch id:220	 lr:0.070366 loss:1.621512
[Train] epoch:127	batch id:230	 lr:0.070366 loss:1.491106
[Train] epoch:127	batch id:240	 lr:0.070366 loss:1.583919
[Train] epoch:127	batch id:250	 lr:0.070366 loss:1.547685
[Train] epoch:127	batch id:260	 lr:0.070366 loss:1.672726
[Train] epoch:127	batch id:270	 lr:0.070366 loss:1.660256
[Train] epoch:127	batch id:280	 lr:0.070366 loss:1.601760
[Train] epoch:127	batch id:290	 lr:0.070366 loss:1.563419
[Train] epoch:127	batch id:300	 lr:0.070366 loss:1.635801
[Train] 127, loss: 1.571567, train acc: 0.901466, 
[Test] epoch:127	batch id:0	 loss:1.392096
[Test] epoch:127	batch id:10	 loss:1.502514
[Test] epoch:127	batch id:20	 loss:1.513235
[Test] epoch:127	batch id:30	 loss:1.514911
[Test] epoch:127	batch id:40	 loss:1.458006
[Test] epoch:127	batch id:50	 loss:1.419484
[Test] epoch:127	batch id:60	 loss:1.326512
[Test] epoch:127	batch id:70	 loss:1.424021
[Test] epoch:127	batch id:80	 loss:1.610336
[Test] epoch:127	batch id:90	 loss:1.556027
[Test] epoch:127	batch id:100	 loss:1.901903
[Test] epoch:127	batch id:110	 loss:1.399459
[Test] epoch:127	batch id:120	 loss:1.498517
[Test] epoch:127	batch id:130	 loss:1.421090
[Test] epoch:127	batch id:140	 loss:1.383124
[Test] epoch:127	batch id:150	 loss:1.914459
[Test] 127, loss: 1.536879, test acc: 0.901540,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:128	batch id:0	 lr:0.069959 loss:1.604832
[Train] epoch:128	batch id:10	 lr:0.069959 loss:1.612634
[Train] epoch:128	batch id:20	 lr:0.069959 loss:1.501712
[Train] epoch:128	batch id:30	 lr:0.069959 loss:1.512865
[Train] epoch:128	batch id:40	 lr:0.069959 loss:1.503154
[Train] epoch:128	batch id:50	 lr:0.069959 loss:1.613976
[Train] epoch:128	batch id:60	 lr:0.069959 loss:1.530138
[Train] epoch:128	batch id:70	 lr:0.069959 loss:1.636298
[Train] epoch:128	batch id:80	 lr:0.069959 loss:1.464917
[Train] epoch:128	batch id:90	 lr:0.069959 loss:1.512036
[Train] epoch:128	batch id:100	 lr:0.069959 loss:1.522786
[Train] epoch:128	batch id:110	 lr:0.069959 loss:1.510162
[Train] epoch:128	batch id:120	 lr:0.069959 loss:1.490353
[Train] epoch:128	batch id:130	 lr:0.069959 loss:1.640034
[Train] epoch:128	batch id:140	 lr:0.069959 loss:1.517944
[Train] epoch:128	batch id:150	 lr:0.069959 loss:1.600268
[Train] epoch:128	batch id:160	 lr:0.069959 loss:1.552653
[Train] epoch:128	batch id:170	 lr:0.069959 loss:1.828092
[Train] epoch:128	batch id:180	 lr:0.069959 loss:1.542637
[Train] epoch:128	batch id:190	 lr:0.069959 loss:1.568819
[Train] epoch:128	batch id:200	 lr:0.069959 loss:1.656142
[Train] epoch:128	batch id:210	 lr:0.069959 loss:1.480743
[Train] epoch:128	batch id:220	 lr:0.069959 loss:1.606640
[Train] epoch:128	batch id:230	 lr:0.069959 loss:1.436962
[Train] epoch:128	batch id:240	 lr:0.069959 loss:1.530272
[Train] epoch:128	batch id:250	 lr:0.069959 loss:1.570958
[Train] epoch:128	batch id:260	 lr:0.069959 loss:1.506495
[Train] epoch:128	batch id:270	 lr:0.069959 loss:1.593269
[Train] epoch:128	batch id:280	 lr:0.069959 loss:1.497254
[Train] epoch:128	batch id:290	 lr:0.069959 loss:1.545139
[Train] epoch:128	batch id:300	 lr:0.069959 loss:1.376199
[Train] 128, loss: 1.555350, train acc: 0.912968, 
[Test] epoch:128	batch id:0	 loss:1.404855
[Test] epoch:128	batch id:10	 loss:1.578660
[Test] epoch:128	batch id:20	 loss:1.499598
[Test] epoch:128	batch id:30	 loss:1.438169
[Test] epoch:128	batch id:40	 loss:1.409301
[Test] epoch:128	batch id:50	 loss:1.510686
[Test] epoch:128	batch id:60	 loss:1.287924
[Test] epoch:128	batch id:70	 loss:1.400375
[Test] epoch:128	batch id:80	 loss:1.726006
[Test] epoch:128	batch id:90	 loss:1.658930
[Test] epoch:128	batch id:100	 loss:1.889054
[Test] epoch:128	batch id:110	 loss:1.425807
[Test] epoch:128	batch id:120	 loss:1.451078
[Test] epoch:128	batch id:130	 loss:1.526139
[Test] epoch:128	batch id:140	 loss:1.472529
[Test] epoch:128	batch id:150	 loss:1.815596
[Test] 128, loss: 1.537469, test acc: 0.893031,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:129	batch id:0	 lr:0.069549 loss:1.529304
[Train] epoch:129	batch id:10	 lr:0.069549 loss:1.688188
[Train] epoch:129	batch id:20	 lr:0.069549 loss:1.499460
[Train] epoch:129	batch id:30	 lr:0.069549 loss:1.527179
[Train] epoch:129	batch id:40	 lr:0.069549 loss:1.525659
[Train] epoch:129	batch id:50	 lr:0.069549 loss:1.379653
[Train] epoch:129	batch id:60	 lr:0.069549 loss:1.574728
[Train] epoch:129	batch id:70	 lr:0.069549 loss:1.530767
[Train] epoch:129	batch id:80	 lr:0.069549 loss:1.627122
[Train] epoch:129	batch id:90	 lr:0.069549 loss:1.469342
[Train] epoch:129	batch id:100	 lr:0.069549 loss:1.677097
[Train] epoch:129	batch id:110	 lr:0.069549 loss:1.486660
[Train] epoch:129	batch id:120	 lr:0.069549 loss:1.618340
[Train] epoch:129	batch id:130	 lr:0.069549 loss:1.597662
[Train] epoch:129	batch id:140	 lr:0.069549 loss:1.556826
[Train] epoch:129	batch id:150	 lr:0.069549 loss:1.665641
[Train] epoch:129	batch id:160	 lr:0.069549 loss:1.537556
[Train] epoch:129	batch id:170	 lr:0.069549 loss:1.644117
[Train] epoch:129	batch id:180	 lr:0.069549 loss:1.565830
[Train] epoch:129	batch id:190	 lr:0.069549 loss:1.580977
[Train] epoch:129	batch id:200	 lr:0.069549 loss:1.636580
[Train] epoch:129	batch id:210	 lr:0.069549 loss:1.509865
[Train] epoch:129	batch id:220	 lr:0.069549 loss:1.568381
[Train] epoch:129	batch id:230	 lr:0.069549 loss:1.524537
[Train] epoch:129	batch id:240	 lr:0.069549 loss:1.420798
[Train] epoch:129	batch id:250	 lr:0.069549 loss:1.612996
[Train] epoch:129	batch id:260	 lr:0.069549 loss:1.653032
[Train] epoch:129	batch id:270	 lr:0.069549 loss:1.697292
[Train] epoch:129	batch id:280	 lr:0.069549 loss:1.494676
[Train] epoch:129	batch id:290	 lr:0.069549 loss:1.635024
[Train] epoch:129	batch id:300	 lr:0.069549 loss:1.521594
[Train] 129, loss: 1.560929, train acc: 0.909202, 
[Test] epoch:129	batch id:0	 loss:1.363343
[Test] epoch:129	batch id:10	 loss:1.538181
[Test] epoch:129	batch id:20	 loss:1.484046
[Test] epoch:129	batch id:30	 loss:1.470949
[Test] epoch:129	batch id:40	 loss:1.421257
[Test] epoch:129	batch id:50	 loss:1.626488
[Test] epoch:129	batch id:60	 loss:1.309162
[Test] epoch:129	batch id:70	 loss:1.435919
[Test] epoch:129	batch id:80	 loss:1.747194
[Test] epoch:129	batch id:90	 loss:1.618339
[Test] epoch:129	batch id:100	 loss:1.749736
[Test] epoch:129	batch id:110	 loss:1.458697
[Test] epoch:129	batch id:120	 loss:1.423686
[Test] epoch:129	batch id:130	 loss:1.425091
[Test] epoch:129	batch id:140	 loss:1.383459
[Test] epoch:129	batch id:150	 loss:1.911420
[Test] 129, loss: 1.551026, test acc: 0.894652,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:130	batch id:0	 lr:0.069139 loss:1.610479
[Train] epoch:130	batch id:10	 lr:0.069139 loss:1.442881
[Train] epoch:130	batch id:20	 lr:0.069139 loss:1.547669
[Train] epoch:130	batch id:30	 lr:0.069139 loss:1.588689
[Train] epoch:130	batch id:40	 lr:0.069139 loss:1.510068
[Train] epoch:130	batch id:50	 lr:0.069139 loss:1.626503
[Train] epoch:130	batch id:60	 lr:0.069139 loss:1.465059
[Train] epoch:130	batch id:70	 lr:0.069139 loss:1.719937
[Train] epoch:130	batch id:80	 lr:0.069139 loss:1.674242
[Train] epoch:130	batch id:90	 lr:0.069139 loss:1.546140
[Train] epoch:130	batch id:100	 lr:0.069139 loss:1.520011
[Train] epoch:130	batch id:110	 lr:0.069139 loss:1.512574
[Train] epoch:130	batch id:120	 lr:0.069139 loss:1.583589
[Train] epoch:130	batch id:130	 lr:0.069139 loss:1.517664
[Train] epoch:130	batch id:140	 lr:0.069139 loss:1.517000
[Train] epoch:130	batch id:150	 lr:0.069139 loss:1.573151
[Train] epoch:130	batch id:160	 lr:0.069139 loss:1.512239
[Train] epoch:130	batch id:170	 lr:0.069139 loss:1.563422
[Train] epoch:130	batch id:180	 lr:0.069139 loss:1.518921
[Train] epoch:130	batch id:190	 lr:0.069139 loss:1.655721
[Train] epoch:130	batch id:200	 lr:0.069139 loss:1.619640
[Train] epoch:130	batch id:210	 lr:0.069139 loss:1.476268
[Train] epoch:130	batch id:220	 lr:0.069139 loss:1.481203
[Train] epoch:130	batch id:230	 lr:0.069139 loss:1.511232
[Train] epoch:130	batch id:240	 lr:0.069139 loss:1.437504
[Train] epoch:130	batch id:250	 lr:0.069139 loss:1.551970
[Train] epoch:130	batch id:260	 lr:0.069139 loss:1.394455
[Train] epoch:130	batch id:270	 lr:0.069139 loss:1.526091
[Train] epoch:130	batch id:280	 lr:0.069139 loss:1.573901
[Train] epoch:130	batch id:290	 lr:0.069139 loss:1.617057
[Train] epoch:130	batch id:300	 lr:0.069139 loss:1.506104
[Train] 130, loss: 1.558870, train acc: 0.909406, 
[Test] epoch:130	batch id:0	 loss:1.387782
[Test] epoch:130	batch id:10	 loss:1.484578
[Test] epoch:130	batch id:20	 loss:1.540738
[Test] epoch:130	batch id:30	 loss:1.389261
[Test] epoch:130	batch id:40	 loss:1.447534
[Test] epoch:130	batch id:50	 loss:1.408274
[Test] epoch:130	batch id:60	 loss:1.305774
[Test] epoch:130	batch id:70	 loss:1.523194
[Test] epoch:130	batch id:80	 loss:1.657055
[Test] epoch:130	batch id:90	 loss:1.592130
[Test] epoch:130	batch id:100	 loss:1.920334
[Test] epoch:130	batch id:110	 loss:1.396758
[Test] epoch:130	batch id:120	 loss:1.451580
[Test] epoch:130	batch id:130	 loss:1.466677
[Test] epoch:130	batch id:140	 loss:1.585147
[Test] epoch:130	batch id:150	 loss:1.757329
[Test] 130, loss: 1.531615, test acc: 0.893031,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:131	batch id:0	 lr:0.068726 loss:1.544846
[Train] epoch:131	batch id:10	 lr:0.068726 loss:1.486043
[Train] epoch:131	batch id:20	 lr:0.068726 loss:1.571999
[Train] epoch:131	batch id:30	 lr:0.068726 loss:1.515742
[Train] epoch:131	batch id:40	 lr:0.068726 loss:1.646944
[Train] epoch:131	batch id:50	 lr:0.068726 loss:1.564599
[Train] epoch:131	batch id:60	 lr:0.068726 loss:1.476907
[Train] epoch:131	batch id:70	 lr:0.068726 loss:1.695644
[Train] epoch:131	batch id:80	 lr:0.068726 loss:1.579147
[Train] epoch:131	batch id:90	 lr:0.068726 loss:1.495364
[Train] epoch:131	batch id:100	 lr:0.068726 loss:1.714467
[Train] epoch:131	batch id:110	 lr:0.068726 loss:1.491882
[Train] epoch:131	batch id:120	 lr:0.068726 loss:1.701588
[Train] epoch:131	batch id:130	 lr:0.068726 loss:1.498445
[Train] epoch:131	batch id:140	 lr:0.068726 loss:1.495244
[Train] epoch:131	batch id:150	 lr:0.068726 loss:1.510139
[Train] epoch:131	batch id:160	 lr:0.068726 loss:1.636204
[Train] epoch:131	batch id:170	 lr:0.068726 loss:1.454782
[Train] epoch:131	batch id:180	 lr:0.068726 loss:1.520613
[Train] epoch:131	batch id:190	 lr:0.068726 loss:1.471344
[Train] epoch:131	batch id:200	 lr:0.068726 loss:1.641926
[Train] epoch:131	batch id:210	 lr:0.068726 loss:1.460561
[Train] epoch:131	batch id:220	 lr:0.068726 loss:1.575272
[Train] epoch:131	batch id:230	 lr:0.068726 loss:1.426402
[Train] epoch:131	batch id:240	 lr:0.068726 loss:1.582557
[Train] epoch:131	batch id:250	 lr:0.068726 loss:1.636470
[Train] epoch:131	batch id:260	 lr:0.068726 loss:1.556129
[Train] epoch:131	batch id:270	 lr:0.068726 loss:1.501662
[Train] epoch:131	batch id:280	 lr:0.068726 loss:1.476811
[Train] epoch:131	batch id:290	 lr:0.068726 loss:1.510162
[Train] epoch:131	batch id:300	 lr:0.068726 loss:1.518924
[Train] 131, loss: 1.566951, train acc: 0.907166, 
[Test] epoch:131	batch id:0	 loss:1.432894
[Test] epoch:131	batch id:10	 loss:1.673544
[Test] epoch:131	batch id:20	 loss:1.497798
[Test] epoch:131	batch id:30	 loss:1.469831
[Test] epoch:131	batch id:40	 loss:1.488031
[Test] epoch:131	batch id:50	 loss:1.453096
[Test] epoch:131	batch id:60	 loss:1.301673
[Test] epoch:131	batch id:70	 loss:1.422077
[Test] epoch:131	batch id:80	 loss:1.559669
[Test] epoch:131	batch id:90	 loss:1.473258
[Test] epoch:131	batch id:100	 loss:1.948001
[Test] epoch:131	batch id:110	 loss:1.417546
[Test] epoch:131	batch id:120	 loss:1.348589
[Test] epoch:131	batch id:130	 loss:1.555370
[Test] epoch:131	batch id:140	 loss:1.414975
[Test] epoch:131	batch id:150	 loss:1.814708
[Test] 131, loss: 1.530572, test acc: 0.885332,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:132	batch id:0	 lr:0.068312 loss:1.514199
[Train] epoch:132	batch id:10	 lr:0.068312 loss:1.421855
[Train] epoch:132	batch id:20	 lr:0.068312 loss:1.441455
[Train] epoch:132	batch id:30	 lr:0.068312 loss:1.559452
[Train] epoch:132	batch id:40	 lr:0.068312 loss:1.614083
[Train] epoch:132	batch id:50	 lr:0.068312 loss:1.628463
[Train] epoch:132	batch id:60	 lr:0.068312 loss:1.453277
[Train] epoch:132	batch id:70	 lr:0.068312 loss:1.637112
[Train] epoch:132	batch id:80	 lr:0.068312 loss:1.540470
[Train] epoch:132	batch id:90	 lr:0.068312 loss:1.623927
[Train] epoch:132	batch id:100	 lr:0.068312 loss:1.531259
[Train] epoch:132	batch id:110	 lr:0.068312 loss:1.488900
[Train] epoch:132	batch id:120	 lr:0.068312 loss:1.573031
[Train] epoch:132	batch id:130	 lr:0.068312 loss:1.746707
[Train] epoch:132	batch id:140	 lr:0.068312 loss:1.525272
[Train] epoch:132	batch id:150	 lr:0.068312 loss:1.779735
[Train] epoch:132	batch id:160	 lr:0.068312 loss:1.629653
[Train] epoch:132	batch id:170	 lr:0.068312 loss:1.571117
[Train] epoch:132	batch id:180	 lr:0.068312 loss:1.553456
[Train] epoch:132	batch id:190	 lr:0.068312 loss:1.642451
[Train] epoch:132	batch id:200	 lr:0.068312 loss:1.662506
[Train] epoch:132	batch id:210	 lr:0.068312 loss:1.567194
[Train] epoch:132	batch id:220	 lr:0.068312 loss:1.534870
[Train] epoch:132	batch id:230	 lr:0.068312 loss:1.607924
[Train] epoch:132	batch id:240	 lr:0.068312 loss:1.655630
[Train] epoch:132	batch id:250	 lr:0.068312 loss:1.534716
[Train] epoch:132	batch id:260	 lr:0.068312 loss:1.481055
[Train] epoch:132	batch id:270	 lr:0.068312 loss:1.450681
[Train] epoch:132	batch id:280	 lr:0.068312 loss:1.644706
[Train] epoch:132	batch id:290	 lr:0.068312 loss:1.716691
[Train] epoch:132	batch id:300	 lr:0.068312 loss:1.583052
[Train] 132, loss: 1.562215, train acc: 0.909711, 
[Test] epoch:132	batch id:0	 loss:1.350390
[Test] epoch:132	batch id:10	 loss:1.449862
[Test] epoch:132	batch id:20	 loss:1.460456
[Test] epoch:132	batch id:30	 loss:1.438797
[Test] epoch:132	batch id:40	 loss:1.455094
[Test] epoch:132	batch id:50	 loss:1.511459
[Test] epoch:132	batch id:60	 loss:1.318704
[Test] epoch:132	batch id:70	 loss:1.468194
[Test] epoch:132	batch id:80	 loss:1.656016
[Test] epoch:132	batch id:90	 loss:1.561101
[Test] epoch:132	batch id:100	 loss:2.085442
[Test] epoch:132	batch id:110	 loss:1.399130
[Test] epoch:132	batch id:120	 loss:1.399904
[Test] epoch:132	batch id:130	 loss:1.423503
[Test] epoch:132	batch id:140	 loss:1.365004
[Test] epoch:132	batch id:150	 loss:1.738619
[Test] 132, loss: 1.513490, test acc: 0.908833,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:133	batch id:0	 lr:0.067897 loss:1.516147
[Train] epoch:133	batch id:10	 lr:0.067897 loss:1.433857
[Train] epoch:133	batch id:20	 lr:0.067897 loss:1.456000
[Train] epoch:133	batch id:30	 lr:0.067897 loss:1.557086
[Train] epoch:133	batch id:40	 lr:0.067897 loss:1.543424
[Train] epoch:133	batch id:50	 lr:0.067897 loss:1.705400
[Train] epoch:133	batch id:60	 lr:0.067897 loss:1.463650
[Train] epoch:133	batch id:70	 lr:0.067897 loss:1.534374
[Train] epoch:133	batch id:80	 lr:0.067897 loss:1.527605
[Train] epoch:133	batch id:90	 lr:0.067897 loss:1.605008
[Train] epoch:133	batch id:100	 lr:0.067897 loss:1.464248
[Train] epoch:133	batch id:110	 lr:0.067897 loss:1.698090
[Train] epoch:133	batch id:120	 lr:0.067897 loss:1.626881
[Train] epoch:133	batch id:130	 lr:0.067897 loss:1.535884
[Train] epoch:133	batch id:140	 lr:0.067897 loss:1.545118
[Train] epoch:133	batch id:150	 lr:0.067897 loss:1.438241
[Train] epoch:133	batch id:160	 lr:0.067897 loss:1.619250
[Train] epoch:133	batch id:170	 lr:0.067897 loss:1.532951
[Train] epoch:133	batch id:180	 lr:0.067897 loss:1.498734
[Train] epoch:133	batch id:190	 lr:0.067897 loss:1.519967
[Train] epoch:133	batch id:200	 lr:0.067897 loss:1.547619
[Train] epoch:133	batch id:210	 lr:0.067897 loss:1.428343
[Train] epoch:133	batch id:220	 lr:0.067897 loss:1.528579
[Train] epoch:133	batch id:230	 lr:0.067897 loss:1.588939
[Train] epoch:133	batch id:240	 lr:0.067897 loss:1.618251
[Train] epoch:133	batch id:250	 lr:0.067897 loss:1.438423
[Train] epoch:133	batch id:260	 lr:0.067897 loss:1.398294
[Train] epoch:133	batch id:270	 lr:0.067897 loss:1.572488
[Train] epoch:133	batch id:280	 lr:0.067897 loss:1.550970
[Train] epoch:133	batch id:290	 lr:0.067897 loss:1.585877
[Train] epoch:133	batch id:300	 lr:0.067897 loss:1.506259
[Train] 133, loss: 1.553851, train acc: 0.913783, 
[Test] epoch:133	batch id:0	 loss:1.305219
[Test] epoch:133	batch id:10	 loss:1.472717
[Test] epoch:133	batch id:20	 loss:1.426296
[Test] epoch:133	batch id:30	 loss:1.445144
[Test] epoch:133	batch id:40	 loss:1.400408
[Test] epoch:133	batch id:50	 loss:1.347796
[Test] epoch:133	batch id:60	 loss:1.292892
[Test] epoch:133	batch id:70	 loss:1.402391
[Test] epoch:133	batch id:80	 loss:1.759370
[Test] epoch:133	batch id:90	 loss:1.508828
[Test] epoch:133	batch id:100	 loss:1.889258
[Test] epoch:133	batch id:110	 loss:1.392576
[Test] epoch:133	batch id:120	 loss:1.485044
[Test] epoch:133	batch id:130	 loss:1.415299
[Test] epoch:133	batch id:140	 loss:1.480237
[Test] epoch:133	batch id:150	 loss:1.866532
[Test] 133, loss: 1.503922, test acc: 0.905997,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:134	batch id:0	 lr:0.067480 loss:1.423151
[Train] epoch:134	batch id:10	 lr:0.067480 loss:1.565183
[Train] epoch:134	batch id:20	 lr:0.067480 loss:1.486105
[Train] epoch:134	batch id:30	 lr:0.067480 loss:1.426394
[Train] epoch:134	batch id:40	 lr:0.067480 loss:1.598212
[Train] epoch:134	batch id:50	 lr:0.067480 loss:1.652782
[Train] epoch:134	batch id:60	 lr:0.067480 loss:1.629295
[Train] epoch:134	batch id:70	 lr:0.067480 loss:1.643054
[Train] epoch:134	batch id:80	 lr:0.067480 loss:1.528887
[Train] epoch:134	batch id:90	 lr:0.067480 loss:1.540356
[Train] epoch:134	batch id:100	 lr:0.067480 loss:1.425045
[Train] epoch:134	batch id:110	 lr:0.067480 loss:1.560895
[Train] epoch:134	batch id:120	 lr:0.067480 loss:1.576855
[Train] epoch:134	batch id:130	 lr:0.067480 loss:1.460486
[Train] epoch:134	batch id:140	 lr:0.067480 loss:1.440857
[Train] epoch:134	batch id:150	 lr:0.067480 loss:1.486223
[Train] epoch:134	batch id:160	 lr:0.067480 loss:1.456833
[Train] epoch:134	batch id:170	 lr:0.067480 loss:1.620345
[Train] epoch:134	batch id:180	 lr:0.067480 loss:1.596424
[Train] epoch:134	batch id:190	 lr:0.067480 loss:1.526493
[Train] epoch:134	batch id:200	 lr:0.067480 loss:1.659173
[Train] epoch:134	batch id:210	 lr:0.067480 loss:1.596602
[Train] epoch:134	batch id:220	 lr:0.067480 loss:1.494831
[Train] epoch:134	batch id:230	 lr:0.067480 loss:1.657368
[Train] epoch:134	batch id:240	 lr:0.067480 loss:1.650057
[Train] epoch:134	batch id:250	 lr:0.067480 loss:1.514470
[Train] epoch:134	batch id:260	 lr:0.067480 loss:1.487610
[Train] epoch:134	batch id:270	 lr:0.067480 loss:1.569191
[Train] epoch:134	batch id:280	 lr:0.067480 loss:1.745604
[Train] epoch:134	batch id:290	 lr:0.067480 loss:1.618184
[Train] epoch:134	batch id:300	 lr:0.067480 loss:1.512110
[Train] 134, loss: 1.544497, train acc: 0.915818, 
[Test] epoch:134	batch id:0	 loss:1.334665
[Test] epoch:134	batch id:10	 loss:1.457422
[Test] epoch:134	batch id:20	 loss:1.393617
[Test] epoch:134	batch id:30	 loss:1.431854
[Test] epoch:134	batch id:40	 loss:1.430948
[Test] epoch:134	batch id:50	 loss:1.451690
[Test] epoch:134	batch id:60	 loss:1.297859
[Test] epoch:134	batch id:70	 loss:1.434474
[Test] epoch:134	batch id:80	 loss:1.692744
[Test] epoch:134	batch id:90	 loss:1.483891
[Test] epoch:134	batch id:100	 loss:2.007196
[Test] epoch:134	batch id:110	 loss:1.426916
[Test] epoch:134	batch id:120	 loss:1.383817
[Test] epoch:134	batch id:130	 loss:1.412910
[Test] epoch:134	batch id:140	 loss:1.409795
[Test] epoch:134	batch id:150	 loss:1.993838
[Test] 134, loss: 1.516401, test acc: 0.901945,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:135	batch id:0	 lr:0.067062 loss:1.518633
[Train] epoch:135	batch id:10	 lr:0.067062 loss:1.639245
[Train] epoch:135	batch id:20	 lr:0.067062 loss:1.541642
[Train] epoch:135	batch id:30	 lr:0.067062 loss:1.476700
[Train] epoch:135	batch id:40	 lr:0.067062 loss:1.552896
[Train] epoch:135	batch id:50	 lr:0.067062 loss:1.694581
[Train] epoch:135	batch id:60	 lr:0.067062 loss:1.577343
[Train] epoch:135	batch id:70	 lr:0.067062 loss:1.514012
[Train] epoch:135	batch id:80	 lr:0.067062 loss:1.737473
[Train] epoch:135	batch id:90	 lr:0.067062 loss:1.651850
[Train] epoch:135	batch id:100	 lr:0.067062 loss:1.434998
[Train] epoch:135	batch id:110	 lr:0.067062 loss:1.556405
[Train] epoch:135	batch id:120	 lr:0.067062 loss:1.500487
[Train] epoch:135	batch id:130	 lr:0.067062 loss:1.501987
[Train] epoch:135	batch id:140	 lr:0.067062 loss:1.682301
[Train] epoch:135	batch id:150	 lr:0.067062 loss:1.589700
[Train] epoch:135	batch id:160	 lr:0.067062 loss:1.575909
[Train] epoch:135	batch id:170	 lr:0.067062 loss:1.505694
[Train] epoch:135	batch id:180	 lr:0.067062 loss:1.509326
[Train] epoch:135	batch id:190	 lr:0.067062 loss:1.550270
[Train] epoch:135	batch id:200	 lr:0.067062 loss:1.625337
[Train] epoch:135	batch id:210	 lr:0.067062 loss:1.532199
[Train] epoch:135	batch id:220	 lr:0.067062 loss:1.592755
[Train] epoch:135	batch id:230	 lr:0.067062 loss:1.646066
[Train] epoch:135	batch id:240	 lr:0.067062 loss:1.423701
[Train] epoch:135	batch id:250	 lr:0.067062 loss:1.444939
[Train] epoch:135	batch id:260	 lr:0.067062 loss:1.566517
[Train] epoch:135	batch id:270	 lr:0.067062 loss:1.543403
[Train] epoch:135	batch id:280	 lr:0.067062 loss:1.663637
[Train] epoch:135	batch id:290	 lr:0.067062 loss:1.544837
[Train] epoch:135	batch id:300	 lr:0.067062 loss:1.472692
[Train] 135, loss: 1.543664, train acc: 0.916531, 
[Test] epoch:135	batch id:0	 loss:1.330639
[Test] epoch:135	batch id:10	 loss:1.415272
[Test] epoch:135	batch id:20	 loss:1.348970
[Test] epoch:135	batch id:30	 loss:1.479336
[Test] epoch:135	batch id:40	 loss:1.487705
[Test] epoch:135	batch id:50	 loss:1.561972
[Test] epoch:135	batch id:60	 loss:1.356626
[Test] epoch:135	batch id:70	 loss:1.503492
[Test] epoch:135	batch id:80	 loss:1.731659
[Test] epoch:135	batch id:90	 loss:1.487544
[Test] epoch:135	batch id:100	 loss:1.920491
[Test] epoch:135	batch id:110	 loss:1.410921
[Test] epoch:135	batch id:120	 loss:1.482824
[Test] epoch:135	batch id:130	 loss:1.495770
[Test] epoch:135	batch id:140	 loss:1.375767
[Test] epoch:135	batch id:150	 loss:1.824128
[Test] 135, loss: 1.524447, test acc: 0.903566,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:136	batch id:0	 lr:0.066643 loss:1.680294
[Train] epoch:136	batch id:10	 lr:0.066643 loss:1.416579
[Train] epoch:136	batch id:20	 lr:0.066643 loss:1.632928
[Train] epoch:136	batch id:30	 lr:0.066643 loss:1.394536
[Train] epoch:136	batch id:40	 lr:0.066643 loss:1.473548
[Train] epoch:136	batch id:50	 lr:0.066643 loss:1.557829
[Train] epoch:136	batch id:60	 lr:0.066643 loss:1.747973
[Train] epoch:136	batch id:70	 lr:0.066643 loss:1.379223
[Train] epoch:136	batch id:80	 lr:0.066643 loss:1.573069
[Train] epoch:136	batch id:90	 lr:0.066643 loss:1.611508
[Train] epoch:136	batch id:100	 lr:0.066643 loss:1.766123
[Train] epoch:136	batch id:110	 lr:0.066643 loss:1.574297
[Train] epoch:136	batch id:120	 lr:0.066643 loss:1.505654
[Train] epoch:136	batch id:130	 lr:0.066643 loss:1.463685
[Train] epoch:136	batch id:140	 lr:0.066643 loss:1.525772
[Train] epoch:136	batch id:150	 lr:0.066643 loss:1.552244
[Train] epoch:136	batch id:160	 lr:0.066643 loss:1.494519
[Train] epoch:136	batch id:170	 lr:0.066643 loss:1.496343
[Train] epoch:136	batch id:180	 lr:0.066643 loss:1.649635
[Train] epoch:136	batch id:190	 lr:0.066643 loss:1.650170
[Train] epoch:136	batch id:200	 lr:0.066643 loss:1.644580
[Train] epoch:136	batch id:210	 lr:0.066643 loss:1.561592
[Train] epoch:136	batch id:220	 lr:0.066643 loss:1.554949
[Train] epoch:136	batch id:230	 lr:0.066643 loss:1.503590
[Train] epoch:136	batch id:240	 lr:0.066643 loss:1.593791
[Train] epoch:136	batch id:250	 lr:0.066643 loss:1.520380
[Train] epoch:136	batch id:260	 lr:0.066643 loss:1.571452
[Train] epoch:136	batch id:270	 lr:0.066643 loss:1.764754
[Train] epoch:136	batch id:280	 lr:0.066643 loss:1.633414
[Train] epoch:136	batch id:290	 lr:0.066643 loss:1.531610
[Train] epoch:136	batch id:300	 lr:0.066643 loss:1.672624
[Train] 136, loss: 1.559133, train acc: 0.908082, 
[Test] epoch:136	batch id:0	 loss:1.338027
[Test] epoch:136	batch id:10	 loss:1.524218
[Test] epoch:136	batch id:20	 loss:1.476807
[Test] epoch:136	batch id:30	 loss:1.459740
[Test] epoch:136	batch id:40	 loss:1.439143
[Test] epoch:136	batch id:50	 loss:1.604264
[Test] epoch:136	batch id:60	 loss:1.309914
[Test] epoch:136	batch id:70	 loss:1.413831
[Test] epoch:136	batch id:80	 loss:1.774294
[Test] epoch:136	batch id:90	 loss:1.493177
[Test] epoch:136	batch id:100	 loss:1.956952
[Test] epoch:136	batch id:110	 loss:1.408292
[Test] epoch:136	batch id:120	 loss:1.422625
[Test] epoch:136	batch id:130	 loss:1.514347
[Test] epoch:136	batch id:140	 loss:1.449908
[Test] epoch:136	batch id:150	 loss:1.831478
[Test] 136, loss: 1.537330, test acc: 0.903971,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:137	batch id:0	 lr:0.066222 loss:1.549939
[Train] epoch:137	batch id:10	 lr:0.066222 loss:1.643496
[Train] epoch:137	batch id:20	 lr:0.066222 loss:1.495766
[Train] epoch:137	batch id:30	 lr:0.066222 loss:1.517106
[Train] epoch:137	batch id:40	 lr:0.066222 loss:1.555602
[Train] epoch:137	batch id:50	 lr:0.066222 loss:1.360353
[Train] epoch:137	batch id:60	 lr:0.066222 loss:1.585717
[Train] epoch:137	batch id:70	 lr:0.066222 loss:1.627656
[Train] epoch:137	batch id:80	 lr:0.066222 loss:1.454814
[Train] epoch:137	batch id:90	 lr:0.066222 loss:1.653548
[Train] epoch:137	batch id:100	 lr:0.066222 loss:1.481178
[Train] epoch:137	batch id:110	 lr:0.066222 loss:1.568767
[Train] epoch:137	batch id:120	 lr:0.066222 loss:1.537102
[Train] epoch:137	batch id:130	 lr:0.066222 loss:1.563155
[Train] epoch:137	batch id:140	 lr:0.066222 loss:1.503511
[Train] epoch:137	batch id:150	 lr:0.066222 loss:1.524309
[Train] epoch:137	batch id:160	 lr:0.066222 loss:1.533026
[Train] epoch:137	batch id:170	 lr:0.066222 loss:1.505301
[Train] epoch:137	batch id:180	 lr:0.066222 loss:1.521410
[Train] epoch:137	batch id:190	 lr:0.066222 loss:1.532552
[Train] epoch:137	batch id:200	 lr:0.066222 loss:1.603345
[Train] epoch:137	batch id:210	 lr:0.066222 loss:1.523849
[Train] epoch:137	batch id:220	 lr:0.066222 loss:1.574113
[Train] epoch:137	batch id:230	 lr:0.066222 loss:1.616210
[Train] epoch:137	batch id:240	 lr:0.066222 loss:1.703766
[Train] epoch:137	batch id:250	 lr:0.066222 loss:1.453050
[Train] epoch:137	batch id:260	 lr:0.066222 loss:1.603389
[Train] epoch:137	batch id:270	 lr:0.066222 loss:1.560792
[Train] epoch:137	batch id:280	 lr:0.066222 loss:1.600487
[Train] epoch:137	batch id:290	 lr:0.066222 loss:1.762524
[Train] epoch:137	batch id:300	 lr:0.066222 loss:1.501038
[Train] 137, loss: 1.552163, train acc: 0.912866, 
[Test] epoch:137	batch id:0	 loss:1.348191
[Test] epoch:137	batch id:10	 loss:1.558999
[Test] epoch:137	batch id:20	 loss:1.421256
[Test] epoch:137	batch id:30	 loss:1.482823
[Test] epoch:137	batch id:40	 loss:1.391370
[Test] epoch:137	batch id:50	 loss:1.489264
[Test] epoch:137	batch id:60	 loss:1.313391
[Test] epoch:137	batch id:70	 loss:1.431898
[Test] epoch:137	batch id:80	 loss:1.640640
[Test] epoch:137	batch id:90	 loss:1.480871
[Test] epoch:137	batch id:100	 loss:1.959299
[Test] epoch:137	batch id:110	 loss:1.465745
[Test] epoch:137	batch id:120	 loss:1.390092
[Test] epoch:137	batch id:130	 loss:1.488629
[Test] epoch:137	batch id:140	 loss:1.349126
[Test] epoch:137	batch id:150	 loss:1.904846
[Test] 137, loss: 1.510424, test acc: 0.909643,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:138	batch id:0	 lr:0.065800 loss:1.548273
[Train] epoch:138	batch id:10	 lr:0.065800 loss:1.671315
[Train] epoch:138	batch id:20	 lr:0.065800 loss:1.418380
[Train] epoch:138	batch id:30	 lr:0.065800 loss:1.638062
[Train] epoch:138	batch id:40	 lr:0.065800 loss:1.463003
[Train] epoch:138	batch id:50	 lr:0.065800 loss:1.568545
[Train] epoch:138	batch id:60	 lr:0.065800 loss:1.608465
[Train] epoch:138	batch id:70	 lr:0.065800 loss:1.607417
[Train] epoch:138	batch id:80	 lr:0.065800 loss:1.598913
[Train] epoch:138	batch id:90	 lr:0.065800 loss:1.520585
[Train] epoch:138	batch id:100	 lr:0.065800 loss:1.476659
[Train] epoch:138	batch id:110	 lr:0.065800 loss:1.637834
[Train] epoch:138	batch id:120	 lr:0.065800 loss:1.628168
[Train] epoch:138	batch id:130	 lr:0.065800 loss:1.533519
[Train] epoch:138	batch id:140	 lr:0.065800 loss:1.682078
[Train] epoch:138	batch id:150	 lr:0.065800 loss:1.455423
[Train] epoch:138	batch id:160	 lr:0.065800 loss:1.569833
[Train] epoch:138	batch id:170	 lr:0.065800 loss:1.629612
[Train] epoch:138	batch id:180	 lr:0.065800 loss:1.477064
[Train] epoch:138	batch id:190	 lr:0.065800 loss:1.382490
[Train] epoch:138	batch id:200	 lr:0.065800 loss:1.698300
[Train] epoch:138	batch id:210	 lr:0.065800 loss:1.430464
[Train] epoch:138	batch id:220	 lr:0.065800 loss:1.622902
[Train] epoch:138	batch id:230	 lr:0.065800 loss:1.516953
[Train] epoch:138	batch id:240	 lr:0.065800 loss:1.651812
[Train] epoch:138	batch id:250	 lr:0.065800 loss:1.527460
[Train] epoch:138	batch id:260	 lr:0.065800 loss:1.446652
[Train] epoch:138	batch id:270	 lr:0.065800 loss:1.568554
[Train] epoch:138	batch id:280	 lr:0.065800 loss:1.594113
[Train] epoch:138	batch id:290	 lr:0.065800 loss:1.572496
[Train] epoch:138	batch id:300	 lr:0.065800 loss:1.601353
[Train] 138, loss: 1.549756, train acc: 0.911645, 
[Test] epoch:138	batch id:0	 loss:1.408580
[Test] epoch:138	batch id:10	 loss:1.592389
[Test] epoch:138	batch id:20	 loss:1.372713
[Test] epoch:138	batch id:30	 loss:1.457112
[Test] epoch:138	batch id:40	 loss:1.512405
[Test] epoch:138	batch id:50	 loss:1.455052
[Test] epoch:138	batch id:60	 loss:1.314389
[Test] epoch:138	batch id:70	 loss:1.395062
[Test] epoch:138	batch id:80	 loss:1.698817
[Test] epoch:138	batch id:90	 loss:1.455204
[Test] epoch:138	batch id:100	 loss:1.845662
[Test] epoch:138	batch id:110	 loss:1.409014
[Test] epoch:138	batch id:120	 loss:1.400072
[Test] epoch:138	batch id:130	 loss:1.558831
[Test] epoch:138	batch id:140	 loss:1.439158
[Test] epoch:138	batch id:150	 loss:1.730750
[Test] 138, loss: 1.528818, test acc: 0.905997,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:139	batch id:0	 lr:0.065377 loss:1.426111
[Train] epoch:139	batch id:10	 lr:0.065377 loss:1.536333
[Train] epoch:139	batch id:20	 lr:0.065377 loss:1.537179
[Train] epoch:139	batch id:30	 lr:0.065377 loss:1.422319
[Train] epoch:139	batch id:40	 lr:0.065377 loss:1.476068
[Train] epoch:139	batch id:50	 lr:0.065377 loss:1.526223
[Train] epoch:139	batch id:60	 lr:0.065377 loss:1.521892
[Train] epoch:139	batch id:70	 lr:0.065377 loss:1.641162
[Train] epoch:139	batch id:80	 lr:0.065377 loss:1.478092
[Train] epoch:139	batch id:90	 lr:0.065377 loss:1.764646
[Train] epoch:139	batch id:100	 lr:0.065377 loss:1.569601
[Train] epoch:139	batch id:110	 lr:0.065377 loss:1.547023
[Train] epoch:139	batch id:120	 lr:0.065377 loss:1.692047
[Train] epoch:139	batch id:130	 lr:0.065377 loss:1.597899
[Train] epoch:139	batch id:140	 lr:0.065377 loss:1.556447
[Train] epoch:139	batch id:150	 lr:0.065377 loss:1.487112
[Train] epoch:139	batch id:160	 lr:0.065377 loss:1.486483
[Train] epoch:139	batch id:170	 lr:0.065377 loss:1.700493
[Train] epoch:139	batch id:180	 lr:0.065377 loss:1.658308
[Train] epoch:139	batch id:190	 lr:0.065377 loss:1.610608
[Train] epoch:139	batch id:200	 lr:0.065377 loss:1.535320
[Train] epoch:139	batch id:210	 lr:0.065377 loss:1.569032
[Train] epoch:139	batch id:220	 lr:0.065377 loss:1.494372
[Train] epoch:139	batch id:230	 lr:0.065377 loss:1.636942
[Train] epoch:139	batch id:240	 lr:0.065377 loss:1.532785
[Train] epoch:139	batch id:250	 lr:0.065377 loss:1.624339
[Train] epoch:139	batch id:260	 lr:0.065377 loss:1.461550
[Train] epoch:139	batch id:270	 lr:0.065377 loss:1.384091
[Train] epoch:139	batch id:280	 lr:0.065377 loss:1.534840
[Train] epoch:139	batch id:290	 lr:0.065377 loss:1.380800
[Train] epoch:139	batch id:300	 lr:0.065377 loss:1.501359
[Train] 139, loss: 1.549526, train acc: 0.912357, 
[Test] epoch:139	batch id:0	 loss:1.329780
[Test] epoch:139	batch id:10	 loss:1.466398
[Test] epoch:139	batch id:20	 loss:1.351761
[Test] epoch:139	batch id:30	 loss:1.722756
[Test] epoch:139	batch id:40	 loss:1.432855
[Test] epoch:139	batch id:50	 loss:1.514844
[Test] epoch:139	batch id:60	 loss:1.292491
[Test] epoch:139	batch id:70	 loss:1.494120
[Test] epoch:139	batch id:80	 loss:1.707166
[Test] epoch:139	batch id:90	 loss:1.596289
[Test] epoch:139	batch id:100	 loss:2.117451
[Test] epoch:139	batch id:110	 loss:1.487962
[Test] epoch:139	batch id:120	 loss:1.508077
[Test] epoch:139	batch id:130	 loss:1.415888
[Test] epoch:139	batch id:140	 loss:1.456785
[Test] epoch:139	batch id:150	 loss:1.794720
[Test] 139, loss: 1.525946, test acc: 0.905186,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:140	batch id:0	 lr:0.064953 loss:1.557707
[Train] epoch:140	batch id:10	 lr:0.064953 loss:1.816702
[Train] epoch:140	batch id:20	 lr:0.064953 loss:1.580478
[Train] epoch:140	batch id:30	 lr:0.064953 loss:1.604939
[Train] epoch:140	batch id:40	 lr:0.064953 loss:1.552548
[Train] epoch:140	batch id:50	 lr:0.064953 loss:1.488101
[Train] epoch:140	batch id:60	 lr:0.064953 loss:1.555998
[Train] epoch:140	batch id:70	 lr:0.064953 loss:1.482800
[Train] epoch:140	batch id:80	 lr:0.064953 loss:1.585901
[Train] epoch:140	batch id:90	 lr:0.064953 loss:1.435892
[Train] epoch:140	batch id:100	 lr:0.064953 loss:1.439975
[Train] epoch:140	batch id:110	 lr:0.064953 loss:1.477862
[Train] epoch:140	batch id:120	 lr:0.064953 loss:1.477124
[Train] epoch:140	batch id:130	 lr:0.064953 loss:1.441066
[Train] epoch:140	batch id:140	 lr:0.064953 loss:1.526742
[Train] epoch:140	batch id:150	 lr:0.064953 loss:1.521805
[Train] epoch:140	batch id:160	 lr:0.064953 loss:1.512148
[Train] epoch:140	batch id:170	 lr:0.064953 loss:1.564949
[Train] epoch:140	batch id:180	 lr:0.064953 loss:1.535394
[Train] epoch:140	batch id:190	 lr:0.064953 loss:1.607193
[Train] epoch:140	batch id:200	 lr:0.064953 loss:1.516189
[Train] epoch:140	batch id:210	 lr:0.064953 loss:1.520307
[Train] epoch:140	batch id:220	 lr:0.064953 loss:1.454731
[Train] epoch:140	batch id:230	 lr:0.064953 loss:1.573184
[Train] epoch:140	batch id:240	 lr:0.064953 loss:1.491061
[Train] epoch:140	batch id:250	 lr:0.064953 loss:1.535342
[Train] epoch:140	batch id:260	 lr:0.064953 loss:1.438422
[Train] epoch:140	batch id:270	 lr:0.064953 loss:1.501691
[Train] epoch:140	batch id:280	 lr:0.064953 loss:1.469401
[Train] epoch:140	batch id:290	 lr:0.064953 loss:1.642281
[Train] epoch:140	batch id:300	 lr:0.064953 loss:1.596425
[Train] 140, loss: 1.542783, train acc: 0.917651, 
[Test] epoch:140	batch id:0	 loss:1.326426
[Test] epoch:140	batch id:10	 loss:1.471224
[Test] epoch:140	batch id:20	 loss:1.510892
[Test] epoch:140	batch id:30	 loss:1.419637
[Test] epoch:140	batch id:40	 loss:1.382879
[Test] epoch:140	batch id:50	 loss:1.468686
[Test] epoch:140	batch id:60	 loss:1.314385
[Test] epoch:140	batch id:70	 loss:1.460474
[Test] epoch:140	batch id:80	 loss:1.665997
[Test] epoch:140	batch id:90	 loss:1.531682
[Test] epoch:140	batch id:100	 loss:2.038864
[Test] epoch:140	batch id:110	 loss:1.464500
[Test] epoch:140	batch id:120	 loss:1.423627
[Test] epoch:140	batch id:130	 loss:1.581938
[Test] epoch:140	batch id:140	 loss:1.464002
[Test] epoch:140	batch id:150	 loss:1.760989
[Test] 140, loss: 1.541402, test acc: 0.888574,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:141	batch id:0	 lr:0.064527 loss:1.604878
[Train] epoch:141	batch id:10	 lr:0.064527 loss:1.755817
[Train] epoch:141	batch id:20	 lr:0.064527 loss:1.477081
[Train] epoch:141	batch id:30	 lr:0.064527 loss:1.548140
[Train] epoch:141	batch id:40	 lr:0.064527 loss:1.465229
[Train] epoch:141	batch id:50	 lr:0.064527 loss:1.509821
[Train] epoch:141	batch id:60	 lr:0.064527 loss:1.593067
[Train] epoch:141	batch id:70	 lr:0.064527 loss:1.434817
[Train] epoch:141	batch id:80	 lr:0.064527 loss:1.414275
[Train] epoch:141	batch id:90	 lr:0.064527 loss:1.577383
[Train] epoch:141	batch id:100	 lr:0.064527 loss:1.400867
[Train] epoch:141	batch id:110	 lr:0.064527 loss:1.546957
[Train] epoch:141	batch id:120	 lr:0.064527 loss:1.557080
[Train] epoch:141	batch id:130	 lr:0.064527 loss:1.644182
[Train] epoch:141	batch id:140	 lr:0.064527 loss:1.607360
[Train] epoch:141	batch id:150	 lr:0.064527 loss:1.501483
[Train] epoch:141	batch id:160	 lr:0.064527 loss:1.379684
[Train] epoch:141	batch id:170	 lr:0.064527 loss:1.494176
[Train] epoch:141	batch id:180	 lr:0.064527 loss:1.480074
[Train] epoch:141	batch id:190	 lr:0.064527 loss:1.532476
[Train] epoch:141	batch id:200	 lr:0.064527 loss:1.836065
[Train] epoch:141	batch id:210	 lr:0.064527 loss:1.612520
[Train] epoch:141	batch id:220	 lr:0.064527 loss:1.535181
[Train] epoch:141	batch id:230	 lr:0.064527 loss:1.502061
[Train] epoch:141	batch id:240	 lr:0.064527 loss:1.660398
[Train] epoch:141	batch id:250	 lr:0.064527 loss:1.681118
[Train] epoch:141	batch id:260	 lr:0.064527 loss:1.704154
[Train] epoch:141	batch id:270	 lr:0.064527 loss:1.544740
[Train] epoch:141	batch id:280	 lr:0.064527 loss:1.584155
[Train] epoch:141	batch id:290	 lr:0.064527 loss:1.593282
[Train] epoch:141	batch id:300	 lr:0.064527 loss:1.580810
[Train] 141, loss: 1.543978, train acc: 0.914902, 
[Test] epoch:141	batch id:0	 loss:1.330370
[Test] epoch:141	batch id:10	 loss:1.414523
[Test] epoch:141	batch id:20	 loss:1.516620
[Test] epoch:141	batch id:30	 loss:1.426160
[Test] epoch:141	batch id:40	 loss:1.482487
[Test] epoch:141	batch id:50	 loss:1.564497
[Test] epoch:141	batch id:60	 loss:1.357495
[Test] epoch:141	batch id:70	 loss:1.519466
[Test] epoch:141	batch id:80	 loss:1.618590
[Test] epoch:141	batch id:90	 loss:1.537019
[Test] epoch:141	batch id:100	 loss:1.844167
[Test] epoch:141	batch id:110	 loss:1.437907
[Test] epoch:141	batch id:120	 loss:1.407114
[Test] epoch:141	batch id:130	 loss:1.542873
[Test] epoch:141	batch id:140	 loss:1.483551
[Test] epoch:141	batch id:150	 loss:1.736508
[Test] 141, loss: 1.536108, test acc: 0.909238,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:142	batch id:0	 lr:0.064101 loss:1.404360
[Train] epoch:142	batch id:10	 lr:0.064101 loss:1.464094
[Train] epoch:142	batch id:20	 lr:0.064101 loss:1.475040
[Train] epoch:142	batch id:30	 lr:0.064101 loss:1.395761
[Train] epoch:142	batch id:40	 lr:0.064101 loss:1.693508
[Train] epoch:142	batch id:50	 lr:0.064101 loss:1.557331
[Train] epoch:142	batch id:60	 lr:0.064101 loss:1.460045
[Train] epoch:142	batch id:70	 lr:0.064101 loss:1.548101
[Train] epoch:142	batch id:80	 lr:0.064101 loss:1.457014
[Train] epoch:142	batch id:90	 lr:0.064101 loss:1.467193
[Train] epoch:142	batch id:100	 lr:0.064101 loss:1.440410
[Train] epoch:142	batch id:110	 lr:0.064101 loss:1.592861
[Train] epoch:142	batch id:120	 lr:0.064101 loss:1.684348
[Train] epoch:142	batch id:130	 lr:0.064101 loss:1.558479
[Train] epoch:142	batch id:140	 lr:0.064101 loss:1.489000
[Train] epoch:142	batch id:150	 lr:0.064101 loss:1.635085
[Train] epoch:142	batch id:160	 lr:0.064101 loss:1.457248
[Train] epoch:142	batch id:170	 lr:0.064101 loss:1.563969
[Train] epoch:142	batch id:180	 lr:0.064101 loss:1.462968
[Train] epoch:142	batch id:190	 lr:0.064101 loss:1.432046
[Train] epoch:142	batch id:200	 lr:0.064101 loss:1.640566
[Train] epoch:142	batch id:210	 lr:0.064101 loss:1.646219
[Train] epoch:142	batch id:220	 lr:0.064101 loss:1.397915
[Train] epoch:142	batch id:230	 lr:0.064101 loss:1.554033
[Train] epoch:142	batch id:240	 lr:0.064101 loss:1.563854
[Train] epoch:142	batch id:250	 lr:0.064101 loss:1.605988
[Train] epoch:142	batch id:260	 lr:0.064101 loss:1.480114
[Train] epoch:142	batch id:270	 lr:0.064101 loss:1.422353
[Train] epoch:142	batch id:280	 lr:0.064101 loss:1.522623
[Train] epoch:142	batch id:290	 lr:0.064101 loss:1.580122
[Train] epoch:142	batch id:300	 lr:0.064101 loss:1.515586
[Train] 142, loss: 1.547381, train acc: 0.914393, 
[Test] epoch:142	batch id:0	 loss:1.341747
[Test] epoch:142	batch id:10	 loss:1.532450
[Test] epoch:142	batch id:20	 loss:1.371597
[Test] epoch:142	batch id:30	 loss:1.388689
[Test] epoch:142	batch id:40	 loss:1.413556
[Test] epoch:142	batch id:50	 loss:1.449312
[Test] epoch:142	batch id:60	 loss:1.297907
[Test] epoch:142	batch id:70	 loss:1.403267
[Test] epoch:142	batch id:80	 loss:1.668249
[Test] epoch:142	batch id:90	 loss:1.492598
[Test] epoch:142	batch id:100	 loss:1.925073
[Test] epoch:142	batch id:110	 loss:1.364002
[Test] epoch:142	batch id:120	 loss:1.362321
[Test] epoch:142	batch id:130	 loss:1.518721
[Test] epoch:142	batch id:140	 loss:1.389805
[Test] epoch:142	batch id:150	 loss:1.886434
[Test] 142, loss: 1.520853, test acc: 0.904376,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:143	batch id:0	 lr:0.063673 loss:1.411014
[Train] epoch:143	batch id:10	 lr:0.063673 loss:1.630093
[Train] epoch:143	batch id:20	 lr:0.063673 loss:1.517551
[Train] epoch:143	batch id:30	 lr:0.063673 loss:1.477070
[Train] epoch:143	batch id:40	 lr:0.063673 loss:1.512858
[Train] epoch:143	batch id:50	 lr:0.063673 loss:1.401132
[Train] epoch:143	batch id:60	 lr:0.063673 loss:1.538022
[Train] epoch:143	batch id:70	 lr:0.063673 loss:1.449631
[Train] epoch:143	batch id:80	 lr:0.063673 loss:1.437882
[Train] epoch:143	batch id:90	 lr:0.063673 loss:1.449501
[Train] epoch:143	batch id:100	 lr:0.063673 loss:1.485198
[Train] epoch:143	batch id:110	 lr:0.063673 loss:1.677414
[Train] epoch:143	batch id:120	 lr:0.063673 loss:1.634210
[Train] epoch:143	batch id:130	 lr:0.063673 loss:1.511695
[Train] epoch:143	batch id:140	 lr:0.063673 loss:1.497728
[Train] epoch:143	batch id:150	 lr:0.063673 loss:1.539707
[Train] epoch:143	batch id:160	 lr:0.063673 loss:1.464921
[Train] epoch:143	batch id:170	 lr:0.063673 loss:1.497005
[Train] epoch:143	batch id:180	 lr:0.063673 loss:1.590241
[Train] epoch:143	batch id:190	 lr:0.063673 loss:1.650826
[Train] epoch:143	batch id:200	 lr:0.063673 loss:1.735076
[Train] epoch:143	batch id:210	 lr:0.063673 loss:1.604304
[Train] epoch:143	batch id:220	 lr:0.063673 loss:1.522773
[Train] epoch:143	batch id:230	 lr:0.063673 loss:1.453061
[Train] epoch:143	batch id:240	 lr:0.063673 loss:1.485748
[Train] epoch:143	batch id:250	 lr:0.063673 loss:1.480802
[Train] epoch:143	batch id:260	 lr:0.063673 loss:1.752990
[Train] epoch:143	batch id:270	 lr:0.063673 loss:1.631797
[Train] epoch:143	batch id:280	 lr:0.063673 loss:1.597499
[Train] epoch:143	batch id:290	 lr:0.063673 loss:1.531488
[Train] epoch:143	batch id:300	 lr:0.063673 loss:1.681583
[Train] 143, loss: 1.540860, train acc: 0.915309, 
[Test] epoch:143	batch id:0	 loss:1.326119
[Test] epoch:143	batch id:10	 loss:1.409482
[Test] epoch:143	batch id:20	 loss:1.441225
[Test] epoch:143	batch id:30	 loss:1.464768
[Test] epoch:143	batch id:40	 loss:1.353050
[Test] epoch:143	batch id:50	 loss:1.425083
[Test] epoch:143	batch id:60	 loss:1.293958
[Test] epoch:143	batch id:70	 loss:1.539162
[Test] epoch:143	batch id:80	 loss:1.643475
[Test] epoch:143	batch id:90	 loss:1.526831
[Test] epoch:143	batch id:100	 loss:1.885945
[Test] epoch:143	batch id:110	 loss:1.482044
[Test] epoch:143	batch id:120	 loss:1.338241
[Test] epoch:143	batch id:130	 loss:1.444236
[Test] epoch:143	batch id:140	 loss:1.368530
[Test] epoch:143	batch id:150	 loss:1.718176
[Test] 143, loss: 1.494989, test acc: 0.907212,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:144	batch id:0	 lr:0.063244 loss:1.492213
[Train] epoch:144	batch id:10	 lr:0.063244 loss:1.545188
[Train] epoch:144	batch id:20	 lr:0.063244 loss:1.455886
[Train] epoch:144	batch id:30	 lr:0.063244 loss:1.517721
[Train] epoch:144	batch id:40	 lr:0.063244 loss:1.611597
[Train] epoch:144	batch id:50	 lr:0.063244 loss:1.507407
[Train] epoch:144	batch id:60	 lr:0.063244 loss:1.537952
[Train] epoch:144	batch id:70	 lr:0.063244 loss:1.633138
[Train] epoch:144	batch id:80	 lr:0.063244 loss:1.505962
[Train] epoch:144	batch id:90	 lr:0.063244 loss:1.507414
[Train] epoch:144	batch id:100	 lr:0.063244 loss:1.485770
[Train] epoch:144	batch id:110	 lr:0.063244 loss:1.562993
[Train] epoch:144	batch id:120	 lr:0.063244 loss:1.683105
[Train] epoch:144	batch id:130	 lr:0.063244 loss:1.461127
[Train] epoch:144	batch id:140	 lr:0.063244 loss:1.445363
[Train] epoch:144	batch id:150	 lr:0.063244 loss:1.456904
[Train] epoch:144	batch id:160	 lr:0.063244 loss:1.445509
[Train] epoch:144	batch id:170	 lr:0.063244 loss:1.415999
[Train] epoch:144	batch id:180	 lr:0.063244 loss:1.623109
[Train] epoch:144	batch id:190	 lr:0.063244 loss:1.476107
[Train] epoch:144	batch id:200	 lr:0.063244 loss:1.520689
[Train] epoch:144	batch id:210	 lr:0.063244 loss:1.464801
[Train] epoch:144	batch id:220	 lr:0.063244 loss:1.420044
[Train] epoch:144	batch id:230	 lr:0.063244 loss:1.471556
[Train] epoch:144	batch id:240	 lr:0.063244 loss:1.398351
[Train] epoch:144	batch id:250	 lr:0.063244 loss:1.495249
[Train] epoch:144	batch id:260	 lr:0.063244 loss:1.494008
[Train] epoch:144	batch id:270	 lr:0.063244 loss:1.621248
[Train] epoch:144	batch id:280	 lr:0.063244 loss:1.535503
[Train] epoch:144	batch id:290	 lr:0.063244 loss:1.649285
[Train] epoch:144	batch id:300	 lr:0.063244 loss:1.467535
[Train] 144, loss: 1.537071, train acc: 0.918160, 
[Test] epoch:144	batch id:0	 loss:1.328243
[Test] epoch:144	batch id:10	 loss:1.441633
[Test] epoch:144	batch id:20	 loss:1.380061
[Test] epoch:144	batch id:30	 loss:1.467455
[Test] epoch:144	batch id:40	 loss:1.448384
[Test] epoch:144	batch id:50	 loss:1.479521
[Test] epoch:144	batch id:60	 loss:1.303173
[Test] epoch:144	batch id:70	 loss:1.456468
[Test] epoch:144	batch id:80	 loss:1.661637
[Test] epoch:144	batch id:90	 loss:1.514451
[Test] epoch:144	batch id:100	 loss:1.958654
[Test] epoch:144	batch id:110	 loss:1.403223
[Test] epoch:144	batch id:120	 loss:1.364350
[Test] epoch:144	batch id:130	 loss:1.362324
[Test] epoch:144	batch id:140	 loss:1.446920
[Test] epoch:144	batch id:150	 loss:1.822750
[Test] 144, loss: 1.510839, test acc: 0.906402,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:145	batch id:0	 lr:0.062814 loss:1.437474
[Train] epoch:145	batch id:10	 lr:0.062814 loss:1.424426
[Train] epoch:145	batch id:20	 lr:0.062814 loss:1.534309
[Train] epoch:145	batch id:30	 lr:0.062814 loss:1.503543
[Train] epoch:145	batch id:40	 lr:0.062814 loss:1.699279
[Train] epoch:145	batch id:50	 lr:0.062814 loss:1.586262
[Train] epoch:145	batch id:60	 lr:0.062814 loss:1.574794
[Train] epoch:145	batch id:70	 lr:0.062814 loss:1.568400
[Train] epoch:145	batch id:80	 lr:0.062814 loss:1.493872
[Train] epoch:145	batch id:90	 lr:0.062814 loss:1.433305
[Train] epoch:145	batch id:100	 lr:0.062814 loss:1.532293
[Train] epoch:145	batch id:110	 lr:0.062814 loss:1.432031
[Train] epoch:145	batch id:120	 lr:0.062814 loss:1.603870
[Train] epoch:145	batch id:130	 lr:0.062814 loss:1.520116
[Train] epoch:145	batch id:140	 lr:0.062814 loss:1.598896
[Train] epoch:145	batch id:150	 lr:0.062814 loss:1.549275
[Train] epoch:145	batch id:160	 lr:0.062814 loss:1.457328
[Train] epoch:145	batch id:170	 lr:0.062814 loss:1.559994
[Train] epoch:145	batch id:180	 lr:0.062814 loss:1.496835
[Train] epoch:145	batch id:190	 lr:0.062814 loss:1.382412
[Train] epoch:145	batch id:200	 lr:0.062814 loss:1.477474
[Train] epoch:145	batch id:210	 lr:0.062814 loss:1.510636
[Train] epoch:145	batch id:220	 lr:0.062814 loss:1.556016
[Train] epoch:145	batch id:230	 lr:0.062814 loss:1.548111
[Train] epoch:145	batch id:240	 lr:0.062814 loss:1.489482
[Train] epoch:145	batch id:250	 lr:0.062814 loss:1.468221
[Train] epoch:145	batch id:260	 lr:0.062814 loss:1.491017
[Train] epoch:145	batch id:270	 lr:0.062814 loss:1.541268
[Train] epoch:145	batch id:280	 lr:0.062814 loss:1.542060
[Train] epoch:145	batch id:290	 lr:0.062814 loss:1.767185
[Train] epoch:145	batch id:300	 lr:0.062814 loss:1.487684
[Train] 145, loss: 1.537369, train acc: 0.919076, 
[Test] epoch:145	batch id:0	 loss:1.338330
[Test] epoch:145	batch id:10	 loss:1.397053
[Test] epoch:145	batch id:20	 loss:1.406427
[Test] epoch:145	batch id:30	 loss:1.453262
[Test] epoch:145	batch id:40	 loss:1.349243
[Test] epoch:145	batch id:50	 loss:1.476244
[Test] epoch:145	batch id:60	 loss:1.305569
[Test] epoch:145	batch id:70	 loss:1.508062
[Test] epoch:145	batch id:80	 loss:1.504120
[Test] epoch:145	batch id:90	 loss:1.475295
[Test] epoch:145	batch id:100	 loss:2.116974
[Test] epoch:145	batch id:110	 loss:1.464398
[Test] epoch:145	batch id:120	 loss:1.390377
[Test] epoch:145	batch id:130	 loss:1.435725
[Test] epoch:145	batch id:140	 loss:1.372717
[Test] epoch:145	batch id:150	 loss:1.701650
[Test] 145, loss: 1.489837, test acc: 0.911264,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:146	batch id:0	 lr:0.062383 loss:1.418917
[Train] epoch:146	batch id:10	 lr:0.062383 loss:1.466194
[Train] epoch:146	batch id:20	 lr:0.062383 loss:1.415570
[Train] epoch:146	batch id:30	 lr:0.062383 loss:1.613743
[Train] epoch:146	batch id:40	 lr:0.062383 loss:1.462093
[Train] epoch:146	batch id:50	 lr:0.062383 loss:1.575746
[Train] epoch:146	batch id:60	 lr:0.062383 loss:1.585750
[Train] epoch:146	batch id:70	 lr:0.062383 loss:1.505520
[Train] epoch:146	batch id:80	 lr:0.062383 loss:1.715605
[Train] epoch:146	batch id:90	 lr:0.062383 loss:1.523138
[Train] epoch:146	batch id:100	 lr:0.062383 loss:1.479022
[Train] epoch:146	batch id:110	 lr:0.062383 loss:1.555224
[Train] epoch:146	batch id:120	 lr:0.062383 loss:1.525055
[Train] epoch:146	batch id:130	 lr:0.062383 loss:1.446474
[Train] epoch:146	batch id:140	 lr:0.062383 loss:1.526155
[Train] epoch:146	batch id:150	 lr:0.062383 loss:1.423987
[Train] epoch:146	batch id:160	 lr:0.062383 loss:1.382144
[Train] epoch:146	batch id:170	 lr:0.062383 loss:1.540481
[Train] epoch:146	batch id:180	 lr:0.062383 loss:1.545081
[Train] epoch:146	batch id:190	 lr:0.062383 loss:1.612564
[Train] epoch:146	batch id:200	 lr:0.062383 loss:1.487152
[Train] epoch:146	batch id:210	 lr:0.062383 loss:1.557098
[Train] epoch:146	batch id:220	 lr:0.062383 loss:1.655507
[Train] epoch:146	batch id:230	 lr:0.062383 loss:1.427198
[Train] epoch:146	batch id:240	 lr:0.062383 loss:1.599804
[Train] epoch:146	batch id:250	 lr:0.062383 loss:1.523785
[Train] epoch:146	batch id:260	 lr:0.062383 loss:1.501084
[Train] epoch:146	batch id:270	 lr:0.062383 loss:1.539669
[Train] epoch:146	batch id:280	 lr:0.062383 loss:1.509912
[Train] epoch:146	batch id:290	 lr:0.062383 loss:1.418755
[Train] epoch:146	batch id:300	 lr:0.062383 loss:1.568489
[Train] 146, loss: 1.536869, train acc: 0.918974, 
[Test] epoch:146	batch id:0	 loss:1.314711
[Test] epoch:146	batch id:10	 loss:1.478379
[Test] epoch:146	batch id:20	 loss:1.355449
[Test] epoch:146	batch id:30	 loss:1.504845
[Test] epoch:146	batch id:40	 loss:1.414277
[Test] epoch:146	batch id:50	 loss:1.411256
[Test] epoch:146	batch id:60	 loss:1.292801
[Test] epoch:146	batch id:70	 loss:1.490574
[Test] epoch:146	batch id:80	 loss:1.656449
[Test] epoch:146	batch id:90	 loss:1.521435
[Test] epoch:146	batch id:100	 loss:2.001810
[Test] epoch:146	batch id:110	 loss:1.402780
[Test] epoch:146	batch id:120	 loss:1.392608
[Test] epoch:146	batch id:130	 loss:1.445388
[Test] epoch:146	batch id:140	 loss:1.408660
[Test] epoch:146	batch id:150	 loss:1.760856
[Test] 146, loss: 1.513714, test acc: 0.902755,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:147	batch id:0	 lr:0.061951 loss:1.574613
[Train] epoch:147	batch id:10	 lr:0.061951 loss:1.450099
[Train] epoch:147	batch id:20	 lr:0.061951 loss:1.550876
[Train] epoch:147	batch id:30	 lr:0.061951 loss:1.502651
[Train] epoch:147	batch id:40	 lr:0.061951 loss:1.602745
[Train] epoch:147	batch id:50	 lr:0.061951 loss:1.464343
[Train] epoch:147	batch id:60	 lr:0.061951 loss:1.638818
[Train] epoch:147	batch id:70	 lr:0.061951 loss:1.468835
[Train] epoch:147	batch id:80	 lr:0.061951 loss:1.442928
[Train] epoch:147	batch id:90	 lr:0.061951 loss:1.572617
[Train] epoch:147	batch id:100	 lr:0.061951 loss:1.522224
[Train] epoch:147	batch id:110	 lr:0.061951 loss:1.577955
[Train] epoch:147	batch id:120	 lr:0.061951 loss:1.610448
[Train] epoch:147	batch id:130	 lr:0.061951 loss:1.539221
[Train] epoch:147	batch id:140	 lr:0.061951 loss:1.493563
[Train] epoch:147	batch id:150	 lr:0.061951 loss:1.548085
[Train] epoch:147	batch id:160	 lr:0.061951 loss:1.442390
[Train] epoch:147	batch id:170	 lr:0.061951 loss:1.690466
[Train] epoch:147	batch id:180	 lr:0.061951 loss:1.589066
[Train] epoch:147	batch id:190	 lr:0.061951 loss:1.467260
[Train] epoch:147	batch id:200	 lr:0.061951 loss:1.488853
[Train] epoch:147	batch id:210	 lr:0.061951 loss:1.567679
[Train] epoch:147	batch id:220	 lr:0.061951 loss:1.536407
[Train] epoch:147	batch id:230	 lr:0.061951 loss:1.706656
[Train] epoch:147	batch id:240	 lr:0.061951 loss:1.559202
[Train] epoch:147	batch id:250	 lr:0.061951 loss:1.583087
[Train] epoch:147	batch id:260	 lr:0.061951 loss:1.581040
[Train] epoch:147	batch id:270	 lr:0.061951 loss:1.481370
[Train] epoch:147	batch id:280	 lr:0.061951 loss:1.495341
[Train] epoch:147	batch id:290	 lr:0.061951 loss:1.604224
[Train] epoch:147	batch id:300	 lr:0.061951 loss:1.601940
[Train] 147, loss: 1.534478, train acc: 0.920704, 
[Test] epoch:147	batch id:0	 loss:1.321479
[Test] epoch:147	batch id:10	 loss:1.430331
[Test] epoch:147	batch id:20	 loss:1.435521
[Test] epoch:147	batch id:30	 loss:1.465316
[Test] epoch:147	batch id:40	 loss:1.354161
[Test] epoch:147	batch id:50	 loss:1.509800
[Test] epoch:147	batch id:60	 loss:1.293007
[Test] epoch:147	batch id:70	 loss:1.386349
[Test] epoch:147	batch id:80	 loss:1.606889
[Test] epoch:147	batch id:90	 loss:1.513195
[Test] epoch:147	batch id:100	 loss:1.880917
[Test] epoch:147	batch id:110	 loss:1.442291
[Test] epoch:147	batch id:120	 loss:1.337890
[Test] epoch:147	batch id:130	 loss:1.462480
[Test] epoch:147	batch id:140	 loss:1.454372
[Test] epoch:147	batch id:150	 loss:1.939728
[Test] 147, loss: 1.513909, test acc: 0.901540,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:148	batch id:0	 lr:0.061519 loss:1.481121
[Train] epoch:148	batch id:10	 lr:0.061519 loss:1.505356
[Train] epoch:148	batch id:20	 lr:0.061519 loss:1.474256
[Train] epoch:148	batch id:30	 lr:0.061519 loss:1.476750
[Train] epoch:148	batch id:40	 lr:0.061519 loss:1.618641
[Train] epoch:148	batch id:50	 lr:0.061519 loss:1.509585
[Train] epoch:148	batch id:60	 lr:0.061519 loss:1.472215
[Train] epoch:148	batch id:70	 lr:0.061519 loss:1.572327
[Train] epoch:148	batch id:80	 lr:0.061519 loss:1.532657
[Train] epoch:148	batch id:90	 lr:0.061519 loss:1.557574
[Train] epoch:148	batch id:100	 lr:0.061519 loss:1.531976
[Train] epoch:148	batch id:110	 lr:0.061519 loss:1.511374
[Train] epoch:148	batch id:120	 lr:0.061519 loss:1.481715
[Train] epoch:148	batch id:130	 lr:0.061519 loss:1.492602
[Train] epoch:148	batch id:140	 lr:0.061519 loss:1.456439
[Train] epoch:148	batch id:150	 lr:0.061519 loss:1.498954
[Train] epoch:148	batch id:160	 lr:0.061519 loss:1.522963
[Train] epoch:148	batch id:170	 lr:0.061519 loss:1.702050
[Train] epoch:148	batch id:180	 lr:0.061519 loss:1.519550
[Train] epoch:148	batch id:190	 lr:0.061519 loss:1.624164
[Train] epoch:148	batch id:200	 lr:0.061519 loss:1.653725
[Train] epoch:148	batch id:210	 lr:0.061519 loss:1.443333
[Train] epoch:148	batch id:220	 lr:0.061519 loss:1.590674
[Train] epoch:148	batch id:230	 lr:0.061519 loss:1.611134
[Train] epoch:148	batch id:240	 lr:0.061519 loss:1.612077
[Train] epoch:148	batch id:250	 lr:0.061519 loss:1.505504
[Train] epoch:148	batch id:260	 lr:0.061519 loss:1.457794
[Train] epoch:148	batch id:270	 lr:0.061519 loss:1.605247
[Train] epoch:148	batch id:280	 lr:0.061519 loss:1.463869
[Train] epoch:148	batch id:290	 lr:0.061519 loss:1.492048
[Train] epoch:148	batch id:300	 lr:0.061519 loss:1.476442
[Train] 148, loss: 1.539228, train acc: 0.918770, 
[Test] epoch:148	batch id:0	 loss:1.357724
[Test] epoch:148	batch id:10	 loss:1.498166
[Test] epoch:148	batch id:20	 loss:1.436797
[Test] epoch:148	batch id:30	 loss:1.512261
[Test] epoch:148	batch id:40	 loss:1.391680
[Test] epoch:148	batch id:50	 loss:1.458169
[Test] epoch:148	batch id:60	 loss:1.307051
[Test] epoch:148	batch id:70	 loss:1.490421
[Test] epoch:148	batch id:80	 loss:1.818507
[Test] epoch:148	batch id:90	 loss:1.653412
[Test] epoch:148	batch id:100	 loss:1.892729
[Test] epoch:148	batch id:110	 loss:1.461895
[Test] epoch:148	batch id:120	 loss:1.363384
[Test] epoch:148	batch id:130	 loss:1.496062
[Test] epoch:148	batch id:140	 loss:1.417748
[Test] epoch:148	batch id:150	 loss:1.689459
[Test] 148, loss: 1.529997, test acc: 0.897893,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:149	batch id:0	 lr:0.061085 loss:1.505177
[Train] epoch:149	batch id:10	 lr:0.061085 loss:1.428519
[Train] epoch:149	batch id:20	 lr:0.061085 loss:1.540590
[Train] epoch:149	batch id:30	 lr:0.061085 loss:1.461140
[Train] epoch:149	batch id:40	 lr:0.061085 loss:1.645356
[Train] epoch:149	batch id:50	 lr:0.061085 loss:1.599508
[Train] epoch:149	batch id:60	 lr:0.061085 loss:1.562044
[Train] epoch:149	batch id:70	 lr:0.061085 loss:1.722821
[Train] epoch:149	batch id:80	 lr:0.061085 loss:1.566748
[Train] epoch:149	batch id:90	 lr:0.061085 loss:1.501369
[Train] epoch:149	batch id:100	 lr:0.061085 loss:1.579343
[Train] epoch:149	batch id:110	 lr:0.061085 loss:1.591880
[Train] epoch:149	batch id:120	 lr:0.061085 loss:1.397513
[Train] epoch:149	batch id:130	 lr:0.061085 loss:1.533571
[Train] epoch:149	batch id:140	 lr:0.061085 loss:1.553287
[Train] epoch:149	batch id:150	 lr:0.061085 loss:1.537823
[Train] epoch:149	batch id:160	 lr:0.061085 loss:1.423694
[Train] epoch:149	batch id:170	 lr:0.061085 loss:1.517942
[Train] epoch:149	batch id:180	 lr:0.061085 loss:1.667036
[Train] epoch:149	batch id:190	 lr:0.061085 loss:1.499791
[Train] epoch:149	batch id:200	 lr:0.061085 loss:1.530108
[Train] epoch:149	batch id:210	 lr:0.061085 loss:1.568403
[Train] epoch:149	batch id:220	 lr:0.061085 loss:1.452583
[Train] epoch:149	batch id:230	 lr:0.061085 loss:1.741932
[Train] epoch:149	batch id:240	 lr:0.061085 loss:1.487379
[Train] epoch:149	batch id:250	 lr:0.061085 loss:1.476192
[Train] epoch:149	batch id:260	 lr:0.061085 loss:1.624885
[Train] epoch:149	batch id:270	 lr:0.061085 loss:1.434137
[Train] epoch:149	batch id:280	 lr:0.061085 loss:1.722903
[Train] epoch:149	batch id:290	 lr:0.061085 loss:1.402345
[Train] epoch:149	batch id:300	 lr:0.061085 loss:1.568022
[Train] 149, loss: 1.539171, train acc: 0.918160, 
[Test] epoch:149	batch id:0	 loss:1.374085
[Test] epoch:149	batch id:10	 loss:1.587937
[Test] epoch:149	batch id:20	 loss:1.389870
[Test] epoch:149	batch id:30	 loss:1.542202
[Test] epoch:149	batch id:40	 loss:1.362963
[Test] epoch:149	batch id:50	 loss:1.483302
[Test] epoch:149	batch id:60	 loss:1.325581
[Test] epoch:149	batch id:70	 loss:1.485417
[Test] epoch:149	batch id:80	 loss:1.671206
[Test] epoch:149	batch id:90	 loss:1.638952
[Test] epoch:149	batch id:100	 loss:2.027700
[Test] epoch:149	batch id:110	 loss:1.370381
[Test] epoch:149	batch id:120	 loss:1.355485
[Test] epoch:149	batch id:130	 loss:1.445114
[Test] epoch:149	batch id:140	 loss:1.403023
[Test] epoch:149	batch id:150	 loss:1.846210
[Test] 149, loss: 1.526383, test acc: 0.905592,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:150	batch id:0	 lr:0.060651 loss:1.701525
[Train] epoch:150	batch id:10	 lr:0.060651 loss:1.588868
[Train] epoch:150	batch id:20	 lr:0.060651 loss:1.635495
[Train] epoch:150	batch id:30	 lr:0.060651 loss:1.519643
[Train] epoch:150	batch id:40	 lr:0.060651 loss:1.554324
[Train] epoch:150	batch id:50	 lr:0.060651 loss:1.446862
[Train] epoch:150	batch id:60	 lr:0.060651 loss:1.600115
[Train] epoch:150	batch id:70	 lr:0.060651 loss:1.641764
[Train] epoch:150	batch id:80	 lr:0.060651 loss:1.501366
[Train] epoch:150	batch id:90	 lr:0.060651 loss:1.629526
[Train] epoch:150	batch id:100	 lr:0.060651 loss:1.584661
[Train] epoch:150	batch id:110	 lr:0.060651 loss:1.503096
[Train] epoch:150	batch id:120	 lr:0.060651 loss:1.643499
[Train] epoch:150	batch id:130	 lr:0.060651 loss:1.662850
[Train] epoch:150	batch id:140	 lr:0.060651 loss:1.494567
[Train] epoch:150	batch id:150	 lr:0.060651 loss:1.564027
[Train] epoch:150	batch id:160	 lr:0.060651 loss:1.492672
[Train] epoch:150	batch id:170	 lr:0.060651 loss:1.635364
[Train] epoch:150	batch id:180	 lr:0.060651 loss:1.456338
[Train] epoch:150	batch id:190	 lr:0.060651 loss:1.470988
[Train] epoch:150	batch id:200	 lr:0.060651 loss:1.490703
[Train] epoch:150	batch id:210	 lr:0.060651 loss:1.409490
[Train] epoch:150	batch id:220	 lr:0.060651 loss:1.477025
[Train] epoch:150	batch id:230	 lr:0.060651 loss:1.546707
[Train] epoch:150	batch id:240	 lr:0.060651 loss:1.553022
[Train] epoch:150	batch id:250	 lr:0.060651 loss:1.635740
[Train] epoch:150	batch id:260	 lr:0.060651 loss:1.498448
[Train] epoch:150	batch id:270	 lr:0.060651 loss:1.568011
[Train] epoch:150	batch id:280	 lr:0.060651 loss:1.657309
[Train] epoch:150	batch id:290	 lr:0.060651 loss:1.432940
[Train] epoch:150	batch id:300	 lr:0.060651 loss:1.570037
[Train] 150, loss: 1.542001, train acc: 0.915717, 
[Test] epoch:150	batch id:0	 loss:1.346510
[Test] epoch:150	batch id:10	 loss:1.568310
[Test] epoch:150	batch id:20	 loss:1.443374
[Test] epoch:150	batch id:30	 loss:1.441617
[Test] epoch:150	batch id:40	 loss:1.369052
[Test] epoch:150	batch id:50	 loss:1.561688
[Test] epoch:150	batch id:60	 loss:1.331311
[Test] epoch:150	batch id:70	 loss:1.485826
[Test] epoch:150	batch id:80	 loss:1.638602
[Test] epoch:150	batch id:90	 loss:1.486645
[Test] epoch:150	batch id:100	 loss:1.974595
[Test] epoch:150	batch id:110	 loss:1.423775
[Test] epoch:150	batch id:120	 loss:1.440995
[Test] epoch:150	batch id:130	 loss:1.527114
[Test] epoch:150	batch id:140	 loss:1.446645
[Test] epoch:150	batch id:150	 loss:1.814332
[Test] 150, loss: 1.526969, test acc: 0.902755,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:151	batch id:0	 lr:0.060215 loss:1.556865
[Train] epoch:151	batch id:10	 lr:0.060215 loss:1.478433
[Train] epoch:151	batch id:20	 lr:0.060215 loss:1.512009
[Train] epoch:151	batch id:30	 lr:0.060215 loss:1.615507
[Train] epoch:151	batch id:40	 lr:0.060215 loss:1.608654
[Train] epoch:151	batch id:50	 lr:0.060215 loss:1.455788
[Train] epoch:151	batch id:60	 lr:0.060215 loss:1.656188
[Train] epoch:151	batch id:70	 lr:0.060215 loss:1.566539
[Train] epoch:151	batch id:80	 lr:0.060215 loss:1.485192
[Train] epoch:151	batch id:90	 lr:0.060215 loss:1.522245
[Train] epoch:151	batch id:100	 lr:0.060215 loss:1.540360
[Train] epoch:151	batch id:110	 lr:0.060215 loss:1.445384
[Train] epoch:151	batch id:120	 lr:0.060215 loss:1.464342
[Train] epoch:151	batch id:130	 lr:0.060215 loss:1.742413
[Train] epoch:151	batch id:140	 lr:0.060215 loss:1.572100
[Train] epoch:151	batch id:150	 lr:0.060215 loss:1.691052
[Train] epoch:151	batch id:160	 lr:0.060215 loss:1.451975
[Train] epoch:151	batch id:170	 lr:0.060215 loss:1.616726
[Train] epoch:151	batch id:180	 lr:0.060215 loss:1.648256
[Train] epoch:151	batch id:190	 lr:0.060215 loss:1.655319
[Train] epoch:151	batch id:200	 lr:0.060215 loss:1.733394
[Train] epoch:151	batch id:210	 lr:0.060215 loss:1.629337
[Train] epoch:151	batch id:220	 lr:0.060215 loss:1.661054
[Train] epoch:151	batch id:230	 lr:0.060215 loss:1.527649
[Train] epoch:151	batch id:240	 lr:0.060215 loss:1.457806
[Train] epoch:151	batch id:250	 lr:0.060215 loss:1.658700
[Train] epoch:151	batch id:260	 lr:0.060215 loss:1.546063
[Train] epoch:151	batch id:270	 lr:0.060215 loss:1.678587
[Train] epoch:151	batch id:280	 lr:0.060215 loss:1.451179
[Train] epoch:151	batch id:290	 lr:0.060215 loss:1.457764
[Train] epoch:151	batch id:300	 lr:0.060215 loss:1.446203
[Train] 151, loss: 1.542732, train acc: 0.915920, 
[Test] epoch:151	batch id:0	 loss:1.372761
[Test] epoch:151	batch id:10	 loss:1.430955
[Test] epoch:151	batch id:20	 loss:1.455689
[Test] epoch:151	batch id:30	 loss:1.591134
[Test] epoch:151	batch id:40	 loss:1.370695
[Test] epoch:151	batch id:50	 loss:1.491093
[Test] epoch:151	batch id:60	 loss:1.310478
[Test] epoch:151	batch id:70	 loss:1.625173
[Test] epoch:151	batch id:80	 loss:1.629896
[Test] epoch:151	batch id:90	 loss:1.490744
[Test] epoch:151	batch id:100	 loss:1.868438
[Test] epoch:151	batch id:110	 loss:1.423424
[Test] epoch:151	batch id:120	 loss:1.382723
[Test] epoch:151	batch id:130	 loss:1.483061
[Test] epoch:151	batch id:140	 loss:1.338768
[Test] epoch:151	batch id:150	 loss:1.936821
[Test] 151, loss: 1.500213, test acc: 0.910859,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:152	batch id:0	 lr:0.059779 loss:1.500408
[Train] epoch:152	batch id:10	 lr:0.059779 loss:1.564260
[Train] epoch:152	batch id:20	 lr:0.059779 loss:1.525868
[Train] epoch:152	batch id:30	 lr:0.059779 loss:1.525250
[Train] epoch:152	batch id:40	 lr:0.059779 loss:1.418200
[Train] epoch:152	batch id:50	 lr:0.059779 loss:1.470660
[Train] epoch:152	batch id:60	 lr:0.059779 loss:1.641512
[Train] epoch:152	batch id:70	 lr:0.059779 loss:1.426786
[Train] epoch:152	batch id:80	 lr:0.059779 loss:1.842556
[Train] epoch:152	batch id:90	 lr:0.059779 loss:1.722591
[Train] epoch:152	batch id:100	 lr:0.059779 loss:1.478026
[Train] epoch:152	batch id:110	 lr:0.059779 loss:1.545767
[Train] epoch:152	batch id:120	 lr:0.059779 loss:1.618286
[Train] epoch:152	batch id:130	 lr:0.059779 loss:1.599705
[Train] epoch:152	batch id:140	 lr:0.059779 loss:1.532531
[Train] epoch:152	batch id:150	 lr:0.059779 loss:1.423951
[Train] epoch:152	batch id:160	 lr:0.059779 loss:1.463645
[Train] epoch:152	batch id:170	 lr:0.059779 loss:1.541539
[Train] epoch:152	batch id:180	 lr:0.059779 loss:1.423796
[Train] epoch:152	batch id:190	 lr:0.059779 loss:1.490418
[Train] epoch:152	batch id:200	 lr:0.059779 loss:1.480879
[Train] epoch:152	batch id:210	 lr:0.059779 loss:1.493106
[Train] epoch:152	batch id:220	 lr:0.059779 loss:1.494363
[Train] epoch:152	batch id:230	 lr:0.059779 loss:1.590927
[Train] epoch:152	batch id:240	 lr:0.059779 loss:1.500540
[Train] epoch:152	batch id:250	 lr:0.059779 loss:1.397055
[Train] epoch:152	batch id:260	 lr:0.059779 loss:1.530702
[Train] epoch:152	batch id:270	 lr:0.059779 loss:1.497659
[Train] epoch:152	batch id:280	 lr:0.059779 loss:1.590250
[Train] epoch:152	batch id:290	 lr:0.059779 loss:1.506992
[Train] epoch:152	batch id:300	 lr:0.059779 loss:1.499036
[Train] 152, loss: 1.528678, train acc: 0.924572, 
[Test] epoch:152	batch id:0	 loss:1.311225
[Test] epoch:152	batch id:10	 loss:1.468292
[Test] epoch:152	batch id:20	 loss:1.429332
[Test] epoch:152	batch id:30	 loss:1.453862
[Test] epoch:152	batch id:40	 loss:1.402677
[Test] epoch:152	batch id:50	 loss:1.431823
[Test] epoch:152	batch id:60	 loss:1.294464
[Test] epoch:152	batch id:70	 loss:1.412349
[Test] epoch:152	batch id:80	 loss:1.758433
[Test] epoch:152	batch id:90	 loss:1.465644
[Test] epoch:152	batch id:100	 loss:1.872695
[Test] epoch:152	batch id:110	 loss:1.338767
[Test] epoch:152	batch id:120	 loss:1.382275
[Test] epoch:152	batch id:130	 loss:1.396642
[Test] epoch:152	batch id:140	 loss:1.437575
[Test] epoch:152	batch id:150	 loss:1.722392
[Test] 152, loss: 1.513122, test acc: 0.907212,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:153	batch id:0	 lr:0.059343 loss:1.596788
[Train] epoch:153	batch id:10	 lr:0.059343 loss:1.464511
[Train] epoch:153	batch id:20	 lr:0.059343 loss:1.482427
[Train] epoch:153	batch id:30	 lr:0.059343 loss:1.408304
[Train] epoch:153	batch id:40	 lr:0.059343 loss:1.493625
[Train] epoch:153	batch id:50	 lr:0.059343 loss:1.493536
[Train] epoch:153	batch id:60	 lr:0.059343 loss:1.437261
[Train] epoch:153	batch id:70	 lr:0.059343 loss:1.519550
[Train] epoch:153	batch id:80	 lr:0.059343 loss:1.634472
[Train] epoch:153	batch id:90	 lr:0.059343 loss:1.537378
[Train] epoch:153	batch id:100	 lr:0.059343 loss:1.436609
[Train] epoch:153	batch id:110	 lr:0.059343 loss:1.655550
[Train] epoch:153	batch id:120	 lr:0.059343 loss:1.487673
[Train] epoch:153	batch id:130	 lr:0.059343 loss:1.440089
[Train] epoch:153	batch id:140	 lr:0.059343 loss:1.547118
[Train] epoch:153	batch id:150	 lr:0.059343 loss:1.632829
[Train] epoch:153	batch id:160	 lr:0.059343 loss:1.584019
[Train] epoch:153	batch id:170	 lr:0.059343 loss:1.526430
[Train] epoch:153	batch id:180	 lr:0.059343 loss:1.424718
[Train] epoch:153	batch id:190	 lr:0.059343 loss:1.500345
[Train] epoch:153	batch id:200	 lr:0.059343 loss:1.677009
[Train] epoch:153	batch id:210	 lr:0.059343 loss:1.472482
[Train] epoch:153	batch id:220	 lr:0.059343 loss:1.515878
[Train] epoch:153	batch id:230	 lr:0.059343 loss:1.453983
[Train] epoch:153	batch id:240	 lr:0.059343 loss:1.839433
[Train] epoch:153	batch id:250	 lr:0.059343 loss:1.514227
[Train] epoch:153	batch id:260	 lr:0.059343 loss:1.582599
[Train] epoch:153	batch id:270	 lr:0.059343 loss:1.503592
[Train] epoch:153	batch id:280	 lr:0.059343 loss:1.564620
[Train] epoch:153	batch id:290	 lr:0.059343 loss:1.471877
[Train] epoch:153	batch id:300	 lr:0.059343 loss:1.741757
[Train] 153, loss: 1.528960, train acc: 0.921824, 
[Test] epoch:153	batch id:0	 loss:1.346987
[Test] epoch:153	batch id:10	 loss:1.500159
[Test] epoch:153	batch id:20	 loss:1.345353
[Test] epoch:153	batch id:30	 loss:1.426436
[Test] epoch:153	batch id:40	 loss:1.378700
[Test] epoch:153	batch id:50	 loss:1.457842
[Test] epoch:153	batch id:60	 loss:1.307869
[Test] epoch:153	batch id:70	 loss:1.472482
[Test] epoch:153	batch id:80	 loss:1.785991
[Test] epoch:153	batch id:90	 loss:1.663439
[Test] epoch:153	batch id:100	 loss:1.955609
[Test] epoch:153	batch id:110	 loss:1.497179
[Test] epoch:153	batch id:120	 loss:1.367521
[Test] epoch:153	batch id:130	 loss:1.393945
[Test] epoch:153	batch id:140	 loss:1.392955
[Test] epoch:153	batch id:150	 loss:1.822972
[Test] 153, loss: 1.517754, test acc: 0.913290,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:154	batch id:0	 lr:0.058905 loss:1.483185
[Train] epoch:154	batch id:10	 lr:0.058905 loss:1.403497
[Train] epoch:154	batch id:20	 lr:0.058905 loss:1.363676
[Train] epoch:154	batch id:30	 lr:0.058905 loss:1.617909
[Train] epoch:154	batch id:40	 lr:0.058905 loss:1.626703
[Train] epoch:154	batch id:50	 lr:0.058905 loss:1.425332
[Train] epoch:154	batch id:60	 lr:0.058905 loss:1.443745
[Train] epoch:154	batch id:70	 lr:0.058905 loss:1.438250
[Train] epoch:154	batch id:80	 lr:0.058905 loss:1.668860
[Train] epoch:154	batch id:90	 lr:0.058905 loss:1.557301
[Train] epoch:154	batch id:100	 lr:0.058905 loss:1.457426
[Train] epoch:154	batch id:110	 lr:0.058905 loss:1.419641
[Train] epoch:154	batch id:120	 lr:0.058905 loss:1.505532
[Train] epoch:154	batch id:130	 lr:0.058905 loss:1.369315
[Train] epoch:154	batch id:140	 lr:0.058905 loss:1.458057
[Train] epoch:154	batch id:150	 lr:0.058905 loss:1.466242
[Train] epoch:154	batch id:160	 lr:0.058905 loss:1.585412
[Train] epoch:154	batch id:170	 lr:0.058905 loss:1.558327
[Train] epoch:154	batch id:180	 lr:0.058905 loss:1.443736
[Train] epoch:154	batch id:190	 lr:0.058905 loss:1.494522
[Train] epoch:154	batch id:200	 lr:0.058905 loss:1.521532
[Train] epoch:154	batch id:210	 lr:0.058905 loss:1.671195
[Train] epoch:154	batch id:220	 lr:0.058905 loss:1.605948
[Train] epoch:154	batch id:230	 lr:0.058905 loss:1.447809
[Train] epoch:154	batch id:240	 lr:0.058905 loss:1.541593
[Train] epoch:154	batch id:250	 lr:0.058905 loss:1.524477
[Train] epoch:154	batch id:260	 lr:0.058905 loss:1.514237
[Train] epoch:154	batch id:270	 lr:0.058905 loss:1.497827
[Train] epoch:154	batch id:280	 lr:0.058905 loss:1.481930
[Train] epoch:154	batch id:290	 lr:0.058905 loss:1.526118
[Train] epoch:154	batch id:300	 lr:0.058905 loss:1.378992
[Train] 154, loss: 1.522833, train acc: 0.924369, 
[Test] epoch:154	batch id:0	 loss:1.360649
[Test] epoch:154	batch id:10	 loss:1.546942
[Test] epoch:154	batch id:20	 loss:1.578583
[Test] epoch:154	batch id:30	 loss:1.607695
[Test] epoch:154	batch id:40	 loss:1.418100
[Test] epoch:154	batch id:50	 loss:1.484670
[Test] epoch:154	batch id:60	 loss:1.306844
[Test] epoch:154	batch id:70	 loss:1.378293
[Test] epoch:154	batch id:80	 loss:1.727308
[Test] epoch:154	batch id:90	 loss:1.605752
[Test] epoch:154	batch id:100	 loss:1.861342
[Test] epoch:154	batch id:110	 loss:1.538165
[Test] epoch:154	batch id:120	 loss:1.385817
[Test] epoch:154	batch id:130	 loss:1.550005
[Test] epoch:154	batch id:140	 loss:1.412718
[Test] epoch:154	batch id:150	 loss:1.762253
[Test] 154, loss: 1.548311, test acc: 0.893436,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:155	batch id:0	 lr:0.058467 loss:1.700686
[Train] epoch:155	batch id:10	 lr:0.058467 loss:1.483676
[Train] epoch:155	batch id:20	 lr:0.058467 loss:1.518552
[Train] epoch:155	batch id:30	 lr:0.058467 loss:1.427617
[Train] epoch:155	batch id:40	 lr:0.058467 loss:1.471541
[Train] epoch:155	batch id:50	 lr:0.058467 loss:1.560640
[Train] epoch:155	batch id:60	 lr:0.058467 loss:1.610082
[Train] epoch:155	batch id:70	 lr:0.058467 loss:1.550465
[Train] epoch:155	batch id:80	 lr:0.058467 loss:1.496974
[Train] epoch:155	batch id:90	 lr:0.058467 loss:1.552905
[Train] epoch:155	batch id:100	 lr:0.058467 loss:1.494343
[Train] epoch:155	batch id:110	 lr:0.058467 loss:1.502516
[Train] epoch:155	batch id:120	 lr:0.058467 loss:1.498217
[Train] epoch:155	batch id:130	 lr:0.058467 loss:1.659261
[Train] epoch:155	batch id:140	 lr:0.058467 loss:1.518182
[Train] epoch:155	batch id:150	 lr:0.058467 loss:1.381576
[Train] epoch:155	batch id:160	 lr:0.058467 loss:1.439123
[Train] epoch:155	batch id:170	 lr:0.058467 loss:1.619821
[Train] epoch:155	batch id:180	 lr:0.058467 loss:1.624280
[Train] epoch:155	batch id:190	 lr:0.058467 loss:1.600143
[Train] epoch:155	batch id:200	 lr:0.058467 loss:1.500402
[Train] epoch:155	batch id:210	 lr:0.058467 loss:1.754097
[Train] epoch:155	batch id:220	 lr:0.058467 loss:1.505959
[Train] epoch:155	batch id:230	 lr:0.058467 loss:1.727692
[Train] epoch:155	batch id:240	 lr:0.058467 loss:1.435850
[Train] epoch:155	batch id:250	 lr:0.058467 loss:1.559845
[Train] epoch:155	batch id:260	 lr:0.058467 loss:1.776632
[Train] epoch:155	batch id:270	 lr:0.058467 loss:1.580917
[Train] epoch:155	batch id:280	 lr:0.058467 loss:1.421918
[Train] epoch:155	batch id:290	 lr:0.058467 loss:1.662779
[Train] epoch:155	batch id:300	 lr:0.058467 loss:1.609327
[Train] 155, loss: 1.535755, train acc: 0.917345, 
[Test] epoch:155	batch id:0	 loss:1.325165
[Test] epoch:155	batch id:10	 loss:1.527874
[Test] epoch:155	batch id:20	 loss:1.438883
[Test] epoch:155	batch id:30	 loss:1.480599
[Test] epoch:155	batch id:40	 loss:1.416403
[Test] epoch:155	batch id:50	 loss:1.528799
[Test] epoch:155	batch id:60	 loss:1.292550
[Test] epoch:155	batch id:70	 loss:1.354163
[Test] epoch:155	batch id:80	 loss:1.586191
[Test] epoch:155	batch id:90	 loss:1.672807
[Test] epoch:155	batch id:100	 loss:1.839341
[Test] epoch:155	batch id:110	 loss:1.443728
[Test] epoch:155	batch id:120	 loss:1.355294
[Test] epoch:155	batch id:130	 loss:1.522305
[Test] epoch:155	batch id:140	 loss:1.337614
[Test] epoch:155	batch id:150	 loss:1.835508
[Test] 155, loss: 1.510967, test acc: 0.905997,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:156	batch id:0	 lr:0.058028 loss:1.451717
[Train] epoch:156	batch id:10	 lr:0.058028 loss:1.576708
[Train] epoch:156	batch id:20	 lr:0.058028 loss:1.542176
[Train] epoch:156	batch id:30	 lr:0.058028 loss:1.564288
[Train] epoch:156	batch id:40	 lr:0.058028 loss:1.429954
[Train] epoch:156	batch id:50	 lr:0.058028 loss:1.676177
[Train] epoch:156	batch id:60	 lr:0.058028 loss:1.437783
[Train] epoch:156	batch id:70	 lr:0.058028 loss:1.425174
[Train] epoch:156	batch id:80	 lr:0.058028 loss:1.558650
[Train] epoch:156	batch id:90	 lr:0.058028 loss:1.382879
[Train] epoch:156	batch id:100	 lr:0.058028 loss:1.395268
[Train] epoch:156	batch id:110	 lr:0.058028 loss:1.399012
[Train] epoch:156	batch id:120	 lr:0.058028 loss:1.544951
[Train] epoch:156	batch id:130	 lr:0.058028 loss:1.507721
[Train] epoch:156	batch id:140	 lr:0.058028 loss:1.618271
[Train] epoch:156	batch id:150	 lr:0.058028 loss:1.695556
[Train] epoch:156	batch id:160	 lr:0.058028 loss:1.480432
[Train] epoch:156	batch id:170	 lr:0.058028 loss:1.443720
[Train] epoch:156	batch id:180	 lr:0.058028 loss:1.397146
[Train] epoch:156	batch id:190	 lr:0.058028 loss:1.502602
[Train] epoch:156	batch id:200	 lr:0.058028 loss:1.491826
[Train] epoch:156	batch id:210	 lr:0.058028 loss:1.707880
[Train] epoch:156	batch id:220	 lr:0.058028 loss:1.712256
[Train] epoch:156	batch id:230	 lr:0.058028 loss:1.457991
[Train] epoch:156	batch id:240	 lr:0.058028 loss:1.470887
[Train] epoch:156	batch id:250	 lr:0.058028 loss:1.467567
[Train] epoch:156	batch id:260	 lr:0.058028 loss:1.485738
[Train] epoch:156	batch id:270	 lr:0.058028 loss:1.512766
[Train] epoch:156	batch id:280	 lr:0.058028 loss:1.418581
[Train] epoch:156	batch id:290	 lr:0.058028 loss:1.517616
[Train] epoch:156	batch id:300	 lr:0.058028 loss:1.694177
[Train] 156, loss: 1.523661, train acc: 0.926812, 
[Test] epoch:156	batch id:0	 loss:1.287670
[Test] epoch:156	batch id:10	 loss:1.362583
[Test] epoch:156	batch id:20	 loss:1.493533
[Test] epoch:156	batch id:30	 loss:1.389348
[Test] epoch:156	batch id:40	 loss:1.372635
[Test] epoch:156	batch id:50	 loss:1.434560
[Test] epoch:156	batch id:60	 loss:1.333759
[Test] epoch:156	batch id:70	 loss:1.420306
[Test] epoch:156	batch id:80	 loss:1.654424
[Test] epoch:156	batch id:90	 loss:1.579214
[Test] epoch:156	batch id:100	 loss:2.022630
[Test] epoch:156	batch id:110	 loss:1.406755
[Test] epoch:156	batch id:120	 loss:1.359400
[Test] epoch:156	batch id:130	 loss:1.389204
[Test] epoch:156	batch id:140	 loss:1.393128
[Test] epoch:156	batch id:150	 loss:2.001792
[Test] 156, loss: 1.506196, test acc: 0.905997,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:157	batch id:0	 lr:0.057589 loss:1.499296
[Train] epoch:157	batch id:10	 lr:0.057589 loss:1.441700
[Train] epoch:157	batch id:20	 lr:0.057589 loss:1.676666
[Train] epoch:157	batch id:30	 lr:0.057589 loss:1.392533
[Train] epoch:157	batch id:40	 lr:0.057589 loss:1.659051
[Train] epoch:157	batch id:50	 lr:0.057589 loss:1.487842
[Train] epoch:157	batch id:60	 lr:0.057589 loss:1.631508
[Train] epoch:157	batch id:70	 lr:0.057589 loss:1.590407
[Train] epoch:157	batch id:80	 lr:0.057589 loss:1.546115
[Train] epoch:157	batch id:90	 lr:0.057589 loss:1.515598
[Train] epoch:157	batch id:100	 lr:0.057589 loss:1.727506
[Train] epoch:157	batch id:110	 lr:0.057589 loss:1.666650
[Train] epoch:157	batch id:120	 lr:0.057589 loss:1.560615
[Train] epoch:157	batch id:130	 lr:0.057589 loss:1.613945
[Train] epoch:157	batch id:140	 lr:0.057589 loss:1.555165
[Train] epoch:157	batch id:150	 lr:0.057589 loss:1.616420
[Train] epoch:157	batch id:160	 lr:0.057589 loss:1.558657
[Train] epoch:157	batch id:170	 lr:0.057589 loss:1.407668
[Train] epoch:157	batch id:180	 lr:0.057589 loss:1.429752
[Train] epoch:157	batch id:190	 lr:0.057589 loss:1.504200
[Train] epoch:157	batch id:200	 lr:0.057589 loss:1.670988
[Train] epoch:157	batch id:210	 lr:0.057589 loss:1.536157
[Train] epoch:157	batch id:220	 lr:0.057589 loss:1.545340
[Train] epoch:157	batch id:230	 lr:0.057589 loss:1.772388
[Train] epoch:157	batch id:240	 lr:0.057589 loss:1.559779
[Train] epoch:157	batch id:250	 lr:0.057589 loss:1.517853
[Train] epoch:157	batch id:260	 lr:0.057589 loss:1.511470
[Train] epoch:157	batch id:270	 lr:0.057589 loss:1.546754
[Train] epoch:157	batch id:280	 lr:0.057589 loss:1.643891
[Train] epoch:157	batch id:290	 lr:0.057589 loss:1.550039
[Train] epoch:157	batch id:300	 lr:0.057589 loss:1.577444
[Train] 157, loss: 1.527439, train acc: 0.922129, 
[Test] epoch:157	batch id:0	 loss:1.310384
[Test] epoch:157	batch id:10	 loss:1.418908
[Test] epoch:157	batch id:20	 loss:1.429440
[Test] epoch:157	batch id:30	 loss:1.402089
[Test] epoch:157	batch id:40	 loss:1.437403
[Test] epoch:157	batch id:50	 loss:1.483313
[Test] epoch:157	batch id:60	 loss:1.309678
[Test] epoch:157	batch id:70	 loss:1.376128
[Test] epoch:157	batch id:80	 loss:1.718203
[Test] epoch:157	batch id:90	 loss:1.509703
[Test] epoch:157	batch id:100	 loss:1.930418
[Test] epoch:157	batch id:110	 loss:1.370571
[Test] epoch:157	batch id:120	 loss:1.364956
[Test] epoch:157	batch id:130	 loss:1.385970
[Test] epoch:157	batch id:140	 loss:1.352469
[Test] epoch:157	batch id:150	 loss:1.891315
[Test] 157, loss: 1.488923, test acc: 0.910049,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:158	batch id:0	 lr:0.057149 loss:1.566272
[Train] epoch:158	batch id:10	 lr:0.057149 loss:1.534348
[Train] epoch:158	batch id:20	 lr:0.057149 loss:1.674294
[Train] epoch:158	batch id:30	 lr:0.057149 loss:1.572534
[Train] epoch:158	batch id:40	 lr:0.057149 loss:1.606800
[Train] epoch:158	batch id:50	 lr:0.057149 loss:1.497286
[Train] epoch:158	batch id:60	 lr:0.057149 loss:1.498441
[Train] epoch:158	batch id:70	 lr:0.057149 loss:1.494134
[Train] epoch:158	batch id:80	 lr:0.057149 loss:1.557762
[Train] epoch:158	batch id:90	 lr:0.057149 loss:1.622681
[Train] epoch:158	batch id:100	 lr:0.057149 loss:1.490727
[Train] epoch:158	batch id:110	 lr:0.057149 loss:1.521534
[Train] epoch:158	batch id:120	 lr:0.057149 loss:1.508686
[Train] epoch:158	batch id:130	 lr:0.057149 loss:1.585117
[Train] epoch:158	batch id:140	 lr:0.057149 loss:1.542459
[Train] epoch:158	batch id:150	 lr:0.057149 loss:1.714983
[Train] epoch:158	batch id:160	 lr:0.057149 loss:1.547477
[Train] epoch:158	batch id:170	 lr:0.057149 loss:1.464106
[Train] epoch:158	batch id:180	 lr:0.057149 loss:1.459875
[Train] epoch:158	batch id:190	 lr:0.057149 loss:1.546267
[Train] epoch:158	batch id:200	 lr:0.057149 loss:1.454786
[Train] epoch:158	batch id:210	 lr:0.057149 loss:1.611613
[Train] epoch:158	batch id:220	 lr:0.057149 loss:1.509792
[Train] epoch:158	batch id:230	 lr:0.057149 loss:1.574800
[Train] epoch:158	batch id:240	 lr:0.057149 loss:1.538740
[Train] epoch:158	batch id:250	 lr:0.057149 loss:1.501161
[Train] epoch:158	batch id:260	 lr:0.057149 loss:1.519114
[Train] epoch:158	batch id:270	 lr:0.057149 loss:1.517317
[Train] epoch:158	batch id:280	 lr:0.057149 loss:1.576247
[Train] epoch:158	batch id:290	 lr:0.057149 loss:1.454034
[Train] epoch:158	batch id:300	 lr:0.057149 loss:1.635193
[Train] 158, loss: 1.521576, train acc: 0.923249, 
[Test] epoch:158	batch id:0	 loss:1.311221
[Test] epoch:158	batch id:10	 loss:1.475830
[Test] epoch:158	batch id:20	 loss:1.466636
[Test] epoch:158	batch id:30	 loss:1.354337
[Test] epoch:158	batch id:40	 loss:1.442913
[Test] epoch:158	batch id:50	 loss:1.468055
[Test] epoch:158	batch id:60	 loss:1.290311
[Test] epoch:158	batch id:70	 loss:1.392590
[Test] epoch:158	batch id:80	 loss:1.630068
[Test] epoch:158	batch id:90	 loss:1.447525
[Test] epoch:158	batch id:100	 loss:2.003870
[Test] epoch:158	batch id:110	 loss:1.342454
[Test] epoch:158	batch id:120	 loss:1.371974
[Test] epoch:158	batch id:130	 loss:1.346781
[Test] epoch:158	batch id:140	 loss:1.361695
[Test] epoch:158	batch id:150	 loss:1.787504
[Test] 158, loss: 1.480339, test acc: 0.918152,
Max Acc:0.918152
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:159	batch id:0	 lr:0.056708 loss:1.474829
[Train] epoch:159	batch id:10	 lr:0.056708 loss:1.600812
[Train] epoch:159	batch id:20	 lr:0.056708 loss:1.482352
[Train] epoch:159	batch id:30	 lr:0.056708 loss:1.535618
[Train] epoch:159	batch id:40	 lr:0.056708 loss:1.383725
[Train] epoch:159	batch id:50	 lr:0.056708 loss:1.482165
[Train] epoch:159	batch id:60	 lr:0.056708 loss:1.500885
[Train] epoch:159	batch id:70	 lr:0.056708 loss:1.546198
[Train] epoch:159	batch id:80	 lr:0.056708 loss:1.507921
[Train] epoch:159	batch id:90	 lr:0.056708 loss:1.527627
[Train] epoch:159	batch id:100	 lr:0.056708 loss:1.417325
[Train] epoch:159	batch id:110	 lr:0.056708 loss:1.643397
[Train] epoch:159	batch id:120	 lr:0.056708 loss:1.584540
[Train] epoch:159	batch id:130	 lr:0.056708 loss:1.497367
[Train] epoch:159	batch id:140	 lr:0.056708 loss:1.583460
[Train] epoch:159	batch id:150	 lr:0.056708 loss:1.555724
[Train] epoch:159	batch id:160	 lr:0.056708 loss:1.564366
[Train] epoch:159	batch id:170	 lr:0.056708 loss:1.548379
[Train] epoch:159	batch id:180	 lr:0.056708 loss:1.547283
[Train] epoch:159	batch id:190	 lr:0.056708 loss:1.437927
[Train] epoch:159	batch id:200	 lr:0.056708 loss:1.452818
[Train] epoch:159	batch id:210	 lr:0.056708 loss:1.611246
[Train] epoch:159	batch id:220	 lr:0.056708 loss:1.449642
[Train] epoch:159	batch id:230	 lr:0.056708 loss:1.573366
[Train] epoch:159	batch id:240	 lr:0.056708 loss:1.586222
[Train] epoch:159	batch id:250	 lr:0.056708 loss:1.532640
[Train] epoch:159	batch id:260	 lr:0.056708 loss:1.472739
[Train] epoch:159	batch id:270	 lr:0.056708 loss:1.444491
[Train] epoch:159	batch id:280	 lr:0.056708 loss:1.663295
[Train] epoch:159	batch id:290	 lr:0.056708 loss:1.545188
[Train] epoch:159	batch id:300	 lr:0.056708 loss:1.479641
[Train] 159, loss: 1.521536, train acc: 0.924878, 
[Test] epoch:159	batch id:0	 loss:1.314270
[Test] epoch:159	batch id:10	 loss:1.377240
[Test] epoch:159	batch id:20	 loss:1.393852
[Test] epoch:159	batch id:30	 loss:1.502519
[Test] epoch:159	batch id:40	 loss:1.396690
[Test] epoch:159	batch id:50	 loss:1.549770
[Test] epoch:159	batch id:60	 loss:1.296506
[Test] epoch:159	batch id:70	 loss:1.446559
[Test] epoch:159	batch id:80	 loss:1.679142
[Test] epoch:159	batch id:90	 loss:1.707018
[Test] epoch:159	batch id:100	 loss:2.025213
[Test] epoch:159	batch id:110	 loss:1.512163
[Test] epoch:159	batch id:120	 loss:1.478773
[Test] epoch:159	batch id:130	 loss:1.544850
[Test] epoch:159	batch id:140	 loss:1.384029
[Test] epoch:159	batch id:150	 loss:1.805756
[Test] 159, loss: 1.519505, test acc: 0.903160,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:160	batch id:0	 lr:0.056267 loss:1.769385
[Train] epoch:160	batch id:10	 lr:0.056267 loss:1.451200
[Train] epoch:160	batch id:20	 lr:0.056267 loss:1.596557
[Train] epoch:160	batch id:30	 lr:0.056267 loss:1.427215
[Train] epoch:160	batch id:40	 lr:0.056267 loss:1.516401
[Train] epoch:160	batch id:50	 lr:0.056267 loss:1.655962
[Train] epoch:160	batch id:60	 lr:0.056267 loss:1.488523
[Train] epoch:160	batch id:70	 lr:0.056267 loss:1.553366
[Train] epoch:160	batch id:80	 lr:0.056267 loss:1.598818
[Train] epoch:160	batch id:90	 lr:0.056267 loss:1.599990
[Train] epoch:160	batch id:100	 lr:0.056267 loss:1.583592
[Train] epoch:160	batch id:110	 lr:0.056267 loss:1.402931
[Train] epoch:160	batch id:120	 lr:0.056267 loss:1.438493
[Train] epoch:160	batch id:130	 lr:0.056267 loss:1.619217
[Train] epoch:160	batch id:140	 lr:0.056267 loss:1.460844
[Train] epoch:160	batch id:150	 lr:0.056267 loss:1.505688
[Train] epoch:160	batch id:160	 lr:0.056267 loss:1.670062
[Train] epoch:160	batch id:170	 lr:0.056267 loss:1.533949
[Train] epoch:160	batch id:180	 lr:0.056267 loss:1.514890
[Train] epoch:160	batch id:190	 lr:0.056267 loss:1.409623
[Train] epoch:160	batch id:200	 lr:0.056267 loss:1.552392
[Train] epoch:160	batch id:210	 lr:0.056267 loss:1.597075
[Train] epoch:160	batch id:220	 lr:0.056267 loss:1.534058
[Train] epoch:160	batch id:230	 lr:0.056267 loss:1.649980
[Train] epoch:160	batch id:240	 lr:0.056267 loss:1.675308
[Train] epoch:160	batch id:250	 lr:0.056267 loss:1.458675
[Train] epoch:160	batch id:260	 lr:0.056267 loss:1.504880
[Train] epoch:160	batch id:270	 lr:0.056267 loss:1.526961
[Train] epoch:160	batch id:280	 lr:0.056267 loss:1.549575
[Train] epoch:160	batch id:290	 lr:0.056267 loss:1.461257
[Train] epoch:160	batch id:300	 lr:0.056267 loss:1.574086
[Train] 160, loss: 1.517153, train acc: 0.926507, 
[Test] epoch:160	batch id:0	 loss:1.364969
[Test] epoch:160	batch id:10	 loss:1.388335
[Test] epoch:160	batch id:20	 loss:1.517735
[Test] epoch:160	batch id:30	 loss:1.579191
[Test] epoch:160	batch id:40	 loss:1.383876
[Test] epoch:160	batch id:50	 loss:1.531346
[Test] epoch:160	batch id:60	 loss:1.294177
[Test] epoch:160	batch id:70	 loss:1.532133
[Test] epoch:160	batch id:80	 loss:1.624041
[Test] epoch:160	batch id:90	 loss:1.655168
[Test] epoch:160	batch id:100	 loss:1.867351
[Test] epoch:160	batch id:110	 loss:1.452265
[Test] epoch:160	batch id:120	 loss:1.441093
[Test] epoch:160	batch id:130	 loss:1.446415
[Test] epoch:160	batch id:140	 loss:1.332484
[Test] epoch:160	batch id:150	 loss:1.925218
[Test] 160, loss: 1.527986, test acc: 0.891410,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:161	batch id:0	 lr:0.055825 loss:1.514943
[Train] epoch:161	batch id:10	 lr:0.055825 loss:1.480119
[Train] epoch:161	batch id:20	 lr:0.055825 loss:1.431857
[Train] epoch:161	batch id:30	 lr:0.055825 loss:1.608999
[Train] epoch:161	batch id:40	 lr:0.055825 loss:1.412107
[Train] epoch:161	batch id:50	 lr:0.055825 loss:1.461656
[Train] epoch:161	batch id:60	 lr:0.055825 loss:1.560565
[Train] epoch:161	batch id:70	 lr:0.055825 loss:1.515191
[Train] epoch:161	batch id:80	 lr:0.055825 loss:1.547879
[Train] epoch:161	batch id:90	 lr:0.055825 loss:1.494566
[Train] epoch:161	batch id:100	 lr:0.055825 loss:1.467010
[Train] epoch:161	batch id:110	 lr:0.055825 loss:1.618000
[Train] epoch:161	batch id:120	 lr:0.055825 loss:1.408060
[Train] epoch:161	batch id:130	 lr:0.055825 loss:1.452780
[Train] epoch:161	batch id:140	 lr:0.055825 loss:1.475598
[Train] epoch:161	batch id:150	 lr:0.055825 loss:1.463300
[Train] epoch:161	batch id:160	 lr:0.055825 loss:1.385644
[Train] epoch:161	batch id:170	 lr:0.055825 loss:1.577861
[Train] epoch:161	batch id:180	 lr:0.055825 loss:1.467832
[Train] epoch:161	batch id:190	 lr:0.055825 loss:1.581043
[Train] epoch:161	batch id:200	 lr:0.055825 loss:1.411485
[Train] epoch:161	batch id:210	 lr:0.055825 loss:1.618722
[Train] epoch:161	batch id:220	 lr:0.055825 loss:1.567618
[Train] epoch:161	batch id:230	 lr:0.055825 loss:1.445782
[Train] epoch:161	batch id:240	 lr:0.055825 loss:1.474170
[Train] epoch:161	batch id:250	 lr:0.055825 loss:1.568466
[Train] epoch:161	batch id:260	 lr:0.055825 loss:1.518077
[Train] epoch:161	batch id:270	 lr:0.055825 loss:1.495830
[Train] epoch:161	batch id:280	 lr:0.055825 loss:1.550246
[Train] epoch:161	batch id:290	 lr:0.055825 loss:1.429343
[Train] epoch:161	batch id:300	 lr:0.055825 loss:1.597374
[Train] 161, loss: 1.516885, train acc: 0.927728, 
[Test] epoch:161	batch id:0	 loss:1.301337
[Test] epoch:161	batch id:10	 loss:1.413964
[Test] epoch:161	batch id:20	 loss:1.342593
[Test] epoch:161	batch id:30	 loss:1.538004
[Test] epoch:161	batch id:40	 loss:1.359720
[Test] epoch:161	batch id:50	 loss:1.458012
[Test] epoch:161	batch id:60	 loss:1.282441
[Test] epoch:161	batch id:70	 loss:1.381437
[Test] epoch:161	batch id:80	 loss:1.602243
[Test] epoch:161	batch id:90	 loss:1.491219
[Test] epoch:161	batch id:100	 loss:1.871318
[Test] epoch:161	batch id:110	 loss:1.404250
[Test] epoch:161	batch id:120	 loss:1.384349
[Test] epoch:161	batch id:130	 loss:1.411338
[Test] epoch:161	batch id:140	 loss:1.370328
[Test] epoch:161	batch id:150	 loss:1.780180
[Test] 161, loss: 1.487081, test acc: 0.912480,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:162	batch id:0	 lr:0.055383 loss:1.400299
[Train] epoch:162	batch id:10	 lr:0.055383 loss:1.391106
[Train] epoch:162	batch id:20	 lr:0.055383 loss:1.481760
[Train] epoch:162	batch id:30	 lr:0.055383 loss:1.460562
[Train] epoch:162	batch id:40	 lr:0.055383 loss:1.516786
[Train] epoch:162	batch id:50	 lr:0.055383 loss:1.497265
[Train] epoch:162	batch id:60	 lr:0.055383 loss:1.511220
[Train] epoch:162	batch id:70	 lr:0.055383 loss:1.481636
[Train] epoch:162	batch id:80	 lr:0.055383 loss:1.474608
[Train] epoch:162	batch id:90	 lr:0.055383 loss:1.407602
[Train] epoch:162	batch id:100	 lr:0.055383 loss:1.539948
[Train] epoch:162	batch id:110	 lr:0.055383 loss:1.409064
[Train] epoch:162	batch id:120	 lr:0.055383 loss:1.524912
[Train] epoch:162	batch id:130	 lr:0.055383 loss:1.411214
[Train] epoch:162	batch id:140	 lr:0.055383 loss:1.589092
[Train] epoch:162	batch id:150	 lr:0.055383 loss:1.455145
[Train] epoch:162	batch id:160	 lr:0.055383 loss:1.463834
[Train] epoch:162	batch id:170	 lr:0.055383 loss:1.445774
[Train] epoch:162	batch id:180	 lr:0.055383 loss:1.397052
[Train] epoch:162	batch id:190	 lr:0.055383 loss:1.513810
[Train] epoch:162	batch id:200	 lr:0.055383 loss:1.574874
[Train] epoch:162	batch id:210	 lr:0.055383 loss:1.512879
[Train] epoch:162	batch id:220	 lr:0.055383 loss:1.471065
[Train] epoch:162	batch id:230	 lr:0.055383 loss:1.556464
[Train] epoch:162	batch id:240	 lr:0.055383 loss:1.569389
[Train] epoch:162	batch id:250	 lr:0.055383 loss:1.548402
[Train] epoch:162	batch id:260	 lr:0.055383 loss:1.529464
[Train] epoch:162	batch id:270	 lr:0.055383 loss:1.515254
[Train] epoch:162	batch id:280	 lr:0.055383 loss:1.436976
[Train] epoch:162	batch id:290	 lr:0.055383 loss:1.520763
[Train] epoch:162	batch id:300	 lr:0.055383 loss:1.545041
[Train] 162, loss: 1.521190, train acc: 0.926710, 
[Test] epoch:162	batch id:0	 loss:1.297695
[Test] epoch:162	batch id:10	 loss:1.350486
[Test] epoch:162	batch id:20	 loss:1.443500
[Test] epoch:162	batch id:30	 loss:1.395319
[Test] epoch:162	batch id:40	 loss:1.368556
[Test] epoch:162	batch id:50	 loss:1.457205
[Test] epoch:162	batch id:60	 loss:1.294535
[Test] epoch:162	batch id:70	 loss:1.396959
[Test] epoch:162	batch id:80	 loss:1.610398
[Test] epoch:162	batch id:90	 loss:1.564936
[Test] epoch:162	batch id:100	 loss:2.005679
[Test] epoch:162	batch id:110	 loss:1.466725
[Test] epoch:162	batch id:120	 loss:1.414041
[Test] epoch:162	batch id:130	 loss:1.418398
[Test] epoch:162	batch id:140	 loss:1.332264
[Test] epoch:162	batch id:150	 loss:1.681506
[Test] 162, loss: 1.485862, test acc: 0.917747,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:163	batch id:0	 lr:0.054941 loss:1.543494
[Train] epoch:163	batch id:10	 lr:0.054941 loss:1.459769
[Train] epoch:163	batch id:20	 lr:0.054941 loss:1.648003
[Train] epoch:163	batch id:30	 lr:0.054941 loss:1.496608
[Train] epoch:163	batch id:40	 lr:0.054941 loss:1.506514
[Train] epoch:163	batch id:50	 lr:0.054941 loss:1.548135
[Train] epoch:163	batch id:60	 lr:0.054941 loss:1.470882
[Train] epoch:163	batch id:70	 lr:0.054941 loss:1.440298
[Train] epoch:163	batch id:80	 lr:0.054941 loss:1.591178
[Train] epoch:163	batch id:90	 lr:0.054941 loss:1.584356
[Train] epoch:163	batch id:100	 lr:0.054941 loss:1.522360
[Train] epoch:163	batch id:110	 lr:0.054941 loss:1.558203
[Train] epoch:163	batch id:120	 lr:0.054941 loss:1.503747
[Train] epoch:163	batch id:130	 lr:0.054941 loss:1.703317
[Train] epoch:163	batch id:140	 lr:0.054941 loss:1.531325
[Train] epoch:163	batch id:150	 lr:0.054941 loss:1.468638
[Train] epoch:163	batch id:160	 lr:0.054941 loss:1.543802
[Train] epoch:163	batch id:170	 lr:0.054941 loss:1.464353
[Train] epoch:163	batch id:180	 lr:0.054941 loss:1.481958
[Train] epoch:163	batch id:190	 lr:0.054941 loss:1.559399
[Train] epoch:163	batch id:200	 lr:0.054941 loss:1.418224
[Train] epoch:163	batch id:210	 lr:0.054941 loss:1.544574
[Train] epoch:163	batch id:220	 lr:0.054941 loss:1.498675
[Train] epoch:163	batch id:230	 lr:0.054941 loss:1.450047
[Train] epoch:163	batch id:240	 lr:0.054941 loss:1.374267
[Train] epoch:163	batch id:250	 lr:0.054941 loss:1.714410
[Train] epoch:163	batch id:260	 lr:0.054941 loss:1.640913
[Train] epoch:163	batch id:270	 lr:0.054941 loss:1.503069
[Train] epoch:163	batch id:280	 lr:0.054941 loss:1.475299
[Train] epoch:163	batch id:290	 lr:0.054941 loss:1.525741
[Train] epoch:163	batch id:300	 lr:0.054941 loss:1.591041
[Train] 163, loss: 1.512076, train acc: 0.930680, 
[Test] epoch:163	batch id:0	 loss:1.344797
[Test] epoch:163	batch id:10	 loss:1.350282
[Test] epoch:163	batch id:20	 loss:1.392294
[Test] epoch:163	batch id:30	 loss:1.428684
[Test] epoch:163	batch id:40	 loss:1.436505
[Test] epoch:163	batch id:50	 loss:1.395202
[Test] epoch:163	batch id:60	 loss:1.303032
[Test] epoch:163	batch id:70	 loss:1.419337
[Test] epoch:163	batch id:80	 loss:1.644457
[Test] epoch:163	batch id:90	 loss:1.575726
[Test] epoch:163	batch id:100	 loss:1.916722
[Test] epoch:163	batch id:110	 loss:1.363716
[Test] epoch:163	batch id:120	 loss:1.344777
[Test] epoch:163	batch id:130	 loss:1.441545
[Test] epoch:163	batch id:140	 loss:1.361869
[Test] epoch:163	batch id:150	 loss:1.963317
[Test] 163, loss: 1.500681, test acc: 0.910049,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:164	batch id:0	 lr:0.054498 loss:1.420399
[Train] epoch:164	batch id:10	 lr:0.054498 loss:1.585070
[Train] epoch:164	batch id:20	 lr:0.054498 loss:1.528096
[Train] epoch:164	batch id:30	 lr:0.054498 loss:1.564238
[Train] epoch:164	batch id:40	 lr:0.054498 loss:1.464555
[Train] epoch:164	batch id:50	 lr:0.054498 loss:1.467386
[Train] epoch:164	batch id:60	 lr:0.054498 loss:1.430813
[Train] epoch:164	batch id:70	 lr:0.054498 loss:1.576027
[Train] epoch:164	batch id:80	 lr:0.054498 loss:1.497786
[Train] epoch:164	batch id:90	 lr:0.054498 loss:1.510173
[Train] epoch:164	batch id:100	 lr:0.054498 loss:1.411356
[Train] epoch:164	batch id:110	 lr:0.054498 loss:1.522669
[Train] epoch:164	batch id:120	 lr:0.054498 loss:1.422479
[Train] epoch:164	batch id:130	 lr:0.054498 loss:1.523717
[Train] epoch:164	batch id:140	 lr:0.054498 loss:1.433278
[Train] epoch:164	batch id:150	 lr:0.054498 loss:1.493505
[Train] epoch:164	batch id:160	 lr:0.054498 loss:1.429414
[Train] epoch:164	batch id:170	 lr:0.054498 loss:1.556092
[Train] epoch:164	batch id:180	 lr:0.054498 loss:1.428380
[Train] epoch:164	batch id:190	 lr:0.054498 loss:1.445137
[Train] epoch:164	batch id:200	 lr:0.054498 loss:1.520550
[Train] epoch:164	batch id:210	 lr:0.054498 loss:1.594376
[Train] epoch:164	batch id:220	 lr:0.054498 loss:1.636571
[Train] epoch:164	batch id:230	 lr:0.054498 loss:1.406560
[Train] epoch:164	batch id:240	 lr:0.054498 loss:1.412587
[Train] epoch:164	batch id:250	 lr:0.054498 loss:1.462470
[Train] epoch:164	batch id:260	 lr:0.054498 loss:1.615839
[Train] epoch:164	batch id:270	 lr:0.054498 loss:1.541482
[Train] epoch:164	batch id:280	 lr:0.054498 loss:1.403306
[Train] epoch:164	batch id:290	 lr:0.054498 loss:1.416940
[Train] epoch:164	batch id:300	 lr:0.054498 loss:1.514379
[Train] 164, loss: 1.523513, train acc: 0.924980, 
[Test] epoch:164	batch id:0	 loss:1.361947
[Test] epoch:164	batch id:10	 loss:1.348627
[Test] epoch:164	batch id:20	 loss:1.432461
[Test] epoch:164	batch id:30	 loss:1.682452
[Test] epoch:164	batch id:40	 loss:1.436744
[Test] epoch:164	batch id:50	 loss:1.480631
[Test] epoch:164	batch id:60	 loss:1.298821
[Test] epoch:164	batch id:70	 loss:1.483107
[Test] epoch:164	batch id:80	 loss:1.629189
[Test] epoch:164	batch id:90	 loss:1.616134
[Test] epoch:164	batch id:100	 loss:1.802311
[Test] epoch:164	batch id:110	 loss:1.356736
[Test] epoch:164	batch id:120	 loss:1.406342
[Test] epoch:164	batch id:130	 loss:1.514729
[Test] epoch:164	batch id:140	 loss:1.310034
[Test] epoch:164	batch id:150	 loss:1.998971
[Test] 164, loss: 1.510427, test acc: 0.905997,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:165	batch id:0	 lr:0.054055 loss:1.489744
[Train] epoch:165	batch id:10	 lr:0.054055 loss:1.449121
[Train] epoch:165	batch id:20	 lr:0.054055 loss:1.496425
[Train] epoch:165	batch id:30	 lr:0.054055 loss:1.556916
[Train] epoch:165	batch id:40	 lr:0.054055 loss:1.536162
[Train] epoch:165	batch id:50	 lr:0.054055 loss:1.551436
[Train] epoch:165	batch id:60	 lr:0.054055 loss:1.413526
[Train] epoch:165	batch id:70	 lr:0.054055 loss:1.437824
[Train] epoch:165	batch id:80	 lr:0.054055 loss:1.593525
[Train] epoch:165	batch id:90	 lr:0.054055 loss:1.660606
[Train] epoch:165	batch id:100	 lr:0.054055 loss:1.470068
[Train] epoch:165	batch id:110	 lr:0.054055 loss:1.489599
[Train] epoch:165	batch id:120	 lr:0.054055 loss:1.499506
[Train] epoch:165	batch id:130	 lr:0.054055 loss:1.504665
[Train] epoch:165	batch id:140	 lr:0.054055 loss:1.608146
[Train] epoch:165	batch id:150	 lr:0.054055 loss:1.405762
[Train] epoch:165	batch id:160	 lr:0.054055 loss:1.465473
[Train] epoch:165	batch id:170	 lr:0.054055 loss:1.598272
[Train] epoch:165	batch id:180	 lr:0.054055 loss:1.477722
[Train] epoch:165	batch id:190	 lr:0.054055 loss:1.578236
[Train] epoch:165	batch id:200	 lr:0.054055 loss:1.487227
[Train] epoch:165	batch id:210	 lr:0.054055 loss:1.502154
[Train] epoch:165	batch id:220	 lr:0.054055 loss:1.416505
[Train] epoch:165	batch id:230	 lr:0.054055 loss:1.364460
[Train] epoch:165	batch id:240	 lr:0.054055 loss:1.521333
[Train] epoch:165	batch id:250	 lr:0.054055 loss:1.377213
[Train] epoch:165	batch id:260	 lr:0.054055 loss:1.576247
[Train] epoch:165	batch id:270	 lr:0.054055 loss:1.427093
[Train] epoch:165	batch id:280	 lr:0.054055 loss:1.480529
[Train] epoch:165	batch id:290	 lr:0.054055 loss:1.486400
[Train] epoch:165	batch id:300	 lr:0.054055 loss:1.463359
[Train] 165, loss: 1.515982, train acc: 0.927321, 
[Test] epoch:165	batch id:0	 loss:1.315185
[Test] epoch:165	batch id:10	 loss:1.379609
[Test] epoch:165	batch id:20	 loss:1.502406
[Test] epoch:165	batch id:30	 loss:1.494800
[Test] epoch:165	batch id:40	 loss:1.371570
[Test] epoch:165	batch id:50	 loss:1.570532
[Test] epoch:165	batch id:60	 loss:1.292524
[Test] epoch:165	batch id:70	 loss:1.519569
[Test] epoch:165	batch id:80	 loss:1.591475
[Test] epoch:165	batch id:90	 loss:1.547257
[Test] epoch:165	batch id:100	 loss:1.854483
[Test] epoch:165	batch id:110	 loss:1.377602
[Test] epoch:165	batch id:120	 loss:1.523293
[Test] epoch:165	batch id:130	 loss:1.484411
[Test] epoch:165	batch id:140	 loss:1.457046
[Test] epoch:165	batch id:150	 loss:1.843793
[Test] 165, loss: 1.502432, test acc: 0.898703,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:166	batch id:0	 lr:0.053612 loss:1.587028
[Train] epoch:166	batch id:10	 lr:0.053612 loss:1.467893
[Train] epoch:166	batch id:20	 lr:0.053612 loss:1.411703
[Train] epoch:166	batch id:30	 lr:0.053612 loss:1.521162
[Train] epoch:166	batch id:40	 lr:0.053612 loss:1.477556
[Train] epoch:166	batch id:50	 lr:0.053612 loss:1.539941
[Train] epoch:166	batch id:60	 lr:0.053612 loss:1.524039
[Train] epoch:166	batch id:70	 lr:0.053612 loss:1.436954
[Train] epoch:166	batch id:80	 lr:0.053612 loss:1.504396
[Train] epoch:166	batch id:90	 lr:0.053612 loss:1.484545
[Train] epoch:166	batch id:100	 lr:0.053612 loss:1.491811
[Train] epoch:166	batch id:110	 lr:0.053612 loss:1.518963
[Train] epoch:166	batch id:120	 lr:0.053612 loss:1.501358
[Train] epoch:166	batch id:130	 lr:0.053612 loss:1.476752
[Train] epoch:166	batch id:140	 lr:0.053612 loss:1.394178
[Train] epoch:166	batch id:150	 lr:0.053612 loss:1.417851
[Train] epoch:166	batch id:160	 lr:0.053612 loss:1.519379
[Train] epoch:166	batch id:170	 lr:0.053612 loss:1.495969
[Train] epoch:166	batch id:180	 lr:0.053612 loss:1.654753
[Train] epoch:166	batch id:190	 lr:0.053612 loss:1.490283
[Train] epoch:166	batch id:200	 lr:0.053612 loss:1.489141
[Train] epoch:166	batch id:210	 lr:0.053612 loss:1.561255
[Train] epoch:166	batch id:220	 lr:0.053612 loss:1.554501
[Train] epoch:166	batch id:230	 lr:0.053612 loss:1.437407
[Train] epoch:166	batch id:240	 lr:0.053612 loss:1.517381
[Train] epoch:166	batch id:250	 lr:0.053612 loss:1.473030
[Train] epoch:166	batch id:260	 lr:0.053612 loss:1.422965
[Train] epoch:166	batch id:270	 lr:0.053612 loss:1.564640
[Train] epoch:166	batch id:280	 lr:0.053612 loss:1.443215
[Train] epoch:166	batch id:290	 lr:0.053612 loss:1.496330
[Train] epoch:166	batch id:300	 lr:0.053612 loss:1.464867
[Train] 166, loss: 1.509550, train acc: 0.929764, 
[Test] epoch:166	batch id:0	 loss:1.322789
[Test] epoch:166	batch id:10	 loss:1.581711
[Test] epoch:166	batch id:20	 loss:1.364530
[Test] epoch:166	batch id:30	 loss:1.404683
[Test] epoch:166	batch id:40	 loss:1.431304
[Test] epoch:166	batch id:50	 loss:1.517421
[Test] epoch:166	batch id:60	 loss:1.294461
[Test] epoch:166	batch id:70	 loss:1.451967
[Test] epoch:166	batch id:80	 loss:1.637379
[Test] epoch:166	batch id:90	 loss:1.670405
[Test] epoch:166	batch id:100	 loss:1.883194
[Test] epoch:166	batch id:110	 loss:1.402740
[Test] epoch:166	batch id:120	 loss:1.428375
[Test] epoch:166	batch id:130	 loss:1.346958
[Test] epoch:166	batch id:140	 loss:1.478574
[Test] epoch:166	batch id:150	 loss:1.853378
[Test] 166, loss: 1.510965, test acc: 0.897893,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:167	batch id:0	 lr:0.053169 loss:1.603217
[Train] epoch:167	batch id:10	 lr:0.053169 loss:1.437503
[Train] epoch:167	batch id:20	 lr:0.053169 loss:1.466343
[Train] epoch:167	batch id:30	 lr:0.053169 loss:1.532648
[Train] epoch:167	batch id:40	 lr:0.053169 loss:1.472209
[Train] epoch:167	batch id:50	 lr:0.053169 loss:1.437583
[Train] epoch:167	batch id:60	 lr:0.053169 loss:1.570280
[Train] epoch:167	batch id:70	 lr:0.053169 loss:1.607160
[Train] epoch:167	batch id:80	 lr:0.053169 loss:1.517649
[Train] epoch:167	batch id:90	 lr:0.053169 loss:1.508947
[Train] epoch:167	batch id:100	 lr:0.053169 loss:1.535857
[Train] epoch:167	batch id:110	 lr:0.053169 loss:1.412748
[Train] epoch:167	batch id:120	 lr:0.053169 loss:1.641662
[Train] epoch:167	batch id:130	 lr:0.053169 loss:1.535536
[Train] epoch:167	batch id:140	 lr:0.053169 loss:1.539918
[Train] epoch:167	batch id:150	 lr:0.053169 loss:1.431371
[Train] epoch:167	batch id:160	 lr:0.053169 loss:1.583740
[Train] epoch:167	batch id:170	 lr:0.053169 loss:1.531868
[Train] epoch:167	batch id:180	 lr:0.053169 loss:1.534617
[Train] epoch:167	batch id:190	 lr:0.053169 loss:1.458768
[Train] epoch:167	batch id:200	 lr:0.053169 loss:1.555750
[Train] epoch:167	batch id:210	 lr:0.053169 loss:1.598075
[Train] epoch:167	batch id:220	 lr:0.053169 loss:1.468791
[Train] epoch:167	batch id:230	 lr:0.053169 loss:1.693188
[Train] epoch:167	batch id:240	 lr:0.053169 loss:1.603202
[Train] epoch:167	batch id:250	 lr:0.053169 loss:1.408504
[Train] epoch:167	batch id:260	 lr:0.053169 loss:1.565273
[Train] epoch:167	batch id:270	 lr:0.053169 loss:1.449485
[Train] epoch:167	batch id:280	 lr:0.053169 loss:1.498341
[Train] epoch:167	batch id:290	 lr:0.053169 loss:1.511524
[Train] epoch:167	batch id:300	 lr:0.053169 loss:1.574084
[Train] 167, loss: 1.505905, train acc: 0.933123, 
[Test] epoch:167	batch id:0	 loss:1.335539
[Test] epoch:167	batch id:10	 loss:1.492404
[Test] epoch:167	batch id:20	 loss:1.482653
[Test] epoch:167	batch id:30	 loss:1.562948
[Test] epoch:167	batch id:40	 loss:1.371146
[Test] epoch:167	batch id:50	 loss:1.535790
[Test] epoch:167	batch id:60	 loss:1.318718
[Test] epoch:167	batch id:70	 loss:1.333992
[Test] epoch:167	batch id:80	 loss:1.774527
[Test] epoch:167	batch id:90	 loss:1.582587
[Test] epoch:167	batch id:100	 loss:2.098393
[Test] epoch:167	batch id:110	 loss:1.455600
[Test] epoch:167	batch id:120	 loss:1.381844
[Test] epoch:167	batch id:130	 loss:1.501283
[Test] epoch:167	batch id:140	 loss:1.382594
[Test] epoch:167	batch id:150	 loss:1.832391
[Test] 167, loss: 1.514761, test acc: 0.908833,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:168	batch id:0	 lr:0.052725 loss:1.476647
[Train] epoch:168	batch id:10	 lr:0.052725 loss:1.572014
[Train] epoch:168	batch id:20	 lr:0.052725 loss:1.515590
[Train] epoch:168	batch id:30	 lr:0.052725 loss:1.585690
[Train] epoch:168	batch id:40	 lr:0.052725 loss:1.501248
[Train] epoch:168	batch id:50	 lr:0.052725 loss:1.426255
[Train] epoch:168	batch id:60	 lr:0.052725 loss:1.493494
[Train] epoch:168	batch id:70	 lr:0.052725 loss:1.609003
[Train] epoch:168	batch id:80	 lr:0.052725 loss:1.456868
[Train] epoch:168	batch id:90	 lr:0.052725 loss:1.471047
[Train] epoch:168	batch id:100	 lr:0.052725 loss:1.595967
[Train] epoch:168	batch id:110	 lr:0.052725 loss:1.413609
[Train] epoch:168	batch id:120	 lr:0.052725 loss:1.468977
[Train] epoch:168	batch id:130	 lr:0.052725 loss:1.575894
[Train] epoch:168	batch id:140	 lr:0.052725 loss:1.539963
[Train] epoch:168	batch id:150	 lr:0.052725 loss:1.513087
[Train] epoch:168	batch id:160	 lr:0.052725 loss:1.487216
[Train] epoch:168	batch id:170	 lr:0.052725 loss:1.454779
[Train] epoch:168	batch id:180	 lr:0.052725 loss:1.437527
[Train] epoch:168	batch id:190	 lr:0.052725 loss:1.486596
[Train] epoch:168	batch id:200	 lr:0.052725 loss:1.657403
[Train] epoch:168	batch id:210	 lr:0.052725 loss:1.448094
[Train] epoch:168	batch id:220	 lr:0.052725 loss:1.462960
[Train] epoch:168	batch id:230	 lr:0.052725 loss:1.616559
[Train] epoch:168	batch id:240	 lr:0.052725 loss:1.447218
[Train] epoch:168	batch id:250	 lr:0.052725 loss:1.380495
[Train] epoch:168	batch id:260	 lr:0.052725 loss:1.508273
[Train] epoch:168	batch id:270	 lr:0.052725 loss:1.512413
[Train] epoch:168	batch id:280	 lr:0.052725 loss:1.409005
[Train] epoch:168	batch id:290	 lr:0.052725 loss:1.374548
[Train] epoch:168	batch id:300	 lr:0.052725 loss:1.535845
[Train] 168, loss: 1.505562, train acc: 0.934039, 
[Test] epoch:168	batch id:0	 loss:1.296645
[Test] epoch:168	batch id:10	 loss:1.357503
[Test] epoch:168	batch id:20	 loss:1.491616
[Test] epoch:168	batch id:30	 loss:1.387523
[Test] epoch:168	batch id:40	 loss:1.422406
[Test] epoch:168	batch id:50	 loss:1.500338
[Test] epoch:168	batch id:60	 loss:1.302102
[Test] epoch:168	batch id:70	 loss:1.490087
[Test] epoch:168	batch id:80	 loss:1.644951
[Test] epoch:168	batch id:90	 loss:1.534104
[Test] epoch:168	batch id:100	 loss:2.043349
[Test] epoch:168	batch id:110	 loss:1.413961
[Test] epoch:168	batch id:120	 loss:1.430627
[Test] epoch:168	batch id:130	 loss:1.358769
[Test] epoch:168	batch id:140	 loss:1.432116
[Test] epoch:168	batch id:150	 loss:1.741738
[Test] 168, loss: 1.500682, test acc: 0.906807,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:169	batch id:0	 lr:0.052281 loss:1.562100
[Train] epoch:169	batch id:10	 lr:0.052281 loss:1.472952
[Train] epoch:169	batch id:20	 lr:0.052281 loss:1.571429
[Train] epoch:169	batch id:30	 lr:0.052281 loss:1.388937
[Train] epoch:169	batch id:40	 lr:0.052281 loss:1.532287
[Train] epoch:169	batch id:50	 lr:0.052281 loss:1.444378
[Train] epoch:169	batch id:60	 lr:0.052281 loss:1.668748
[Train] epoch:169	batch id:70	 lr:0.052281 loss:1.438879
[Train] epoch:169	batch id:80	 lr:0.052281 loss:1.409400
[Train] epoch:169	batch id:90	 lr:0.052281 loss:1.662879
[Train] epoch:169	batch id:100	 lr:0.052281 loss:1.444622
[Train] epoch:169	batch id:110	 lr:0.052281 loss:1.422229
[Train] epoch:169	batch id:120	 lr:0.052281 loss:1.539780
[Train] epoch:169	batch id:130	 lr:0.052281 loss:1.412889
[Train] epoch:169	batch id:140	 lr:0.052281 loss:1.436070
[Train] epoch:169	batch id:150	 lr:0.052281 loss:1.498637
[Train] epoch:169	batch id:160	 lr:0.052281 loss:1.514995
[Train] epoch:169	batch id:170	 lr:0.052281 loss:1.569677
[Train] epoch:169	batch id:180	 lr:0.052281 loss:1.420882
[Train] epoch:169	batch id:190	 lr:0.052281 loss:1.438411
[Train] epoch:169	batch id:200	 lr:0.052281 loss:1.379478
[Train] epoch:169	batch id:210	 lr:0.052281 loss:1.460480
[Train] epoch:169	batch id:220	 lr:0.052281 loss:1.394324
[Train] epoch:169	batch id:230	 lr:0.052281 loss:1.566988
[Train] epoch:169	batch id:240	 lr:0.052281 loss:1.376789
[Train] epoch:169	batch id:250	 lr:0.052281 loss:1.400445
[Train] epoch:169	batch id:260	 lr:0.052281 loss:1.480270
[Train] epoch:169	batch id:270	 lr:0.052281 loss:1.486922
[Train] epoch:169	batch id:280	 lr:0.052281 loss:1.616319
[Train] epoch:169	batch id:290	 lr:0.052281 loss:1.496068
[Train] epoch:169	batch id:300	 lr:0.052281 loss:1.590999
[Train] 169, loss: 1.498920, train acc: 0.936380, 
[Test] epoch:169	batch id:0	 loss:1.320436
[Test] epoch:169	batch id:10	 loss:1.456206
[Test] epoch:169	batch id:20	 loss:1.422351
[Test] epoch:169	batch id:30	 loss:1.589962
[Test] epoch:169	batch id:40	 loss:1.390164
[Test] epoch:169	batch id:50	 loss:1.445884
[Test] epoch:169	batch id:60	 loss:1.270948
[Test] epoch:169	batch id:70	 loss:1.480657
[Test] epoch:169	batch id:80	 loss:1.681833
[Test] epoch:169	batch id:90	 loss:1.489222
[Test] epoch:169	batch id:100	 loss:1.893898
[Test] epoch:169	batch id:110	 loss:1.504440
[Test] epoch:169	batch id:120	 loss:1.439106
[Test] epoch:169	batch id:130	 loss:1.435920
[Test] epoch:169	batch id:140	 loss:1.546766
[Test] epoch:169	batch id:150	 loss:1.941110
[Test] 169, loss: 1.519427, test acc: 0.895057,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:170	batch id:0	 lr:0.051837 loss:1.425806
[Train] epoch:170	batch id:10	 lr:0.051837 loss:1.582768
[Train] epoch:170	batch id:20	 lr:0.051837 loss:1.623898
[Train] epoch:170	batch id:30	 lr:0.051837 loss:1.501140
[Train] epoch:170	batch id:40	 lr:0.051837 loss:1.488090
[Train] epoch:170	batch id:50	 lr:0.051837 loss:1.470002
[Train] epoch:170	batch id:60	 lr:0.051837 loss:1.562441
[Train] epoch:170	batch id:70	 lr:0.051837 loss:1.657821
[Train] epoch:170	batch id:80	 lr:0.051837 loss:1.426228
[Train] epoch:170	batch id:90	 lr:0.051837 loss:1.422573
[Train] epoch:170	batch id:100	 lr:0.051837 loss:1.426844
[Train] epoch:170	batch id:110	 lr:0.051837 loss:1.395550
[Train] epoch:170	batch id:120	 lr:0.051837 loss:1.412801
[Train] epoch:170	batch id:130	 lr:0.051837 loss:1.520365
[Train] epoch:170	batch id:140	 lr:0.051837 loss:1.578971
[Train] epoch:170	batch id:150	 lr:0.051837 loss:1.485479
[Train] epoch:170	batch id:160	 lr:0.051837 loss:1.467918
[Train] epoch:170	batch id:170	 lr:0.051837 loss:1.552870
[Train] epoch:170	batch id:180	 lr:0.051837 loss:1.600135
[Train] epoch:170	batch id:190	 lr:0.051837 loss:1.458058
[Train] epoch:170	batch id:200	 lr:0.051837 loss:1.455662
[Train] epoch:170	batch id:210	 lr:0.051837 loss:1.393342
[Train] epoch:170	batch id:220	 lr:0.051837 loss:1.658580
[Train] epoch:170	batch id:230	 lr:0.051837 loss:1.592659
[Train] epoch:170	batch id:240	 lr:0.051837 loss:1.485467
[Train] epoch:170	batch id:250	 lr:0.051837 loss:1.527428
[Train] epoch:170	batch id:260	 lr:0.051837 loss:1.426628
[Train] epoch:170	batch id:270	 lr:0.051837 loss:1.476678
[Train] epoch:170	batch id:280	 lr:0.051837 loss:1.404565
[Train] epoch:170	batch id:290	 lr:0.051837 loss:1.415143
[Train] epoch:170	batch id:300	 lr:0.051837 loss:1.529336
[Train] 170, loss: 1.501262, train acc: 0.935566, 
[Test] epoch:170	batch id:0	 loss:1.327447
[Test] epoch:170	batch id:10	 loss:1.407896
[Test] epoch:170	batch id:20	 loss:1.336763
[Test] epoch:170	batch id:30	 loss:1.593843
[Test] epoch:170	batch id:40	 loss:1.413797
[Test] epoch:170	batch id:50	 loss:1.502930
[Test] epoch:170	batch id:60	 loss:1.314928
[Test] epoch:170	batch id:70	 loss:1.435178
[Test] epoch:170	batch id:80	 loss:1.583878
[Test] epoch:170	batch id:90	 loss:1.623757
[Test] epoch:170	batch id:100	 loss:1.908268
[Test] epoch:170	batch id:110	 loss:1.422579
[Test] epoch:170	batch id:120	 loss:1.404671
[Test] epoch:170	batch id:130	 loss:1.381114
[Test] epoch:170	batch id:140	 loss:1.399501
[Test] epoch:170	batch id:150	 loss:1.840272
[Test] 170, loss: 1.511293, test acc: 0.903971,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:171	batch id:0	 lr:0.051393 loss:1.421007
[Train] epoch:171	batch id:10	 lr:0.051393 loss:1.328414
[Train] epoch:171	batch id:20	 lr:0.051393 loss:1.534694
[Train] epoch:171	batch id:30	 lr:0.051393 loss:1.433232
[Train] epoch:171	batch id:40	 lr:0.051393 loss:1.445208
[Train] epoch:171	batch id:50	 lr:0.051393 loss:1.482602
[Train] epoch:171	batch id:60	 lr:0.051393 loss:1.414523
[Train] epoch:171	batch id:70	 lr:0.051393 loss:1.462735
[Train] epoch:171	batch id:80	 lr:0.051393 loss:1.526825
[Train] epoch:171	batch id:90	 lr:0.051393 loss:1.480195
[Train] epoch:171	batch id:100	 lr:0.051393 loss:1.472554
[Train] epoch:171	batch id:110	 lr:0.051393 loss:1.502390
[Train] epoch:171	batch id:120	 lr:0.051393 loss:1.482126
[Train] epoch:171	batch id:130	 lr:0.051393 loss:1.408035
[Train] epoch:171	batch id:140	 lr:0.051393 loss:1.425666
[Train] epoch:171	batch id:150	 lr:0.051393 loss:1.435261
[Train] epoch:171	batch id:160	 lr:0.051393 loss:1.419135
[Train] epoch:171	batch id:170	 lr:0.051393 loss:1.429167
[Train] epoch:171	batch id:180	 lr:0.051393 loss:1.486851
[Train] epoch:171	batch id:190	 lr:0.051393 loss:1.473053
[Train] epoch:171	batch id:200	 lr:0.051393 loss:1.497616
[Train] epoch:171	batch id:210	 lr:0.051393 loss:1.560447
[Train] epoch:171	batch id:220	 lr:0.051393 loss:1.615067
[Train] epoch:171	batch id:230	 lr:0.051393 loss:1.472830
[Train] epoch:171	batch id:240	 lr:0.051393 loss:1.593460
[Train] epoch:171	batch id:250	 lr:0.051393 loss:1.459665
[Train] epoch:171	batch id:260	 lr:0.051393 loss:1.413550
[Train] epoch:171	batch id:270	 lr:0.051393 loss:1.478914
[Train] epoch:171	batch id:280	 lr:0.051393 loss:1.517349
[Train] epoch:171	batch id:290	 lr:0.051393 loss:1.536069
[Train] epoch:171	batch id:300	 lr:0.051393 loss:1.609250
[Train] 171, loss: 1.500085, train acc: 0.935159, 
[Test] epoch:171	batch id:0	 loss:1.334477
[Test] epoch:171	batch id:10	 loss:1.363493
[Test] epoch:171	batch id:20	 loss:1.473457
[Test] epoch:171	batch id:30	 loss:1.497633
[Test] epoch:171	batch id:40	 loss:1.360587
[Test] epoch:171	batch id:50	 loss:1.527856
[Test] epoch:171	batch id:60	 loss:1.307877
[Test] epoch:171	batch id:70	 loss:1.466411
[Test] epoch:171	batch id:80	 loss:1.694036
[Test] epoch:171	batch id:90	 loss:1.514026
[Test] epoch:171	batch id:100	 loss:1.965876
[Test] epoch:171	batch id:110	 loss:1.419214
[Test] epoch:171	batch id:120	 loss:1.368749
[Test] epoch:171	batch id:130	 loss:1.405824
[Test] epoch:171	batch id:140	 loss:1.542962
[Test] epoch:171	batch id:150	 loss:1.766747
[Test] 171, loss: 1.502276, test acc: 0.910859,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:172	batch id:0	 lr:0.050948 loss:1.531946
[Train] epoch:172	batch id:10	 lr:0.050948 loss:1.499356
[Train] epoch:172	batch id:20	 lr:0.050948 loss:1.608428
[Train] epoch:172	batch id:30	 lr:0.050948 loss:1.508578
[Train] epoch:172	batch id:40	 lr:0.050948 loss:1.413372
[Train] epoch:172	batch id:50	 lr:0.050948 loss:1.418935
[Train] epoch:172	batch id:60	 lr:0.050948 loss:1.422276
[Train] epoch:172	batch id:70	 lr:0.050948 loss:1.525352
[Train] epoch:172	batch id:80	 lr:0.050948 loss:1.446701
[Train] epoch:172	batch id:90	 lr:0.050948 loss:1.543847
[Train] epoch:172	batch id:100	 lr:0.050948 loss:1.422877
[Train] epoch:172	batch id:110	 lr:0.050948 loss:1.512810
[Train] epoch:172	batch id:120	 lr:0.050948 loss:1.662425
[Train] epoch:172	batch id:130	 lr:0.050948 loss:1.656780
[Train] epoch:172	batch id:140	 lr:0.050948 loss:1.458288
[Train] epoch:172	batch id:150	 lr:0.050948 loss:1.594840
[Train] epoch:172	batch id:160	 lr:0.050948 loss:1.438681
[Train] epoch:172	batch id:170	 lr:0.050948 loss:1.543083
[Train] epoch:172	batch id:180	 lr:0.050948 loss:1.481436
[Train] epoch:172	batch id:190	 lr:0.050948 loss:1.402655
[Train] epoch:172	batch id:200	 lr:0.050948 loss:1.470079
[Train] epoch:172	batch id:210	 lr:0.050948 loss:1.633333
[Train] epoch:172	batch id:220	 lr:0.050948 loss:1.528638
[Train] epoch:172	batch id:230	 lr:0.050948 loss:1.468489
[Train] epoch:172	batch id:240	 lr:0.050948 loss:1.528909
[Train] epoch:172	batch id:250	 lr:0.050948 loss:1.618302
[Train] epoch:172	batch id:260	 lr:0.050948 loss:1.581117
[Train] epoch:172	batch id:270	 lr:0.050948 loss:1.619598
[Train] epoch:172	batch id:280	 lr:0.050948 loss:1.555904
[Train] epoch:172	batch id:290	 lr:0.050948 loss:1.493122
[Train] epoch:172	batch id:300	 lr:0.050948 loss:1.433324
[Train] 172, loss: 1.501588, train acc: 0.935159, 
[Test] epoch:172	batch id:0	 loss:1.354003
[Test] epoch:172	batch id:10	 loss:1.445561
[Test] epoch:172	batch id:20	 loss:1.397962
[Test] epoch:172	batch id:30	 loss:1.555841
[Test] epoch:172	batch id:40	 loss:1.401162
[Test] epoch:172	batch id:50	 loss:1.546583
[Test] epoch:172	batch id:60	 loss:1.305206
[Test] epoch:172	batch id:70	 loss:1.388303
[Test] epoch:172	batch id:80	 loss:1.604125
[Test] epoch:172	batch id:90	 loss:1.561045
[Test] epoch:172	batch id:100	 loss:1.907429
[Test] epoch:172	batch id:110	 loss:1.404911
[Test] epoch:172	batch id:120	 loss:1.393286
[Test] epoch:172	batch id:130	 loss:1.504769
[Test] epoch:172	batch id:140	 loss:1.426946
[Test] epoch:172	batch id:150	 loss:1.868112
[Test] 172, loss: 1.513971, test acc: 0.895057,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:173	batch id:0	 lr:0.050504 loss:1.467482
[Train] epoch:173	batch id:10	 lr:0.050504 loss:1.433336
[Train] epoch:173	batch id:20	 lr:0.050504 loss:1.463182
[Train] epoch:173	batch id:30	 lr:0.050504 loss:1.493461
[Train] epoch:173	batch id:40	 lr:0.050504 loss:1.378103
[Train] epoch:173	batch id:50	 lr:0.050504 loss:1.682049
[Train] epoch:173	batch id:60	 lr:0.050504 loss:1.567408
[Train] epoch:173	batch id:70	 lr:0.050504 loss:1.475842
[Train] epoch:173	batch id:80	 lr:0.050504 loss:1.471435
[Train] epoch:173	batch id:90	 lr:0.050504 loss:1.484458
[Train] epoch:173	batch id:100	 lr:0.050504 loss:1.513677
[Train] epoch:173	batch id:110	 lr:0.050504 loss:1.593600
[Train] epoch:173	batch id:120	 lr:0.050504 loss:1.479849
[Train] epoch:173	batch id:130	 lr:0.050504 loss:1.558899
[Train] epoch:173	batch id:140	 lr:0.050504 loss:1.425643
[Train] epoch:173	batch id:150	 lr:0.050504 loss:1.513310
[Train] epoch:173	batch id:160	 lr:0.050504 loss:1.553510
[Train] epoch:173	batch id:170	 lr:0.050504 loss:1.575856
[Train] epoch:173	batch id:180	 lr:0.050504 loss:1.494988
[Train] epoch:173	batch id:190	 lr:0.050504 loss:1.641453
[Train] epoch:173	batch id:200	 lr:0.050504 loss:1.512959
[Train] epoch:173	batch id:210	 lr:0.050504 loss:1.450259
[Train] epoch:173	batch id:220	 lr:0.050504 loss:1.544780
[Train] epoch:173	batch id:230	 lr:0.050504 loss:1.421130
[Train] epoch:173	batch id:240	 lr:0.050504 loss:1.492968
[Train] epoch:173	batch id:250	 lr:0.050504 loss:1.470307
[Train] epoch:173	batch id:260	 lr:0.050504 loss:1.565056
[Train] epoch:173	batch id:270	 lr:0.050504 loss:1.449071
[Train] epoch:173	batch id:280	 lr:0.050504 loss:1.644698
[Train] epoch:173	batch id:290	 lr:0.050504 loss:1.551000
[Train] epoch:173	batch id:300	 lr:0.050504 loss:1.494106
[Train] 173, loss: 1.504749, train acc: 0.934446, 
[Test] epoch:173	batch id:0	 loss:1.356217
[Test] epoch:173	batch id:10	 loss:1.399219
[Test] epoch:173	batch id:20	 loss:1.422661
[Test] epoch:173	batch id:30	 loss:1.422237
[Test] epoch:173	batch id:40	 loss:1.440541
[Test] epoch:173	batch id:50	 loss:1.454247
[Test] epoch:173	batch id:60	 loss:1.290967
[Test] epoch:173	batch id:70	 loss:1.505727
[Test] epoch:173	batch id:80	 loss:1.609874
[Test] epoch:173	batch id:90	 loss:1.604026
[Test] epoch:173	batch id:100	 loss:1.971394
[Test] epoch:173	batch id:110	 loss:1.378679
[Test] epoch:173	batch id:120	 loss:1.494809
[Test] epoch:173	batch id:130	 loss:1.361983
[Test] epoch:173	batch id:140	 loss:1.361027
[Test] epoch:173	batch id:150	 loss:1.702999
[Test] 173, loss: 1.488101, test acc: 0.907212,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:174	batch id:0	 lr:0.050060 loss:1.466511
[Train] epoch:174	batch id:10	 lr:0.050060 loss:1.486454
[Train] epoch:174	batch id:20	 lr:0.050060 loss:1.387937
[Train] epoch:174	batch id:30	 lr:0.050060 loss:1.469269
[Train] epoch:174	batch id:40	 lr:0.050060 loss:1.531742
[Train] epoch:174	batch id:50	 lr:0.050060 loss:1.525691
[Train] epoch:174	batch id:60	 lr:0.050060 loss:1.614763
[Train] epoch:174	batch id:70	 lr:0.050060 loss:1.415989
[Train] epoch:174	batch id:80	 lr:0.050060 loss:1.486771
[Train] epoch:174	batch id:90	 lr:0.050060 loss:1.455222
[Train] epoch:174	batch id:100	 lr:0.050060 loss:1.463476
[Train] epoch:174	batch id:110	 lr:0.050060 loss:1.559099
[Train] epoch:174	batch id:120	 lr:0.050060 loss:1.483887
[Train] epoch:174	batch id:130	 lr:0.050060 loss:1.446421
[Train] epoch:174	batch id:140	 lr:0.050060 loss:1.423713
[Train] epoch:174	batch id:150	 lr:0.050060 loss:1.488253
[Train] epoch:174	batch id:160	 lr:0.050060 loss:1.590637
[Train] epoch:174	batch id:170	 lr:0.050060 loss:1.460427
[Train] epoch:174	batch id:180	 lr:0.050060 loss:1.467979
[Train] epoch:174	batch id:190	 lr:0.050060 loss:1.628530
[Train] epoch:174	batch id:200	 lr:0.050060 loss:1.508661
[Train] epoch:174	batch id:210	 lr:0.050060 loss:1.404395
[Train] epoch:174	batch id:220	 lr:0.050060 loss:1.505900
[Train] epoch:174	batch id:230	 lr:0.050060 loss:1.448581
[Train] epoch:174	batch id:240	 lr:0.050060 loss:1.542550
[Train] epoch:174	batch id:250	 lr:0.050060 loss:1.544904
[Train] epoch:174	batch id:260	 lr:0.050060 loss:1.517435
[Train] epoch:174	batch id:270	 lr:0.050060 loss:1.612497
[Train] epoch:174	batch id:280	 lr:0.050060 loss:1.451338
[Train] epoch:174	batch id:290	 lr:0.050060 loss:1.452289
[Train] epoch:174	batch id:300	 lr:0.050060 loss:1.452884
[Train] 174, loss: 1.506058, train acc: 0.931087, 
[Test] epoch:174	batch id:0	 loss:1.299153
[Test] epoch:174	batch id:10	 loss:1.378598
[Test] epoch:174	batch id:20	 loss:1.434407
[Test] epoch:174	batch id:30	 loss:1.543741
[Test] epoch:174	batch id:40	 loss:1.378511
[Test] epoch:174	batch id:50	 loss:1.463759
[Test] epoch:174	batch id:60	 loss:1.310795
[Test] epoch:174	batch id:70	 loss:1.422196
[Test] epoch:174	batch id:80	 loss:1.592755
[Test] epoch:174	batch id:90	 loss:1.565873
[Test] epoch:174	batch id:100	 loss:1.838514
[Test] epoch:174	batch id:110	 loss:1.427486
[Test] epoch:174	batch id:120	 loss:1.331322
[Test] epoch:174	batch id:130	 loss:1.458452
[Test] epoch:174	batch id:140	 loss:1.353933
[Test] epoch:174	batch id:150	 loss:1.773062
[Test] 174, loss: 1.478816, test acc: 0.910859,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:175	batch id:0	 lr:0.049615 loss:1.472142
[Train] epoch:175	batch id:10	 lr:0.049615 loss:1.427003
[Train] epoch:175	batch id:20	 lr:0.049615 loss:1.482335
[Train] epoch:175	batch id:30	 lr:0.049615 loss:1.479894
[Train] epoch:175	batch id:40	 lr:0.049615 loss:1.605902
[Train] epoch:175	batch id:50	 lr:0.049615 loss:1.433475
[Train] epoch:175	batch id:60	 lr:0.049615 loss:1.455671
[Train] epoch:175	batch id:70	 lr:0.049615 loss:1.514290
[Train] epoch:175	batch id:80	 lr:0.049615 loss:1.561790
[Train] epoch:175	batch id:90	 lr:0.049615 loss:1.572294
[Train] epoch:175	batch id:100	 lr:0.049615 loss:1.483598
[Train] epoch:175	batch id:110	 lr:0.049615 loss:1.417196
[Train] epoch:175	batch id:120	 lr:0.049615 loss:1.477635
[Train] epoch:175	batch id:130	 lr:0.049615 loss:1.423873
[Train] epoch:175	batch id:140	 lr:0.049615 loss:1.586945
[Train] epoch:175	batch id:150	 lr:0.049615 loss:1.488595
[Train] epoch:175	batch id:160	 lr:0.049615 loss:1.472646
[Train] epoch:175	batch id:170	 lr:0.049615 loss:1.531986
[Train] epoch:175	batch id:180	 lr:0.049615 loss:1.477529
[Train] epoch:175	batch id:190	 lr:0.049615 loss:1.460460
[Train] epoch:175	batch id:200	 lr:0.049615 loss:1.505095
[Train] epoch:175	batch id:210	 lr:0.049615 loss:1.505442
[Train] epoch:175	batch id:220	 lr:0.049615 loss:1.509469
[Train] epoch:175	batch id:230	 lr:0.049615 loss:1.393626
[Train] epoch:175	batch id:240	 lr:0.049615 loss:1.621406
[Train] epoch:175	batch id:250	 lr:0.049615 loss:1.408177
[Train] epoch:175	batch id:260	 lr:0.049615 loss:1.548997
[Train] epoch:175	batch id:270	 lr:0.049615 loss:1.580850
[Train] epoch:175	batch id:280	 lr:0.049615 loss:1.453450
[Train] epoch:175	batch id:290	 lr:0.049615 loss:1.506682
[Train] epoch:175	batch id:300	 lr:0.049615 loss:1.520094
[Train] 175, loss: 1.498084, train acc: 0.936380, 
[Test] epoch:175	batch id:0	 loss:1.371162
[Test] epoch:175	batch id:10	 loss:1.382022
[Test] epoch:175	batch id:20	 loss:1.457676
[Test] epoch:175	batch id:30	 loss:1.395668
[Test] epoch:175	batch id:40	 loss:1.417917
[Test] epoch:175	batch id:50	 loss:1.432287
[Test] epoch:175	batch id:60	 loss:1.291871
[Test] epoch:175	batch id:70	 loss:1.436307
[Test] epoch:175	batch id:80	 loss:1.670235
[Test] epoch:175	batch id:90	 loss:1.560978
[Test] epoch:175	batch id:100	 loss:2.014154
[Test] epoch:175	batch id:110	 loss:1.399327
[Test] epoch:175	batch id:120	 loss:1.457192
[Test] epoch:175	batch id:130	 loss:1.496907
[Test] epoch:175	batch id:140	 loss:1.393717
[Test] epoch:175	batch id:150	 loss:1.712698
[Test] 175, loss: 1.507953, test acc: 0.916937,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:176	batch id:0	 lr:0.049171 loss:1.415957
[Train] epoch:176	batch id:10	 lr:0.049171 loss:1.479170
[Train] epoch:176	batch id:20	 lr:0.049171 loss:1.549022
[Train] epoch:176	batch id:30	 lr:0.049171 loss:1.382646
[Train] epoch:176	batch id:40	 lr:0.049171 loss:1.638871
[Train] epoch:176	batch id:50	 lr:0.049171 loss:1.510069
[Train] epoch:176	batch id:60	 lr:0.049171 loss:1.473487
[Train] epoch:176	batch id:70	 lr:0.049171 loss:1.467879
[Train] epoch:176	batch id:80	 lr:0.049171 loss:1.552456
[Train] epoch:176	batch id:90	 lr:0.049171 loss:1.527742
[Train] epoch:176	batch id:100	 lr:0.049171 loss:1.665242
[Train] epoch:176	batch id:110	 lr:0.049171 loss:1.564320
[Train] epoch:176	batch id:120	 lr:0.049171 loss:1.571739
[Train] epoch:176	batch id:130	 lr:0.049171 loss:1.492572
[Train] epoch:176	batch id:140	 lr:0.049171 loss:1.463882
[Train] epoch:176	batch id:150	 lr:0.049171 loss:1.443866
[Train] epoch:176	batch id:160	 lr:0.049171 loss:1.612652
[Train] epoch:176	batch id:170	 lr:0.049171 loss:1.474187
[Train] epoch:176	batch id:180	 lr:0.049171 loss:1.407980
[Train] epoch:176	batch id:190	 lr:0.049171 loss:1.505421
[Train] epoch:176	batch id:200	 lr:0.049171 loss:1.548461
[Train] epoch:176	batch id:210	 lr:0.049171 loss:1.459721
[Train] epoch:176	batch id:220	 lr:0.049171 loss:1.560045
[Train] epoch:176	batch id:230	 lr:0.049171 loss:1.472261
[Train] epoch:176	batch id:240	 lr:0.049171 loss:1.559365
[Train] epoch:176	batch id:250	 lr:0.049171 loss:1.547362
[Train] epoch:176	batch id:260	 lr:0.049171 loss:1.541382
[Train] epoch:176	batch id:270	 lr:0.049171 loss:1.410212
[Train] epoch:176	batch id:280	 lr:0.049171 loss:1.552106
[Train] epoch:176	batch id:290	 lr:0.049171 loss:1.502900
[Train] epoch:176	batch id:300	 lr:0.049171 loss:1.367695
[Train] 176, loss: 1.495636, train acc: 0.937398, 
[Test] epoch:176	batch id:0	 loss:1.311363
[Test] epoch:176	batch id:10	 loss:1.395727
[Test] epoch:176	batch id:20	 loss:1.539699
[Test] epoch:176	batch id:30	 loss:1.408371
[Test] epoch:176	batch id:40	 loss:1.481249
[Test] epoch:176	batch id:50	 loss:1.392578
[Test] epoch:176	batch id:60	 loss:1.305611
[Test] epoch:176	batch id:70	 loss:1.439118
[Test] epoch:176	batch id:80	 loss:1.683353
[Test] epoch:176	batch id:90	 loss:1.457936
[Test] epoch:176	batch id:100	 loss:1.832178
[Test] epoch:176	batch id:110	 loss:1.361263
[Test] epoch:176	batch id:120	 loss:1.338405
[Test] epoch:176	batch id:130	 loss:1.414060
[Test] epoch:176	batch id:140	 loss:1.389832
[Test] epoch:176	batch id:150	 loss:1.774794
[Test] 176, loss: 1.496388, test acc: 0.910454,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:177	batch id:0	 lr:0.048727 loss:1.578881
[Train] epoch:177	batch id:10	 lr:0.048727 loss:1.475181
[Train] epoch:177	batch id:20	 lr:0.048727 loss:1.619002
[Train] epoch:177	batch id:30	 lr:0.048727 loss:1.393577
[Train] epoch:177	batch id:40	 lr:0.048727 loss:1.514494
[Train] epoch:177	batch id:50	 lr:0.048727 loss:1.488919
[Train] epoch:177	batch id:60	 lr:0.048727 loss:1.442462
[Train] epoch:177	batch id:70	 lr:0.048727 loss:1.478717
[Train] epoch:177	batch id:80	 lr:0.048727 loss:1.455870
[Train] epoch:177	batch id:90	 lr:0.048727 loss:1.556950
[Train] epoch:177	batch id:100	 lr:0.048727 loss:1.533838
[Train] epoch:177	batch id:110	 lr:0.048727 loss:1.408749
[Train] epoch:177	batch id:120	 lr:0.048727 loss:1.413816
[Train] epoch:177	batch id:130	 lr:0.048727 loss:1.420212
[Train] epoch:177	batch id:140	 lr:0.048727 loss:1.444547
[Train] epoch:177	batch id:150	 lr:0.048727 loss:1.511773
[Train] epoch:177	batch id:160	 lr:0.048727 loss:1.477882
[Train] epoch:177	batch id:170	 lr:0.048727 loss:1.489221
[Train] epoch:177	batch id:180	 lr:0.048727 loss:1.615112
[Train] epoch:177	batch id:190	 lr:0.048727 loss:1.463698
[Train] epoch:177	batch id:200	 lr:0.048727 loss:1.493388
[Train] epoch:177	batch id:210	 lr:0.048727 loss:1.426853
[Train] epoch:177	batch id:220	 lr:0.048727 loss:1.442782
[Train] epoch:177	batch id:230	 lr:0.048727 loss:1.580049
[Train] epoch:177	batch id:240	 lr:0.048727 loss:1.508984
[Train] epoch:177	batch id:250	 lr:0.048727 loss:1.545744
[Train] epoch:177	batch id:260	 lr:0.048727 loss:1.627139
[Train] epoch:177	batch id:270	 lr:0.048727 loss:1.621957
[Train] epoch:177	batch id:280	 lr:0.048727 loss:1.583908
[Train] epoch:177	batch id:290	 lr:0.048727 loss:1.537696
[Train] epoch:177	batch id:300	 lr:0.048727 loss:1.393111
[Train] 177, loss: 1.499615, train acc: 0.935668, 
[Test] epoch:177	batch id:0	 loss:1.331528
[Test] epoch:177	batch id:10	 loss:1.472792
[Test] epoch:177	batch id:20	 loss:1.398362
[Test] epoch:177	batch id:30	 loss:1.499550
[Test] epoch:177	batch id:40	 loss:1.421351
[Test] epoch:177	batch id:50	 loss:1.432856
[Test] epoch:177	batch id:60	 loss:1.294549
[Test] epoch:177	batch id:70	 loss:1.559331
[Test] epoch:177	batch id:80	 loss:1.673799
[Test] epoch:177	batch id:90	 loss:1.656800
[Test] epoch:177	batch id:100	 loss:1.908898
[Test] epoch:177	batch id:110	 loss:1.397002
[Test] epoch:177	batch id:120	 loss:1.431592
[Test] epoch:177	batch id:130	 loss:1.357605
[Test] epoch:177	batch id:140	 loss:1.374248
[Test] epoch:177	batch id:150	 loss:1.810849
[Test] 177, loss: 1.513764, test acc: 0.906402,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:178	batch id:0	 lr:0.048283 loss:1.447498
[Train] epoch:178	batch id:10	 lr:0.048283 loss:1.435517
[Train] epoch:178	batch id:20	 lr:0.048283 loss:1.653722
[Train] epoch:178	batch id:30	 lr:0.048283 loss:1.580380
[Train] epoch:178	batch id:40	 lr:0.048283 loss:1.541698
[Train] epoch:178	batch id:50	 lr:0.048283 loss:1.457911
[Train] epoch:178	batch id:60	 lr:0.048283 loss:1.514732
[Train] epoch:178	batch id:70	 lr:0.048283 loss:1.634482
[Train] epoch:178	batch id:80	 lr:0.048283 loss:1.498986
[Train] epoch:178	batch id:90	 lr:0.048283 loss:1.479586
[Train] epoch:178	batch id:100	 lr:0.048283 loss:1.468834
[Train] epoch:178	batch id:110	 lr:0.048283 loss:1.576834
[Train] epoch:178	batch id:120	 lr:0.048283 loss:1.516174
[Train] epoch:178	batch id:130	 lr:0.048283 loss:1.687888
[Train] epoch:178	batch id:140	 lr:0.048283 loss:1.485250
[Train] epoch:178	batch id:150	 lr:0.048283 loss:1.484581
[Train] epoch:178	batch id:160	 lr:0.048283 loss:1.364467
[Train] epoch:178	batch id:170	 lr:0.048283 loss:1.450407
[Train] epoch:178	batch id:180	 lr:0.048283 loss:1.469298
[Train] epoch:178	batch id:190	 lr:0.048283 loss:1.561140
[Train] epoch:178	batch id:200	 lr:0.048283 loss:1.480184
[Train] epoch:178	batch id:210	 lr:0.048283 loss:1.497043
[Train] epoch:178	batch id:220	 lr:0.048283 loss:1.436234
[Train] epoch:178	batch id:230	 lr:0.048283 loss:1.464538
[Train] epoch:178	batch id:240	 lr:0.048283 loss:1.573716
[Train] epoch:178	batch id:250	 lr:0.048283 loss:1.435142
[Train] epoch:178	batch id:260	 lr:0.048283 loss:1.414889
[Train] epoch:178	batch id:270	 lr:0.048283 loss:1.455514
[Train] epoch:178	batch id:280	 lr:0.048283 loss:1.465769
[Train] epoch:178	batch id:290	 lr:0.048283 loss:1.460946
[Train] epoch:178	batch id:300	 lr:0.048283 loss:1.587285
[Train] 178, loss: 1.498164, train acc: 0.935464, 
[Test] epoch:178	batch id:0	 loss:1.319666
[Test] epoch:178	batch id:10	 loss:1.408587
[Test] epoch:178	batch id:20	 loss:1.401267
[Test] epoch:178	batch id:30	 loss:1.376108
[Test] epoch:178	batch id:40	 loss:1.339777
[Test] epoch:178	batch id:50	 loss:1.420040
[Test] epoch:178	batch id:60	 loss:1.277413
[Test] epoch:178	batch id:70	 loss:1.418929
[Test] epoch:178	batch id:80	 loss:1.605648
[Test] epoch:178	batch id:90	 loss:1.397454
[Test] epoch:178	batch id:100	 loss:1.779559
[Test] epoch:178	batch id:110	 loss:1.338802
[Test] epoch:178	batch id:120	 loss:1.337730
[Test] epoch:178	batch id:130	 loss:1.437037
[Test] epoch:178	batch id:140	 loss:1.375493
[Test] epoch:178	batch id:150	 loss:1.776551
[Test] 178, loss: 1.482950, test acc: 0.912885,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:179	batch id:0	 lr:0.047839 loss:1.419994
[Train] epoch:179	batch id:10	 lr:0.047839 loss:1.674945
[Train] epoch:179	batch id:20	 lr:0.047839 loss:1.596347
[Train] epoch:179	batch id:30	 lr:0.047839 loss:1.494194
[Train] epoch:179	batch id:40	 lr:0.047839 loss:1.364915
[Train] epoch:179	batch id:50	 lr:0.047839 loss:1.423630
[Train] epoch:179	batch id:60	 lr:0.047839 loss:1.498484
[Train] epoch:179	batch id:70	 lr:0.047839 loss:1.443351
[Train] epoch:179	batch id:80	 lr:0.047839 loss:1.376187
[Train] epoch:179	batch id:90	 lr:0.047839 loss:1.381298
[Train] epoch:179	batch id:100	 lr:0.047839 loss:1.530770
[Train] epoch:179	batch id:110	 lr:0.047839 loss:1.474709
[Train] epoch:179	batch id:120	 lr:0.047839 loss:1.482851
[Train] epoch:179	batch id:130	 lr:0.047839 loss:1.578829
[Train] epoch:179	batch id:140	 lr:0.047839 loss:1.401328
[Train] epoch:179	batch id:150	 lr:0.047839 loss:1.462573
[Train] epoch:179	batch id:160	 lr:0.047839 loss:1.497912
[Train] epoch:179	batch id:170	 lr:0.047839 loss:1.405499
[Train] epoch:179	batch id:180	 lr:0.047839 loss:1.603677
[Train] epoch:179	batch id:190	 lr:0.047839 loss:1.532371
[Train] epoch:179	batch id:200	 lr:0.047839 loss:1.447979
[Train] epoch:179	batch id:210	 lr:0.047839 loss:1.490611
[Train] epoch:179	batch id:220	 lr:0.047839 loss:1.448709
[Train] epoch:179	batch id:230	 lr:0.047839 loss:1.444141
[Train] epoch:179	batch id:240	 lr:0.047839 loss:1.501737
[Train] epoch:179	batch id:250	 lr:0.047839 loss:1.466114
[Train] epoch:179	batch id:260	 lr:0.047839 loss:1.421164
[Train] epoch:179	batch id:270	 lr:0.047839 loss:1.584999
[Train] epoch:179	batch id:280	 lr:0.047839 loss:1.453292
[Train] epoch:179	batch id:290	 lr:0.047839 loss:1.479841
[Train] epoch:179	batch id:300	 lr:0.047839 loss:1.459807
[Train] 179, loss: 1.504832, train acc: 0.932003, 
[Test] epoch:179	batch id:0	 loss:1.315443
[Test] epoch:179	batch id:10	 loss:1.515632
[Test] epoch:179	batch id:20	 loss:1.471915
[Test] epoch:179	batch id:30	 loss:1.426569
[Test] epoch:179	batch id:40	 loss:1.377529
[Test] epoch:179	batch id:50	 loss:1.658916
[Test] epoch:179	batch id:60	 loss:1.277944
[Test] epoch:179	batch id:70	 loss:1.438177
[Test] epoch:179	batch id:80	 loss:1.625168
[Test] epoch:179	batch id:90	 loss:1.350627
[Test] epoch:179	batch id:100	 loss:1.949088
[Test] epoch:179	batch id:110	 loss:1.376784
[Test] epoch:179	batch id:120	 loss:1.389989
[Test] epoch:179	batch id:130	 loss:1.479857
[Test] epoch:179	batch id:140	 loss:1.308343
[Test] epoch:179	batch id:150	 loss:1.743621
[Test] 179, loss: 1.511267, test acc: 0.905592,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:180	batch id:0	 lr:0.047396 loss:1.410430
[Train] epoch:180	batch id:10	 lr:0.047396 loss:1.428087
[Train] epoch:180	batch id:20	 lr:0.047396 loss:1.513169
[Train] epoch:180	batch id:30	 lr:0.047396 loss:1.600044
[Train] epoch:180	batch id:40	 lr:0.047396 loss:1.561512
[Train] epoch:180	batch id:50	 lr:0.047396 loss:1.472221
[Train] epoch:180	batch id:60	 lr:0.047396 loss:1.445483
[Train] epoch:180	batch id:70	 lr:0.047396 loss:1.441291
[Train] epoch:180	batch id:80	 lr:0.047396 loss:1.449398
[Train] epoch:180	batch id:90	 lr:0.047396 loss:1.468631
[Train] epoch:180	batch id:100	 lr:0.047396 loss:1.514369
[Train] epoch:180	batch id:110	 lr:0.047396 loss:1.379578
[Train] epoch:180	batch id:120	 lr:0.047396 loss:1.498857
[Train] epoch:180	batch id:130	 lr:0.047396 loss:1.449126
[Train] epoch:180	batch id:140	 lr:0.047396 loss:1.398956
[Train] epoch:180	batch id:150	 lr:0.047396 loss:1.527158
[Train] epoch:180	batch id:160	 lr:0.047396 loss:1.490685
[Train] epoch:180	batch id:170	 lr:0.047396 loss:1.442783
[Train] epoch:180	batch id:180	 lr:0.047396 loss:1.462763
[Train] epoch:180	batch id:190	 lr:0.047396 loss:1.576344
[Train] epoch:180	batch id:200	 lr:0.047396 loss:1.551931
[Train] epoch:180	batch id:210	 lr:0.047396 loss:1.391975
[Train] epoch:180	batch id:220	 lr:0.047396 loss:1.485389
[Train] epoch:180	batch id:230	 lr:0.047396 loss:1.496799
[Train] epoch:180	batch id:240	 lr:0.047396 loss:1.391737
[Train] epoch:180	batch id:250	 lr:0.047396 loss:1.591231
[Train] epoch:180	batch id:260	 lr:0.047396 loss:1.443416
[Train] epoch:180	batch id:270	 lr:0.047396 loss:1.398881
[Train] epoch:180	batch id:280	 lr:0.047396 loss:1.454153
[Train] epoch:180	batch id:290	 lr:0.047396 loss:1.500347
[Train] epoch:180	batch id:300	 lr:0.047396 loss:1.627054
[Train] 180, loss: 1.490879, train acc: 0.937093, 
[Test] epoch:180	batch id:0	 loss:1.317735
[Test] epoch:180	batch id:10	 loss:1.503042
[Test] epoch:180	batch id:20	 loss:1.394856
[Test] epoch:180	batch id:30	 loss:1.359064
[Test] epoch:180	batch id:40	 loss:1.371194
[Test] epoch:180	batch id:50	 loss:1.469090
[Test] epoch:180	batch id:60	 loss:1.276103
[Test] epoch:180	batch id:70	 loss:1.371001
[Test] epoch:180	batch id:80	 loss:1.735325
[Test] epoch:180	batch id:90	 loss:1.576689
[Test] epoch:180	batch id:100	 loss:1.904657
[Test] epoch:180	batch id:110	 loss:1.344154
[Test] epoch:180	batch id:120	 loss:1.365439
[Test] epoch:180	batch id:130	 loss:1.371584
[Test] epoch:180	batch id:140	 loss:1.388878
[Test] epoch:180	batch id:150	 loss:1.711521
[Test] 180, loss: 1.490516, test acc: 0.913695,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:181	batch id:0	 lr:0.046953 loss:1.443948
[Train] epoch:181	batch id:10	 lr:0.046953 loss:1.449955
[Train] epoch:181	batch id:20	 lr:0.046953 loss:1.536355
[Train] epoch:181	batch id:30	 lr:0.046953 loss:1.474395
[Train] epoch:181	batch id:40	 lr:0.046953 loss:1.455243
[Train] epoch:181	batch id:50	 lr:0.046953 loss:1.567916
[Train] epoch:181	batch id:60	 lr:0.046953 loss:1.486162
[Train] epoch:181	batch id:70	 lr:0.046953 loss:1.589278
[Train] epoch:181	batch id:80	 lr:0.046953 loss:1.493269
[Train] epoch:181	batch id:90	 lr:0.046953 loss:1.572513
[Train] epoch:181	batch id:100	 lr:0.046953 loss:1.463906
[Train] epoch:181	batch id:110	 lr:0.046953 loss:1.457341
[Train] epoch:181	batch id:120	 lr:0.046953 loss:1.477595
[Train] epoch:181	batch id:130	 lr:0.046953 loss:1.514760
[Train] epoch:181	batch id:140	 lr:0.046953 loss:1.535607
[Train] epoch:181	batch id:150	 lr:0.046953 loss:1.421202
[Train] epoch:181	batch id:160	 lr:0.046953 loss:1.427074
[Train] epoch:181	batch id:170	 lr:0.046953 loss:1.485996
[Train] epoch:181	batch id:180	 lr:0.046953 loss:1.392166
[Train] epoch:181	batch id:190	 lr:0.046953 loss:1.568278
[Train] epoch:181	batch id:200	 lr:0.046953 loss:1.498778
[Train] epoch:181	batch id:210	 lr:0.046953 loss:1.429803
[Train] epoch:181	batch id:220	 lr:0.046953 loss:1.498888
[Train] epoch:181	batch id:230	 lr:0.046953 loss:1.556088
[Train] epoch:181	batch id:240	 lr:0.046953 loss:1.490722
[Train] epoch:181	batch id:250	 lr:0.046953 loss:1.588974
[Train] epoch:181	batch id:260	 lr:0.046953 loss:1.481873
[Train] epoch:181	batch id:270	 lr:0.046953 loss:1.548575
[Train] epoch:181	batch id:280	 lr:0.046953 loss:1.448360
[Train] epoch:181	batch id:290	 lr:0.046953 loss:1.552333
[Train] epoch:181	batch id:300	 lr:0.046953 loss:1.573212
[Train] 181, loss: 1.499448, train acc: 0.934548, 
[Test] epoch:181	batch id:0	 loss:1.308800
[Test] epoch:181	batch id:10	 loss:1.438182
[Test] epoch:181	batch id:20	 loss:1.324614
[Test] epoch:181	batch id:30	 loss:1.481239
[Test] epoch:181	batch id:40	 loss:1.365244
[Test] epoch:181	batch id:50	 loss:1.545692
[Test] epoch:181	batch id:60	 loss:1.274521
[Test] epoch:181	batch id:70	 loss:1.386971
[Test] epoch:181	batch id:80	 loss:1.578726
[Test] epoch:181	batch id:90	 loss:1.640867
[Test] epoch:181	batch id:100	 loss:1.885543
[Test] epoch:181	batch id:110	 loss:1.460721
[Test] epoch:181	batch id:120	 loss:1.445330
[Test] epoch:181	batch id:130	 loss:1.517814
[Test] epoch:181	batch id:140	 loss:1.406805
[Test] epoch:181	batch id:150	 loss:1.730289
[Test] 181, loss: 1.492424, test acc: 0.902755,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:182	batch id:0	 lr:0.046509 loss:1.466897
[Train] epoch:182	batch id:10	 lr:0.046509 loss:1.532587
[Train] epoch:182	batch id:20	 lr:0.046509 loss:1.462281
[Train] epoch:182	batch id:30	 lr:0.046509 loss:1.438688
[Train] epoch:182	batch id:40	 lr:0.046509 loss:1.486402
[Train] epoch:182	batch id:50	 lr:0.046509 loss:1.498447
[Train] epoch:182	batch id:60	 lr:0.046509 loss:1.587622
[Train] epoch:182	batch id:70	 lr:0.046509 loss:1.538135
[Train] epoch:182	batch id:80	 lr:0.046509 loss:1.555644
[Train] epoch:182	batch id:90	 lr:0.046509 loss:1.572565
[Train] epoch:182	batch id:100	 lr:0.046509 loss:1.578246
[Train] epoch:182	batch id:110	 lr:0.046509 loss:1.517958
[Train] epoch:182	batch id:120	 lr:0.046509 loss:1.449690
[Train] epoch:182	batch id:130	 lr:0.046509 loss:1.442397
[Train] epoch:182	batch id:140	 lr:0.046509 loss:1.387326
[Train] epoch:182	batch id:150	 lr:0.046509 loss:1.532727
[Train] epoch:182	batch id:160	 lr:0.046509 loss:1.442386
[Train] epoch:182	batch id:170	 lr:0.046509 loss:1.523410
[Train] epoch:182	batch id:180	 lr:0.046509 loss:1.512202
[Train] epoch:182	batch id:190	 lr:0.046509 loss:1.499678
[Train] epoch:182	batch id:200	 lr:0.046509 loss:1.358265
[Train] epoch:182	batch id:210	 lr:0.046509 loss:1.486388
[Train] epoch:182	batch id:220	 lr:0.046509 loss:1.439426
[Train] epoch:182	batch id:230	 lr:0.046509 loss:1.532859
[Train] epoch:182	batch id:240	 lr:0.046509 loss:1.443552
[Train] epoch:182	batch id:250	 lr:0.046509 loss:1.430805
[Train] epoch:182	batch id:260	 lr:0.046509 loss:1.481655
[Train] epoch:182	batch id:270	 lr:0.046509 loss:1.421802
[Train] epoch:182	batch id:280	 lr:0.046509 loss:1.402647
[Train] epoch:182	batch id:290	 lr:0.046509 loss:1.479215
[Train] epoch:182	batch id:300	 lr:0.046509 loss:1.438073
[Train] 182, loss: 1.487314, train acc: 0.938009, 
[Test] epoch:182	batch id:0	 loss:1.299622
[Test] epoch:182	batch id:10	 loss:1.497696
[Test] epoch:182	batch id:20	 loss:1.463726
[Test] epoch:182	batch id:30	 loss:1.504929
[Test] epoch:182	batch id:40	 loss:1.431478
[Test] epoch:182	batch id:50	 loss:1.593378
[Test] epoch:182	batch id:60	 loss:1.302164
[Test] epoch:182	batch id:70	 loss:1.522252
[Test] epoch:182	batch id:80	 loss:1.682591
[Test] epoch:182	batch id:90	 loss:1.507530
[Test] epoch:182	batch id:100	 loss:1.974488
[Test] epoch:182	batch id:110	 loss:1.365695
[Test] epoch:182	batch id:120	 loss:1.354577
[Test] epoch:182	batch id:130	 loss:1.512307
[Test] epoch:182	batch id:140	 loss:1.498803
[Test] epoch:182	batch id:150	 loss:1.716634
[Test] 182, loss: 1.510725, test acc: 0.901540,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:183	batch id:0	 lr:0.046067 loss:1.508712
[Train] epoch:183	batch id:10	 lr:0.046067 loss:1.545878
[Train] epoch:183	batch id:20	 lr:0.046067 loss:1.521034
[Train] epoch:183	batch id:30	 lr:0.046067 loss:1.546854
[Train] epoch:183	batch id:40	 lr:0.046067 loss:1.500581
[Train] epoch:183	batch id:50	 lr:0.046067 loss:1.451729
[Train] epoch:183	batch id:60	 lr:0.046067 loss:1.469806
[Train] epoch:183	batch id:70	 lr:0.046067 loss:1.493432
[Train] epoch:183	batch id:80	 lr:0.046067 loss:1.492439
[Train] epoch:183	batch id:90	 lr:0.046067 loss:1.527360
[Train] epoch:183	batch id:100	 lr:0.046067 loss:1.499047
[Train] epoch:183	batch id:110	 lr:0.046067 loss:1.445795
[Train] epoch:183	batch id:120	 lr:0.046067 loss:1.541930
[Train] epoch:183	batch id:130	 lr:0.046067 loss:1.372602
[Train] epoch:183	batch id:140	 lr:0.046067 loss:1.454519
[Train] epoch:183	batch id:150	 lr:0.046067 loss:1.477463
[Train] epoch:183	batch id:160	 lr:0.046067 loss:1.455254
[Train] epoch:183	batch id:170	 lr:0.046067 loss:1.477684
[Train] epoch:183	batch id:180	 lr:0.046067 loss:1.471624
[Train] epoch:183	batch id:190	 lr:0.046067 loss:1.386740
[Train] epoch:183	batch id:200	 lr:0.046067 loss:1.558955
[Train] epoch:183	batch id:210	 lr:0.046067 loss:1.389322
[Train] epoch:183	batch id:220	 lr:0.046067 loss:1.493465
[Train] epoch:183	batch id:230	 lr:0.046067 loss:1.418349
[Train] epoch:183	batch id:240	 lr:0.046067 loss:1.514249
[Train] epoch:183	batch id:250	 lr:0.046067 loss:1.614097
[Train] epoch:183	batch id:260	 lr:0.046067 loss:1.441851
[Train] epoch:183	batch id:270	 lr:0.046067 loss:1.456948
[Train] epoch:183	batch id:280	 lr:0.046067 loss:1.406301
[Train] epoch:183	batch id:290	 lr:0.046067 loss:1.405761
[Train] epoch:183	batch id:300	 lr:0.046067 loss:1.499338
[Train] 183, loss: 1.488374, train acc: 0.941470, 
[Test] epoch:183	batch id:0	 loss:1.285597
[Test] epoch:183	batch id:10	 loss:1.384781
[Test] epoch:183	batch id:20	 loss:1.381754
[Test] epoch:183	batch id:30	 loss:1.462272
[Test] epoch:183	batch id:40	 loss:1.343417
[Test] epoch:183	batch id:50	 loss:1.422395
[Test] epoch:183	batch id:60	 loss:1.285075
[Test] epoch:183	batch id:70	 loss:1.455514
[Test] epoch:183	batch id:80	 loss:1.688840
[Test] epoch:183	batch id:90	 loss:1.430491
[Test] epoch:183	batch id:100	 loss:1.859123
[Test] epoch:183	batch id:110	 loss:1.449534
[Test] epoch:183	batch id:120	 loss:1.395990
[Test] epoch:183	batch id:130	 loss:1.460055
[Test] epoch:183	batch id:140	 loss:1.454759
[Test] epoch:183	batch id:150	 loss:1.867356
[Test] 183, loss: 1.479480, test acc: 0.916126,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:184	batch id:0	 lr:0.045624 loss:1.552120
[Train] epoch:184	batch id:10	 lr:0.045624 loss:1.427186
[Train] epoch:184	batch id:20	 lr:0.045624 loss:1.546651
[Train] epoch:184	batch id:30	 lr:0.045624 loss:1.465625
[Train] epoch:184	batch id:40	 lr:0.045624 loss:1.564223
[Train] epoch:184	batch id:50	 lr:0.045624 loss:1.644718
[Train] epoch:184	batch id:60	 lr:0.045624 loss:1.492230
[Train] epoch:184	batch id:70	 lr:0.045624 loss:1.429242
[Train] epoch:184	batch id:80	 lr:0.045624 loss:1.438326
[Train] epoch:184	batch id:90	 lr:0.045624 loss:1.504152
[Train] epoch:184	batch id:100	 lr:0.045624 loss:1.575001
[Train] epoch:184	batch id:110	 lr:0.045624 loss:1.507492
[Train] epoch:184	batch id:120	 lr:0.045624 loss:1.678454
[Train] epoch:184	batch id:130	 lr:0.045624 loss:1.499383
[Train] epoch:184	batch id:140	 lr:0.045624 loss:1.520088
[Train] epoch:184	batch id:150	 lr:0.045624 loss:1.441222
[Train] epoch:184	batch id:160	 lr:0.045624 loss:1.463425
[Train] epoch:184	batch id:170	 lr:0.045624 loss:1.486656
[Train] epoch:184	batch id:180	 lr:0.045624 loss:1.460389
[Train] epoch:184	batch id:190	 lr:0.045624 loss:1.377816
[Train] epoch:184	batch id:200	 lr:0.045624 loss:1.571913
[Train] epoch:184	batch id:210	 lr:0.045624 loss:1.600343
[Train] epoch:184	batch id:220	 lr:0.045624 loss:1.520847
[Train] epoch:184	batch id:230	 lr:0.045624 loss:1.555203
[Train] epoch:184	batch id:240	 lr:0.045624 loss:1.459249
[Train] epoch:184	batch id:250	 lr:0.045624 loss:1.448673
[Train] epoch:184	batch id:260	 lr:0.045624 loss:1.491869
[Train] epoch:184	batch id:270	 lr:0.045624 loss:1.713444
[Train] epoch:184	batch id:280	 lr:0.045624 loss:1.418580
[Train] epoch:184	batch id:290	 lr:0.045624 loss:1.656383
[Train] epoch:184	batch id:300	 lr:0.045624 loss:1.476174
[Train] 184, loss: 1.493175, train acc: 0.937602, 
[Test] epoch:184	batch id:0	 loss:1.292473
[Test] epoch:184	batch id:10	 loss:1.402930
[Test] epoch:184	batch id:20	 loss:1.340826
[Test] epoch:184	batch id:30	 loss:1.454428
[Test] epoch:184	batch id:40	 loss:1.364639
[Test] epoch:184	batch id:50	 loss:1.499825
[Test] epoch:184	batch id:60	 loss:1.283616
[Test] epoch:184	batch id:70	 loss:1.479358
[Test] epoch:184	batch id:80	 loss:1.764302
[Test] epoch:184	batch id:90	 loss:1.677003
[Test] epoch:184	batch id:100	 loss:2.168801
[Test] epoch:184	batch id:110	 loss:1.341779
[Test] epoch:184	batch id:120	 loss:1.383146
[Test] epoch:184	batch id:130	 loss:1.356400
[Test] epoch:184	batch id:140	 loss:1.346686
[Test] epoch:184	batch id:150	 loss:1.719405
[Test] 184, loss: 1.484564, test acc: 0.917342,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:185	batch id:0	 lr:0.045183 loss:1.518890
[Train] epoch:185	batch id:10	 lr:0.045183 loss:1.517679
[Train] epoch:185	batch id:20	 lr:0.045183 loss:1.411027
[Train] epoch:185	batch id:30	 lr:0.045183 loss:1.558279
[Train] epoch:185	batch id:40	 lr:0.045183 loss:1.438749
[Train] epoch:185	batch id:50	 lr:0.045183 loss:1.520144
[Train] epoch:185	batch id:60	 lr:0.045183 loss:1.644406
[Train] epoch:185	batch id:70	 lr:0.045183 loss:1.492961
[Train] epoch:185	batch id:80	 lr:0.045183 loss:1.661481
[Train] epoch:185	batch id:90	 lr:0.045183 loss:1.402380
[Train] epoch:185	batch id:100	 lr:0.045183 loss:1.460821
[Train] epoch:185	batch id:110	 lr:0.045183 loss:1.535633
[Train] epoch:185	batch id:120	 lr:0.045183 loss:1.519846
[Train] epoch:185	batch id:130	 lr:0.045183 loss:1.487033
[Train] epoch:185	batch id:140	 lr:0.045183 loss:1.438350
[Train] epoch:185	batch id:150	 lr:0.045183 loss:1.447187
[Train] epoch:185	batch id:160	 lr:0.045183 loss:1.535287
[Train] epoch:185	batch id:170	 lr:0.045183 loss:1.465269
[Train] epoch:185	batch id:180	 lr:0.045183 loss:1.484646
[Train] epoch:185	batch id:190	 lr:0.045183 loss:1.495947
[Train] epoch:185	batch id:200	 lr:0.045183 loss:1.484717
[Train] epoch:185	batch id:210	 lr:0.045183 loss:1.518501
[Train] epoch:185	batch id:220	 lr:0.045183 loss:1.436412
[Train] epoch:185	batch id:230	 lr:0.045183 loss:1.519877
[Train] epoch:185	batch id:240	 lr:0.045183 loss:1.405585
[Train] epoch:185	batch id:250	 lr:0.045183 loss:1.368164
[Train] epoch:185	batch id:260	 lr:0.045183 loss:1.504632
[Train] epoch:185	batch id:270	 lr:0.045183 loss:1.463690
[Train] epoch:185	batch id:280	 lr:0.045183 loss:1.428757
[Train] epoch:185	batch id:290	 lr:0.045183 loss:1.427239
[Train] epoch:185	batch id:300	 lr:0.045183 loss:1.405613
[Train] 185, loss: 1.483098, train acc: 0.940757, 
[Test] epoch:185	batch id:0	 loss:1.360018
[Test] epoch:185	batch id:10	 loss:1.527427
[Test] epoch:185	batch id:20	 loss:1.502362
[Test] epoch:185	batch id:30	 loss:1.523644
[Test] epoch:185	batch id:40	 loss:1.448031
[Test] epoch:185	batch id:50	 loss:1.562426
[Test] epoch:185	batch id:60	 loss:1.313952
[Test] epoch:185	batch id:70	 loss:1.448722
[Test] epoch:185	batch id:80	 loss:1.672128
[Test] epoch:185	batch id:90	 loss:1.618535
[Test] epoch:185	batch id:100	 loss:1.848550
[Test] epoch:185	batch id:110	 loss:1.480341
[Test] epoch:185	batch id:120	 loss:1.395218
[Test] epoch:185	batch id:130	 loss:1.538445
[Test] epoch:185	batch id:140	 loss:1.322483
[Test] epoch:185	batch id:150	 loss:1.720100
[Test] 185, loss: 1.533487, test acc: 0.906402,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:186	batch id:0	 lr:0.044741 loss:1.539463
[Train] epoch:186	batch id:10	 lr:0.044741 loss:1.385039
[Train] epoch:186	batch id:20	 lr:0.044741 loss:1.650034
[Train] epoch:186	batch id:30	 lr:0.044741 loss:1.485136
[Train] epoch:186	batch id:40	 lr:0.044741 loss:1.434263
[Train] epoch:186	batch id:50	 lr:0.044741 loss:1.520762
[Train] epoch:186	batch id:60	 lr:0.044741 loss:1.432160
[Train] epoch:186	batch id:70	 lr:0.044741 loss:1.353735
[Train] epoch:186	batch id:80	 lr:0.044741 loss:1.594098
[Train] epoch:186	batch id:90	 lr:0.044741 loss:1.581406
[Train] epoch:186	batch id:100	 lr:0.044741 loss:1.431454
[Train] epoch:186	batch id:110	 lr:0.044741 loss:1.418088
[Train] epoch:186	batch id:120	 lr:0.044741 loss:1.396962
[Train] epoch:186	batch id:130	 lr:0.044741 loss:1.487948
[Train] epoch:186	batch id:140	 lr:0.044741 loss:1.496877
[Train] epoch:186	batch id:150	 lr:0.044741 loss:1.424717
[Train] epoch:186	batch id:160	 lr:0.044741 loss:1.432493
[Train] epoch:186	batch id:170	 lr:0.044741 loss:1.461122
[Train] epoch:186	batch id:180	 lr:0.044741 loss:1.417637
[Train] epoch:186	batch id:190	 lr:0.044741 loss:1.564661
[Train] epoch:186	batch id:200	 lr:0.044741 loss:1.625517
[Train] epoch:186	batch id:210	 lr:0.044741 loss:1.380541
[Train] epoch:186	batch id:220	 lr:0.044741 loss:1.611462
[Train] epoch:186	batch id:230	 lr:0.044741 loss:1.482713
[Train] epoch:186	batch id:240	 lr:0.044741 loss:1.405581
[Train] epoch:186	batch id:250	 lr:0.044741 loss:1.525052
[Train] epoch:186	batch id:260	 lr:0.044741 loss:1.455989
[Train] epoch:186	batch id:270	 lr:0.044741 loss:1.474280
[Train] epoch:186	batch id:280	 lr:0.044741 loss:1.452424
[Train] epoch:186	batch id:290	 lr:0.044741 loss:1.600691
[Train] epoch:186	batch id:300	 lr:0.044741 loss:1.582331
[Train] 186, loss: 1.486926, train acc: 0.939943, 
[Test] epoch:186	batch id:0	 loss:1.317445
[Test] epoch:186	batch id:10	 loss:1.457371
[Test] epoch:186	batch id:20	 loss:1.352086
[Test] epoch:186	batch id:30	 loss:1.371111
[Test] epoch:186	batch id:40	 loss:1.371963
[Test] epoch:186	batch id:50	 loss:1.378959
[Test] epoch:186	batch id:60	 loss:1.288166
[Test] epoch:186	batch id:70	 loss:1.378699
[Test] epoch:186	batch id:80	 loss:1.552379
[Test] epoch:186	batch id:90	 loss:1.640696
[Test] epoch:186	batch id:100	 loss:1.848471
[Test] epoch:186	batch id:110	 loss:1.430357
[Test] epoch:186	batch id:120	 loss:1.352331
[Test] epoch:186	batch id:130	 loss:1.407557
[Test] epoch:186	batch id:140	 loss:1.377821
[Test] epoch:186	batch id:150	 loss:1.823581
[Test] 186, loss: 1.486518, test acc: 0.912480,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:187	batch id:0	 lr:0.044300 loss:1.424574
[Train] epoch:187	batch id:10	 lr:0.044300 loss:1.366358
[Train] epoch:187	batch id:20	 lr:0.044300 loss:1.557273
[Train] epoch:187	batch id:30	 lr:0.044300 loss:1.403542
[Train] epoch:187	batch id:40	 lr:0.044300 loss:1.476890
[Train] epoch:187	batch id:50	 lr:0.044300 loss:1.351067
[Train] epoch:187	batch id:60	 lr:0.044300 loss:1.485579
[Train] epoch:187	batch id:70	 lr:0.044300 loss:1.493673
[Train] epoch:187	batch id:80	 lr:0.044300 loss:1.394318
[Train] epoch:187	batch id:90	 lr:0.044300 loss:1.372690
[Train] epoch:187	batch id:100	 lr:0.044300 loss:1.562626
[Train] epoch:187	batch id:110	 lr:0.044300 loss:1.462738
[Train] epoch:187	batch id:120	 lr:0.044300 loss:1.465687
[Train] epoch:187	batch id:130	 lr:0.044300 loss:1.446785
[Train] epoch:187	batch id:140	 lr:0.044300 loss:1.359739
[Train] epoch:187	batch id:150	 lr:0.044300 loss:1.614323
[Train] epoch:187	batch id:160	 lr:0.044300 loss:1.494273
[Train] epoch:187	batch id:170	 lr:0.044300 loss:1.461606
[Train] epoch:187	batch id:180	 lr:0.044300 loss:1.493805
[Train] epoch:187	batch id:190	 lr:0.044300 loss:1.521147
[Train] epoch:187	batch id:200	 lr:0.044300 loss:1.401579
[Train] epoch:187	batch id:210	 lr:0.044300 loss:1.437562
[Train] epoch:187	batch id:220	 lr:0.044300 loss:1.579951
[Train] epoch:187	batch id:230	 lr:0.044300 loss:1.563244
[Train] epoch:187	batch id:240	 lr:0.044300 loss:1.523237
[Train] epoch:187	batch id:250	 lr:0.044300 loss:1.501146
[Train] epoch:187	batch id:260	 lr:0.044300 loss:1.429107
[Train] epoch:187	batch id:270	 lr:0.044300 loss:1.528900
[Train] epoch:187	batch id:280	 lr:0.044300 loss:1.487455
[Train] epoch:187	batch id:290	 lr:0.044300 loss:1.557323
[Train] epoch:187	batch id:300	 lr:0.044300 loss:1.390252
[Train] 187, loss: 1.478984, train acc: 0.944218, 
[Test] epoch:187	batch id:0	 loss:1.305582
[Test] epoch:187	batch id:10	 loss:1.374709
[Test] epoch:187	batch id:20	 loss:1.432318
[Test] epoch:187	batch id:30	 loss:1.476509
[Test] epoch:187	batch id:40	 loss:1.406811
[Test] epoch:187	batch id:50	 loss:1.493296
[Test] epoch:187	batch id:60	 loss:1.276348
[Test] epoch:187	batch id:70	 loss:1.463969
[Test] epoch:187	batch id:80	 loss:1.615050
[Test] epoch:187	batch id:90	 loss:1.497655
[Test] epoch:187	batch id:100	 loss:1.892416
[Test] epoch:187	batch id:110	 loss:1.574187
[Test] epoch:187	batch id:120	 loss:1.354426
[Test] epoch:187	batch id:130	 loss:1.457809
[Test] epoch:187	batch id:140	 loss:1.323837
[Test] epoch:187	batch id:150	 loss:1.823963
[Test] 187, loss: 1.490977, test acc: 0.905997,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:188	batch id:0	 lr:0.043859 loss:1.416351
[Train] epoch:188	batch id:10	 lr:0.043859 loss:1.483644
[Train] epoch:188	batch id:20	 lr:0.043859 loss:1.663582
[Train] epoch:188	batch id:30	 lr:0.043859 loss:1.442176
[Train] epoch:188	batch id:40	 lr:0.043859 loss:1.405567
[Train] epoch:188	batch id:50	 lr:0.043859 loss:1.427732
[Train] epoch:188	batch id:60	 lr:0.043859 loss:1.549093
[Train] epoch:188	batch id:70	 lr:0.043859 loss:1.481005
[Train] epoch:188	batch id:80	 lr:0.043859 loss:1.477644
[Train] epoch:188	batch id:90	 lr:0.043859 loss:1.424763
[Train] epoch:188	batch id:100	 lr:0.043859 loss:1.438200
[Train] epoch:188	batch id:110	 lr:0.043859 loss:1.425063
[Train] epoch:188	batch id:120	 lr:0.043859 loss:1.365700
[Train] epoch:188	batch id:130	 lr:0.043859 loss:1.368930
[Train] epoch:188	batch id:140	 lr:0.043859 loss:1.472439
[Train] epoch:188	batch id:150	 lr:0.043859 loss:1.438554
[Train] epoch:188	batch id:160	 lr:0.043859 loss:1.534597
[Train] epoch:188	batch id:170	 lr:0.043859 loss:1.433104
[Train] epoch:188	batch id:180	 lr:0.043859 loss:1.441404
[Train] epoch:188	batch id:190	 lr:0.043859 loss:1.574836
[Train] epoch:188	batch id:200	 lr:0.043859 loss:1.459688
[Train] epoch:188	batch id:210	 lr:0.043859 loss:1.707991
[Train] epoch:188	batch id:220	 lr:0.043859 loss:1.530158
[Train] epoch:188	batch id:230	 lr:0.043859 loss:1.392595
[Train] epoch:188	batch id:240	 lr:0.043859 loss:1.578887
[Train] epoch:188	batch id:250	 lr:0.043859 loss:1.539913
[Train] epoch:188	batch id:260	 lr:0.043859 loss:1.515547
[Train] epoch:188	batch id:270	 lr:0.043859 loss:1.488431
[Train] epoch:188	batch id:280	 lr:0.043859 loss:1.458924
[Train] epoch:188	batch id:290	 lr:0.043859 loss:1.474421
[Train] epoch:188	batch id:300	 lr:0.043859 loss:1.462102
[Train] 188, loss: 1.479024, train acc: 0.941979, 
[Test] epoch:188	batch id:0	 loss:1.316815
[Test] epoch:188	batch id:10	 loss:1.402791
[Test] epoch:188	batch id:20	 loss:1.379785
[Test] epoch:188	batch id:30	 loss:1.448163
[Test] epoch:188	batch id:40	 loss:1.370436
[Test] epoch:188	batch id:50	 loss:1.492108
[Test] epoch:188	batch id:60	 loss:1.285460
[Test] epoch:188	batch id:70	 loss:1.477320
[Test] epoch:188	batch id:80	 loss:1.665082
[Test] epoch:188	batch id:90	 loss:1.606018
[Test] epoch:188	batch id:100	 loss:2.030957
[Test] epoch:188	batch id:110	 loss:1.439907
[Test] epoch:188	batch id:120	 loss:1.342977
[Test] epoch:188	batch id:130	 loss:1.427217
[Test] epoch:188	batch id:140	 loss:1.327129
[Test] epoch:188	batch id:150	 loss:1.900587
[Test] 188, loss: 1.500021, test acc: 0.910049,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:189	batch id:0	 lr:0.043419 loss:1.480462
[Train] epoch:189	batch id:10	 lr:0.043419 loss:1.423261
[Train] epoch:189	batch id:20	 lr:0.043419 loss:1.375443
[Train] epoch:189	batch id:30	 lr:0.043419 loss:1.389938
[Train] epoch:189	batch id:40	 lr:0.043419 loss:1.428266
[Train] epoch:189	batch id:50	 lr:0.043419 loss:1.544084
[Train] epoch:189	batch id:60	 lr:0.043419 loss:1.464977
[Train] epoch:189	batch id:70	 lr:0.043419 loss:1.480193
[Train] epoch:189	batch id:80	 lr:0.043419 loss:1.421386
[Train] epoch:189	batch id:90	 lr:0.043419 loss:1.502947
[Train] epoch:189	batch id:100	 lr:0.043419 loss:1.464454
[Train] epoch:189	batch id:110	 lr:0.043419 loss:1.535013
[Train] epoch:189	batch id:120	 lr:0.043419 loss:1.478951
[Train] epoch:189	batch id:130	 lr:0.043419 loss:1.560379
[Train] epoch:189	batch id:140	 lr:0.043419 loss:1.448495
[Train] epoch:189	batch id:150	 lr:0.043419 loss:1.413396
[Train] epoch:189	batch id:160	 lr:0.043419 loss:1.467398
[Train] epoch:189	batch id:170	 lr:0.043419 loss:1.457709
[Train] epoch:189	batch id:180	 lr:0.043419 loss:1.395921
[Train] epoch:189	batch id:190	 lr:0.043419 loss:1.605332
[Train] epoch:189	batch id:200	 lr:0.043419 loss:1.432572
[Train] epoch:189	batch id:210	 lr:0.043419 loss:1.400016
[Train] epoch:189	batch id:220	 lr:0.043419 loss:1.445231
[Train] epoch:189	batch id:230	 lr:0.043419 loss:1.400618
[Train] epoch:189	batch id:240	 lr:0.043419 loss:1.590577
[Train] epoch:189	batch id:250	 lr:0.043419 loss:1.443633
[Train] epoch:189	batch id:260	 lr:0.043419 loss:1.608323
[Train] epoch:189	batch id:270	 lr:0.043419 loss:1.523998
[Train] epoch:189	batch id:280	 lr:0.043419 loss:1.559927
[Train] epoch:189	batch id:290	 lr:0.043419 loss:1.452109
[Train] epoch:189	batch id:300	 lr:0.043419 loss:1.471352
[Train] 189, loss: 1.483668, train acc: 0.941368, 
[Test] epoch:189	batch id:0	 loss:1.333965
[Test] epoch:189	batch id:10	 loss:1.587728
[Test] epoch:189	batch id:20	 loss:1.513379
[Test] epoch:189	batch id:30	 loss:1.483408
[Test] epoch:189	batch id:40	 loss:1.443389
[Test] epoch:189	batch id:50	 loss:1.435572
[Test] epoch:189	batch id:60	 loss:1.292309
[Test] epoch:189	batch id:70	 loss:1.384029
[Test] epoch:189	batch id:80	 loss:1.587763
[Test] epoch:189	batch id:90	 loss:1.519268
[Test] epoch:189	batch id:100	 loss:1.846960
[Test] epoch:189	batch id:110	 loss:1.394507
[Test] epoch:189	batch id:120	 loss:1.352183
[Test] epoch:189	batch id:130	 loss:1.617774
[Test] epoch:189	batch id:140	 loss:1.360804
[Test] epoch:189	batch id:150	 loss:1.853585
[Test] 189, loss: 1.526895, test acc: 0.901945,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:190	batch id:0	 lr:0.042980 loss:1.673311
[Train] epoch:190	batch id:10	 lr:0.042980 loss:1.490373
[Train] epoch:190	batch id:20	 lr:0.042980 loss:1.397385
[Train] epoch:190	batch id:30	 lr:0.042980 loss:1.486688
[Train] epoch:190	batch id:40	 lr:0.042980 loss:1.535745
[Train] epoch:190	batch id:50	 lr:0.042980 loss:1.472499
[Train] epoch:190	batch id:60	 lr:0.042980 loss:1.498673
[Train] epoch:190	batch id:70	 lr:0.042980 loss:1.440289
[Train] epoch:190	batch id:80	 lr:0.042980 loss:1.627526
[Train] epoch:190	batch id:90	 lr:0.042980 loss:1.464163
[Train] epoch:190	batch id:100	 lr:0.042980 loss:1.539453
[Train] epoch:190	batch id:110	 lr:0.042980 loss:1.487104
[Train] epoch:190	batch id:120	 lr:0.042980 loss:1.437795
[Train] epoch:190	batch id:130	 lr:0.042980 loss:1.516417
[Train] epoch:190	batch id:140	 lr:0.042980 loss:1.374356
[Train] epoch:190	batch id:150	 lr:0.042980 loss:1.565576
[Train] epoch:190	batch id:160	 lr:0.042980 loss:1.393493
[Train] epoch:190	batch id:170	 lr:0.042980 loss:1.480434
[Train] epoch:190	batch id:180	 lr:0.042980 loss:1.483966
[Train] epoch:190	batch id:190	 lr:0.042980 loss:1.522904
[Train] epoch:190	batch id:200	 lr:0.042980 loss:1.476192
[Train] epoch:190	batch id:210	 lr:0.042980 loss:1.460551
[Train] epoch:190	batch id:220	 lr:0.042980 loss:1.475445
[Train] epoch:190	batch id:230	 lr:0.042980 loss:1.538044
[Train] epoch:190	batch id:240	 lr:0.042980 loss:1.423448
[Train] epoch:190	batch id:250	 lr:0.042980 loss:1.503925
[Train] epoch:190	batch id:260	 lr:0.042980 loss:1.503734
[Train] epoch:190	batch id:270	 lr:0.042980 loss:1.468101
[Train] epoch:190	batch id:280	 lr:0.042980 loss:1.448724
[Train] epoch:190	batch id:290	 lr:0.042980 loss:1.469633
[Train] epoch:190	batch id:300	 lr:0.042980 loss:1.488316
[Train] 190, loss: 1.476781, train acc: 0.946254, 
[Test] epoch:190	batch id:0	 loss:1.314495
[Test] epoch:190	batch id:10	 loss:1.487925
[Test] epoch:190	batch id:20	 loss:1.343287
[Test] epoch:190	batch id:30	 loss:1.409150
[Test] epoch:190	batch id:40	 loss:1.383825
[Test] epoch:190	batch id:50	 loss:1.483847
[Test] epoch:190	batch id:60	 loss:1.266027
[Test] epoch:190	batch id:70	 loss:1.320769
[Test] epoch:190	batch id:80	 loss:1.593426
[Test] epoch:190	batch id:90	 loss:1.597305
[Test] epoch:190	batch id:100	 loss:2.000087
[Test] epoch:190	batch id:110	 loss:1.401801
[Test] epoch:190	batch id:120	 loss:1.399290
[Test] epoch:190	batch id:130	 loss:1.478921
[Test] epoch:190	batch id:140	 loss:1.308996
[Test] epoch:190	batch id:150	 loss:1.771690
[Test] 190, loss: 1.491193, test acc: 0.904781,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:191	batch id:0	 lr:0.042541 loss:1.505612
[Train] epoch:191	batch id:10	 lr:0.042541 loss:1.469598
[Train] epoch:191	batch id:20	 lr:0.042541 loss:1.396681
[Train] epoch:191	batch id:30	 lr:0.042541 loss:1.435785
[Train] epoch:191	batch id:40	 lr:0.042541 loss:1.391013
[Train] epoch:191	batch id:50	 lr:0.042541 loss:1.573431
[Train] epoch:191	batch id:60	 lr:0.042541 loss:1.687518
[Train] epoch:191	batch id:70	 lr:0.042541 loss:1.579130
[Train] epoch:191	batch id:80	 lr:0.042541 loss:1.381202
[Train] epoch:191	batch id:90	 lr:0.042541 loss:1.422169
[Train] epoch:191	batch id:100	 lr:0.042541 loss:1.437278
[Train] epoch:191	batch id:110	 lr:0.042541 loss:1.643939
[Train] epoch:191	batch id:120	 lr:0.042541 loss:1.454198
[Train] epoch:191	batch id:130	 lr:0.042541 loss:1.646194
[Train] epoch:191	batch id:140	 lr:0.042541 loss:1.405280
[Train] epoch:191	batch id:150	 lr:0.042541 loss:1.459342
[Train] epoch:191	batch id:160	 lr:0.042541 loss:1.633950
[Train] epoch:191	batch id:170	 lr:0.042541 loss:1.382270
[Train] epoch:191	batch id:180	 lr:0.042541 loss:1.416274
[Train] epoch:191	batch id:190	 lr:0.042541 loss:1.619930
[Train] epoch:191	batch id:200	 lr:0.042541 loss:1.440160
[Train] epoch:191	batch id:210	 lr:0.042541 loss:1.368366
[Train] epoch:191	batch id:220	 lr:0.042541 loss:1.466111
[Train] epoch:191	batch id:230	 lr:0.042541 loss:1.462759
[Train] epoch:191	batch id:240	 lr:0.042541 loss:1.580523
[Train] epoch:191	batch id:250	 lr:0.042541 loss:1.454974
[Train] epoch:191	batch id:260	 lr:0.042541 loss:1.466235
[Train] epoch:191	batch id:270	 lr:0.042541 loss:1.444177
[Train] epoch:191	batch id:280	 lr:0.042541 loss:1.391793
[Train] epoch:191	batch id:290	 lr:0.042541 loss:1.511714
[Train] epoch:191	batch id:300	 lr:0.042541 loss:1.557591
[Train] 191, loss: 1.480345, train acc: 0.941266, 
[Test] epoch:191	batch id:0	 loss:1.312871
[Test] epoch:191	batch id:10	 loss:1.479999
[Test] epoch:191	batch id:20	 loss:1.325119
[Test] epoch:191	batch id:30	 loss:1.369486
[Test] epoch:191	batch id:40	 loss:1.389101
[Test] epoch:191	batch id:50	 loss:1.519095
[Test] epoch:191	batch id:60	 loss:1.279126
[Test] epoch:191	batch id:70	 loss:1.348246
[Test] epoch:191	batch id:80	 loss:1.650707
[Test] epoch:191	batch id:90	 loss:1.761182
[Test] epoch:191	batch id:100	 loss:1.895495
[Test] epoch:191	batch id:110	 loss:1.337286
[Test] epoch:191	batch id:120	 loss:1.602331
[Test] epoch:191	batch id:130	 loss:1.498612
[Test] epoch:191	batch id:140	 loss:1.384903
[Test] epoch:191	batch id:150	 loss:1.699593
[Test] 191, loss: 1.523387, test acc: 0.891005,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:192	batch id:0	 lr:0.042103 loss:1.382007
[Train] epoch:192	batch id:10	 lr:0.042103 loss:1.450396
[Train] epoch:192	batch id:20	 lr:0.042103 loss:1.489069
[Train] epoch:192	batch id:30	 lr:0.042103 loss:1.561490
[Train] epoch:192	batch id:40	 lr:0.042103 loss:1.515434
[Train] epoch:192	batch id:50	 lr:0.042103 loss:1.341274
[Train] epoch:192	batch id:60	 lr:0.042103 loss:1.396898
[Train] epoch:192	batch id:70	 lr:0.042103 loss:1.444980
[Train] epoch:192	batch id:80	 lr:0.042103 loss:1.414963
[Train] epoch:192	batch id:90	 lr:0.042103 loss:1.576020
[Train] epoch:192	batch id:100	 lr:0.042103 loss:1.440445
[Train] epoch:192	batch id:110	 lr:0.042103 loss:1.421541
[Train] epoch:192	batch id:120	 lr:0.042103 loss:1.430445
[Train] epoch:192	batch id:130	 lr:0.042103 loss:1.656238
[Train] epoch:192	batch id:140	 lr:0.042103 loss:1.463570
[Train] epoch:192	batch id:150	 lr:0.042103 loss:1.445994
[Train] epoch:192	batch id:160	 lr:0.042103 loss:1.398908
[Train] epoch:192	batch id:170	 lr:0.042103 loss:1.412944
[Train] epoch:192	batch id:180	 lr:0.042103 loss:1.474155
[Train] epoch:192	batch id:190	 lr:0.042103 loss:1.518403
[Train] epoch:192	batch id:200	 lr:0.042103 loss:1.563084
[Train] epoch:192	batch id:210	 lr:0.042103 loss:1.413492
[Train] epoch:192	batch id:220	 lr:0.042103 loss:1.547258
[Train] epoch:192	batch id:230	 lr:0.042103 loss:1.532123
[Train] epoch:192	batch id:240	 lr:0.042103 loss:1.428859
[Train] epoch:192	batch id:250	 lr:0.042103 loss:1.475767
[Train] epoch:192	batch id:260	 lr:0.042103 loss:1.499653
[Train] epoch:192	batch id:270	 lr:0.042103 loss:1.454848
[Train] epoch:192	batch id:280	 lr:0.042103 loss:1.480032
[Train] epoch:192	batch id:290	 lr:0.042103 loss:1.444346
[Train] epoch:192	batch id:300	 lr:0.042103 loss:1.392320
[Train] 192, loss: 1.473275, train acc: 0.947170, 
[Test] epoch:192	batch id:0	 loss:1.302553
[Test] epoch:192	batch id:10	 loss:1.488115
[Test] epoch:192	batch id:20	 loss:1.418363
[Test] epoch:192	batch id:30	 loss:1.356588
[Test] epoch:192	batch id:40	 loss:1.403310
[Test] epoch:192	batch id:50	 loss:1.503003
[Test] epoch:192	batch id:60	 loss:1.276813
[Test] epoch:192	batch id:70	 loss:1.380310
[Test] epoch:192	batch id:80	 loss:1.665354
[Test] epoch:192	batch id:90	 loss:1.629932
[Test] epoch:192	batch id:100	 loss:1.982533
[Test] epoch:192	batch id:110	 loss:1.364408
[Test] epoch:192	batch id:120	 loss:1.348671
[Test] epoch:192	batch id:130	 loss:1.378660
[Test] epoch:192	batch id:140	 loss:1.336759
[Test] epoch:192	batch id:150	 loss:1.677186
[Test] 192, loss: 1.483348, test acc: 0.911669,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:193	batch id:0	 lr:0.041665 loss:1.468913
[Train] epoch:193	batch id:10	 lr:0.041665 loss:1.433171
[Train] epoch:193	batch id:20	 lr:0.041665 loss:1.522293
[Train] epoch:193	batch id:30	 lr:0.041665 loss:1.429729
[Train] epoch:193	batch id:40	 lr:0.041665 loss:1.440635
[Train] epoch:193	batch id:50	 lr:0.041665 loss:1.482349
[Train] epoch:193	batch id:60	 lr:0.041665 loss:1.542401
[Train] epoch:193	batch id:70	 lr:0.041665 loss:1.442363
[Train] epoch:193	batch id:80	 lr:0.041665 loss:1.490993
[Train] epoch:193	batch id:90	 lr:0.041665 loss:1.432768
[Train] epoch:193	batch id:100	 lr:0.041665 loss:1.519380
[Train] epoch:193	batch id:110	 lr:0.041665 loss:1.461246
[Train] epoch:193	batch id:120	 lr:0.041665 loss:1.474549
[Train] epoch:193	batch id:130	 lr:0.041665 loss:1.401146
[Train] epoch:193	batch id:140	 lr:0.041665 loss:1.488086
[Train] epoch:193	batch id:150	 lr:0.041665 loss:1.419164
[Train] epoch:193	batch id:160	 lr:0.041665 loss:1.388766
[Train] epoch:193	batch id:170	 lr:0.041665 loss:1.584108
[Train] epoch:193	batch id:180	 lr:0.041665 loss:1.543236
[Train] epoch:193	batch id:190	 lr:0.041665 loss:1.542692
[Train] epoch:193	batch id:200	 lr:0.041665 loss:1.497129
[Train] epoch:193	batch id:210	 lr:0.041665 loss:1.481867
[Train] epoch:193	batch id:220	 lr:0.041665 loss:1.542323
[Train] epoch:193	batch id:230	 lr:0.041665 loss:1.492964
[Train] epoch:193	batch id:240	 lr:0.041665 loss:1.555061
[Train] epoch:193	batch id:250	 lr:0.041665 loss:1.421746
[Train] epoch:193	batch id:260	 lr:0.041665 loss:1.667430
[Train] epoch:193	batch id:270	 lr:0.041665 loss:1.461447
[Train] epoch:193	batch id:280	 lr:0.041665 loss:1.477555
[Train] epoch:193	batch id:290	 lr:0.041665 loss:1.702520
[Train] epoch:193	batch id:300	 lr:0.041665 loss:1.467817
[Train] 193, loss: 1.474666, train acc: 0.946356, 
[Test] epoch:193	batch id:0	 loss:1.307392
[Test] epoch:193	batch id:10	 loss:1.371215
[Test] epoch:193	batch id:20	 loss:1.384284
[Test] epoch:193	batch id:30	 loss:1.373343
[Test] epoch:193	batch id:40	 loss:1.354128
[Test] epoch:193	batch id:50	 loss:1.522458
[Test] epoch:193	batch id:60	 loss:1.289341
[Test] epoch:193	batch id:70	 loss:1.501100
[Test] epoch:193	batch id:80	 loss:1.671621
[Test] epoch:193	batch id:90	 loss:1.452781
[Test] epoch:193	batch id:100	 loss:1.893954
[Test] epoch:193	batch id:110	 loss:1.366850
[Test] epoch:193	batch id:120	 loss:1.440063
[Test] epoch:193	batch id:130	 loss:1.365736
[Test] epoch:193	batch id:140	 loss:1.356876
[Test] epoch:193	batch id:150	 loss:1.694954
[Test] 193, loss: 1.486669, test acc: 0.911264,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:194	batch id:0	 lr:0.041229 loss:1.542937
[Train] epoch:194	batch id:10	 lr:0.041229 loss:1.410625
[Train] epoch:194	batch id:20	 lr:0.041229 loss:1.680523
[Train] epoch:194	batch id:30	 lr:0.041229 loss:1.471969
[Train] epoch:194	batch id:40	 lr:0.041229 loss:1.450000
[Train] epoch:194	batch id:50	 lr:0.041229 loss:1.493609
[Train] epoch:194	batch id:60	 lr:0.041229 loss:1.422960
[Train] epoch:194	batch id:70	 lr:0.041229 loss:1.532802
[Train] epoch:194	batch id:80	 lr:0.041229 loss:1.400700
[Train] epoch:194	batch id:90	 lr:0.041229 loss:1.464483
[Train] epoch:194	batch id:100	 lr:0.041229 loss:1.379800
[Train] epoch:194	batch id:110	 lr:0.041229 loss:1.547517
[Train] epoch:194	batch id:120	 lr:0.041229 loss:1.544294
[Train] epoch:194	batch id:130	 lr:0.041229 loss:1.440934
[Train] epoch:194	batch id:140	 lr:0.041229 loss:1.469157
[Train] epoch:194	batch id:150	 lr:0.041229 loss:1.461394
[Train] epoch:194	batch id:160	 lr:0.041229 loss:1.426955
[Train] epoch:194	batch id:170	 lr:0.041229 loss:1.393536
[Train] epoch:194	batch id:180	 lr:0.041229 loss:1.546998
[Train] epoch:194	batch id:190	 lr:0.041229 loss:1.551442
[Train] epoch:194	batch id:200	 lr:0.041229 loss:1.458191
[Train] epoch:194	batch id:210	 lr:0.041229 loss:1.509730
[Train] epoch:194	batch id:220	 lr:0.041229 loss:1.537192
[Train] epoch:194	batch id:230	 lr:0.041229 loss:1.417083
[Train] epoch:194	batch id:240	 lr:0.041229 loss:1.498546
[Train] epoch:194	batch id:250	 lr:0.041229 loss:1.369128
[Train] epoch:194	batch id:260	 lr:0.041229 loss:1.429133
[Train] epoch:194	batch id:270	 lr:0.041229 loss:1.462982
[Train] epoch:194	batch id:280	 lr:0.041229 loss:1.417574
[Train] epoch:194	batch id:290	 lr:0.041229 loss:1.455200
[Train] epoch:194	batch id:300	 lr:0.041229 loss:1.503123
[Train] 194, loss: 1.468159, train acc: 0.947068, 
[Test] epoch:194	batch id:0	 loss:1.312358
[Test] epoch:194	batch id:10	 loss:1.425474
[Test] epoch:194	batch id:20	 loss:1.473671
[Test] epoch:194	batch id:30	 loss:1.425182
[Test] epoch:194	batch id:40	 loss:1.363520
[Test] epoch:194	batch id:50	 loss:1.506026
[Test] epoch:194	batch id:60	 loss:1.336435
[Test] epoch:194	batch id:70	 loss:1.593900
[Test] epoch:194	batch id:80	 loss:1.624123
[Test] epoch:194	batch id:90	 loss:1.470645
[Test] epoch:194	batch id:100	 loss:1.863392
[Test] epoch:194	batch id:110	 loss:1.446127
[Test] epoch:194	batch id:120	 loss:1.349301
[Test] epoch:194	batch id:130	 loss:1.390223
[Test] epoch:194	batch id:140	 loss:1.370052
[Test] epoch:194	batch id:150	 loss:1.782290
[Test] 194, loss: 1.493737, test acc: 0.907212,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:195	batch id:0	 lr:0.040793 loss:1.386914
[Train] epoch:195	batch id:10	 lr:0.040793 loss:1.406951
[Train] epoch:195	batch id:20	 lr:0.040793 loss:1.568733
[Train] epoch:195	batch id:30	 lr:0.040793 loss:1.386163
[Train] epoch:195	batch id:40	 lr:0.040793 loss:1.594791
[Train] epoch:195	batch id:50	 lr:0.040793 loss:1.445610
[Train] epoch:195	batch id:60	 lr:0.040793 loss:1.407007
[Train] epoch:195	batch id:70	 lr:0.040793 loss:1.442336
[Train] epoch:195	batch id:80	 lr:0.040793 loss:1.427051
[Train] epoch:195	batch id:90	 lr:0.040793 loss:1.483008
[Train] epoch:195	batch id:100	 lr:0.040793 loss:1.491781
[Train] epoch:195	batch id:110	 lr:0.040793 loss:1.424795
[Train] epoch:195	batch id:120	 lr:0.040793 loss:1.615839
[Train] epoch:195	batch id:130	 lr:0.040793 loss:1.411162
[Train] epoch:195	batch id:140	 lr:0.040793 loss:1.592958
[Train] epoch:195	batch id:150	 lr:0.040793 loss:1.465662
[Train] epoch:195	batch id:160	 lr:0.040793 loss:1.393816
[Train] epoch:195	batch id:170	 lr:0.040793 loss:1.397247
[Train] epoch:195	batch id:180	 lr:0.040793 loss:1.439909
[Train] epoch:195	batch id:190	 lr:0.040793 loss:1.437310
[Train] epoch:195	batch id:200	 lr:0.040793 loss:1.465049
[Train] epoch:195	batch id:210	 lr:0.040793 loss:1.552937
[Train] epoch:195	batch id:220	 lr:0.040793 loss:1.397994
[Train] epoch:195	batch id:230	 lr:0.040793 loss:1.348248
[Train] epoch:195	batch id:240	 lr:0.040793 loss:1.428122
[Train] epoch:195	batch id:250	 lr:0.040793 loss:1.643582
[Train] epoch:195	batch id:260	 lr:0.040793 loss:1.571282
[Train] epoch:195	batch id:270	 lr:0.040793 loss:1.784834
[Train] epoch:195	batch id:280	 lr:0.040793 loss:1.552042
[Train] epoch:195	batch id:290	 lr:0.040793 loss:1.579241
[Train] epoch:195	batch id:300	 lr:0.040793 loss:1.524285
[Train] 195, loss: 1.472414, train acc: 0.945949, 
[Test] epoch:195	batch id:0	 loss:1.282706
[Test] epoch:195	batch id:10	 loss:1.454594
[Test] epoch:195	batch id:20	 loss:1.396084
[Test] epoch:195	batch id:30	 loss:1.411399
[Test] epoch:195	batch id:40	 loss:1.462290
[Test] epoch:195	batch id:50	 loss:1.528353
[Test] epoch:195	batch id:60	 loss:1.269531
[Test] epoch:195	batch id:70	 loss:1.400717
[Test] epoch:195	batch id:80	 loss:1.694683
[Test] epoch:195	batch id:90	 loss:1.514928
[Test] epoch:195	batch id:100	 loss:2.044310
[Test] epoch:195	batch id:110	 loss:1.369056
[Test] epoch:195	batch id:120	 loss:1.408557
[Test] epoch:195	batch id:130	 loss:1.438657
[Test] epoch:195	batch id:140	 loss:1.477767
[Test] epoch:195	batch id:150	 loss:1.842993
[Test] 195, loss: 1.497066, test acc: 0.905186,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:196	batch id:0	 lr:0.040357 loss:1.474318
[Train] epoch:196	batch id:10	 lr:0.040357 loss:1.462523
[Train] epoch:196	batch id:20	 lr:0.040357 loss:1.449072
[Train] epoch:196	batch id:30	 lr:0.040357 loss:1.489829
[Train] epoch:196	batch id:40	 lr:0.040357 loss:1.366070
[Train] epoch:196	batch id:50	 lr:0.040357 loss:1.462447
[Train] epoch:196	batch id:60	 lr:0.040357 loss:1.423605
[Train] epoch:196	batch id:70	 lr:0.040357 loss:1.363158
[Train] epoch:196	batch id:80	 lr:0.040357 loss:1.573044
[Train] epoch:196	batch id:90	 lr:0.040357 loss:1.462215
[Train] epoch:196	batch id:100	 lr:0.040357 loss:1.404367
[Train] epoch:196	batch id:110	 lr:0.040357 loss:1.464217
[Train] epoch:196	batch id:120	 lr:0.040357 loss:1.505670
[Train] epoch:196	batch id:130	 lr:0.040357 loss:1.506904
[Train] epoch:196	batch id:140	 lr:0.040357 loss:1.376376
[Train] epoch:196	batch id:150	 lr:0.040357 loss:1.461190
[Train] epoch:196	batch id:160	 lr:0.040357 loss:1.468377
[Train] epoch:196	batch id:170	 lr:0.040357 loss:1.472758
[Train] epoch:196	batch id:180	 lr:0.040357 loss:1.401000
[Train] epoch:196	batch id:190	 lr:0.040357 loss:1.591878
[Train] epoch:196	batch id:200	 lr:0.040357 loss:1.494971
[Train] epoch:196	batch id:210	 lr:0.040357 loss:1.449916
[Train] epoch:196	batch id:220	 lr:0.040357 loss:1.403300
[Train] epoch:196	batch id:230	 lr:0.040357 loss:1.537313
[Train] epoch:196	batch id:240	 lr:0.040357 loss:1.414332
[Train] epoch:196	batch id:250	 lr:0.040357 loss:1.472305
[Train] epoch:196	batch id:260	 lr:0.040357 loss:1.440602
[Train] epoch:196	batch id:270	 lr:0.040357 loss:1.393438
[Train] epoch:196	batch id:280	 lr:0.040357 loss:1.532308
[Train] epoch:196	batch id:290	 lr:0.040357 loss:1.490734
[Train] epoch:196	batch id:300	 lr:0.040357 loss:1.390426
[Train] 196, loss: 1.467573, train acc: 0.949511, 
[Test] epoch:196	batch id:0	 loss:1.306407
[Test] epoch:196	batch id:10	 loss:1.388989
[Test] epoch:196	batch id:20	 loss:1.519228
[Test] epoch:196	batch id:30	 loss:1.441833
[Test] epoch:196	batch id:40	 loss:1.343863
[Test] epoch:196	batch id:50	 loss:1.510757
[Test] epoch:196	batch id:60	 loss:1.264535
[Test] epoch:196	batch id:70	 loss:1.410697
[Test] epoch:196	batch id:80	 loss:1.623527
[Test] epoch:196	batch id:90	 loss:1.514559
[Test] epoch:196	batch id:100	 loss:1.777648
[Test] epoch:196	batch id:110	 loss:1.382328
[Test] epoch:196	batch id:120	 loss:1.478066
[Test] epoch:196	batch id:130	 loss:1.400689
[Test] epoch:196	batch id:140	 loss:1.464237
[Test] epoch:196	batch id:150	 loss:1.886926
[Test] 196, loss: 1.511988, test acc: 0.894246,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:197	batch id:0	 lr:0.039923 loss:1.426797
[Train] epoch:197	batch id:10	 lr:0.039923 loss:1.534547
[Train] epoch:197	batch id:20	 lr:0.039923 loss:1.378028
[Train] epoch:197	batch id:30	 lr:0.039923 loss:1.441907
[Train] epoch:197	batch id:40	 lr:0.039923 loss:1.434833
[Train] epoch:197	batch id:50	 lr:0.039923 loss:1.465419
[Train] epoch:197	batch id:60	 lr:0.039923 loss:1.434384
[Train] epoch:197	batch id:70	 lr:0.039923 loss:1.438608
[Train] epoch:197	batch id:80	 lr:0.039923 loss:1.495453
[Train] epoch:197	batch id:90	 lr:0.039923 loss:1.414110
[Train] epoch:197	batch id:100	 lr:0.039923 loss:1.434653
[Train] epoch:197	batch id:110	 lr:0.039923 loss:1.485824
[Train] epoch:197	batch id:120	 lr:0.039923 loss:1.508228
[Train] epoch:197	batch id:130	 lr:0.039923 loss:1.511844
[Train] epoch:197	batch id:140	 lr:0.039923 loss:1.446615
[Train] epoch:197	batch id:150	 lr:0.039923 loss:1.580676
[Train] epoch:197	batch id:160	 lr:0.039923 loss:1.474337
[Train] epoch:197	batch id:170	 lr:0.039923 loss:1.476694
[Train] epoch:197	batch id:180	 lr:0.039923 loss:1.463092
[Train] epoch:197	batch id:190	 lr:0.039923 loss:1.469839
[Train] epoch:197	batch id:200	 lr:0.039923 loss:1.491249
[Train] epoch:197	batch id:210	 lr:0.039923 loss:1.610594
[Train] epoch:197	batch id:220	 lr:0.039923 loss:1.568746
[Train] epoch:197	batch id:230	 lr:0.039923 loss:1.537896
[Train] epoch:197	batch id:240	 lr:0.039923 loss:1.425375
[Train] epoch:197	batch id:250	 lr:0.039923 loss:1.410193
[Train] epoch:197	batch id:260	 lr:0.039923 loss:1.428981
[Train] epoch:197	batch id:270	 lr:0.039923 loss:1.587368
[Train] epoch:197	batch id:280	 lr:0.039923 loss:1.437674
[Train] epoch:197	batch id:290	 lr:0.039923 loss:1.442888
[Train] epoch:197	batch id:300	 lr:0.039923 loss:1.450438
[Train] 197, loss: 1.472651, train acc: 0.947068, 
[Test] epoch:197	batch id:0	 loss:1.385399
[Test] epoch:197	batch id:10	 loss:1.403491
[Test] epoch:197	batch id:20	 loss:1.426007
[Test] epoch:197	batch id:30	 loss:1.522956
[Test] epoch:197	batch id:40	 loss:1.358833
[Test] epoch:197	batch id:50	 loss:1.500187
[Test] epoch:197	batch id:60	 loss:1.324746
[Test] epoch:197	batch id:70	 loss:1.476612
[Test] epoch:197	batch id:80	 loss:1.647061
[Test] epoch:197	batch id:90	 loss:1.516432
[Test] epoch:197	batch id:100	 loss:1.857535
[Test] epoch:197	batch id:110	 loss:1.462757
[Test] epoch:197	batch id:120	 loss:1.391911
[Test] epoch:197	batch id:130	 loss:1.390192
[Test] epoch:197	batch id:140	 loss:1.396620
[Test] epoch:197	batch id:150	 loss:1.783578
[Test] 197, loss: 1.497950, test acc: 0.910049,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:198	batch id:0	 lr:0.039489 loss:1.581239
[Train] epoch:198	batch id:10	 lr:0.039489 loss:1.430141
[Train] epoch:198	batch id:20	 lr:0.039489 loss:1.504845
[Train] epoch:198	batch id:30	 lr:0.039489 loss:1.471703
[Train] epoch:198	batch id:40	 lr:0.039489 loss:1.417460
[Train] epoch:198	batch id:50	 lr:0.039489 loss:1.584397
[Train] epoch:198	batch id:60	 lr:0.039489 loss:1.397785
[Train] epoch:198	batch id:70	 lr:0.039489 loss:1.403162
[Train] epoch:198	batch id:80	 lr:0.039489 loss:1.452812
[Train] epoch:198	batch id:90	 lr:0.039489 loss:1.460507
[Train] epoch:198	batch id:100	 lr:0.039489 loss:1.531126
[Train] epoch:198	batch id:110	 lr:0.039489 loss:1.453514
[Train] epoch:198	batch id:120	 lr:0.039489 loss:1.405295
[Train] epoch:198	batch id:130	 lr:0.039489 loss:1.534965
[Train] epoch:198	batch id:140	 lr:0.039489 loss:1.514759
[Train] epoch:198	batch id:150	 lr:0.039489 loss:1.425559
[Train] epoch:198	batch id:160	 lr:0.039489 loss:1.413571
[Train] epoch:198	batch id:170	 lr:0.039489 loss:1.441683
[Train] epoch:198	batch id:180	 lr:0.039489 loss:1.417984
[Train] epoch:198	batch id:190	 lr:0.039489 loss:1.462554
[Train] epoch:198	batch id:200	 lr:0.039489 loss:1.358658
[Train] epoch:198	batch id:210	 lr:0.039489 loss:1.527174
[Train] epoch:198	batch id:220	 lr:0.039489 loss:1.478831
[Train] epoch:198	batch id:230	 lr:0.039489 loss:1.406877
[Train] epoch:198	batch id:240	 lr:0.039489 loss:1.435956
[Train] epoch:198	batch id:250	 lr:0.039489 loss:1.471894
[Train] epoch:198	batch id:260	 lr:0.039489 loss:1.469713
[Train] epoch:198	batch id:270	 lr:0.039489 loss:1.530527
[Train] epoch:198	batch id:280	 lr:0.039489 loss:1.475392
[Train] epoch:198	batch id:290	 lr:0.039489 loss:1.522959
[Train] epoch:198	batch id:300	 lr:0.039489 loss:1.542998
[Train] 198, loss: 1.464663, train acc: 0.950529, 
[Test] epoch:198	batch id:0	 loss:1.322893
[Test] epoch:198	batch id:10	 loss:1.445285
[Test] epoch:198	batch id:20	 loss:1.369966
[Test] epoch:198	batch id:30	 loss:1.413700
[Test] epoch:198	batch id:40	 loss:1.381398
[Test] epoch:198	batch id:50	 loss:1.532089
[Test] epoch:198	batch id:60	 loss:1.311184
[Test] epoch:198	batch id:70	 loss:1.365679
[Test] epoch:198	batch id:80	 loss:1.712131
[Test] epoch:198	batch id:90	 loss:1.691748
[Test] epoch:198	batch id:100	 loss:1.870196
[Test] epoch:198	batch id:110	 loss:1.429644
[Test] epoch:198	batch id:120	 loss:1.385848
[Test] epoch:198	batch id:130	 loss:1.481148
[Test] epoch:198	batch id:140	 loss:1.392100
[Test] epoch:198	batch id:150	 loss:1.917117
[Test] 198, loss: 1.509201, test acc: 0.903160,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:199	batch id:0	 lr:0.039056 loss:1.379871
[Train] epoch:199	batch id:10	 lr:0.039056 loss:1.450981
[Train] epoch:199	batch id:20	 lr:0.039056 loss:1.480072
[Train] epoch:199	batch id:30	 lr:0.039056 loss:1.415951
[Train] epoch:199	batch id:40	 lr:0.039056 loss:1.387156
[Train] epoch:199	batch id:50	 lr:0.039056 loss:1.410095
[Train] epoch:199	batch id:60	 lr:0.039056 loss:1.413173
[Train] epoch:199	batch id:70	 lr:0.039056 loss:1.390803
[Train] epoch:199	batch id:80	 lr:0.039056 loss:1.536015
[Train] epoch:199	batch id:90	 lr:0.039056 loss:1.415725
[Train] epoch:199	batch id:100	 lr:0.039056 loss:1.372742
[Train] epoch:199	batch id:110	 lr:0.039056 loss:1.451766
[Train] epoch:199	batch id:120	 lr:0.039056 loss:1.375775
[Train] epoch:199	batch id:130	 lr:0.039056 loss:1.505002
[Train] epoch:199	batch id:140	 lr:0.039056 loss:1.476535
[Train] epoch:199	batch id:150	 lr:0.039056 loss:1.525408
[Train] epoch:199	batch id:160	 lr:0.039056 loss:1.493958
[Train] epoch:199	batch id:170	 lr:0.039056 loss:1.545375
[Train] epoch:199	batch id:180	 lr:0.039056 loss:1.500931
[Train] epoch:199	batch id:190	 lr:0.039056 loss:1.509174
[Train] epoch:199	batch id:200	 lr:0.039056 loss:1.482000
[Train] epoch:199	batch id:210	 lr:0.039056 loss:1.485446
[Train] epoch:199	batch id:220	 lr:0.039056 loss:1.456287
[Train] epoch:199	batch id:230	 lr:0.039056 loss:1.468359
[Train] epoch:199	batch id:240	 lr:0.039056 loss:1.443709
[Train] epoch:199	batch id:250	 lr:0.039056 loss:1.482304
[Train] epoch:199	batch id:260	 lr:0.039056 loss:1.424813
[Train] epoch:199	batch id:270	 lr:0.039056 loss:1.447781
[Train] epoch:199	batch id:280	 lr:0.039056 loss:1.466445
[Train] epoch:199	batch id:290	 lr:0.039056 loss:1.393080
[Train] epoch:199	batch id:300	 lr:0.039056 loss:1.445233
[Train] 199, loss: 1.468545, train acc: 0.948595, 
[Test] epoch:199	batch id:0	 loss:1.294659
[Test] epoch:199	batch id:10	 loss:1.416774
[Test] epoch:199	batch id:20	 loss:1.371037
[Test] epoch:199	batch id:30	 loss:1.414283
[Test] epoch:199	batch id:40	 loss:1.334056
[Test] epoch:199	batch id:50	 loss:1.649535
[Test] epoch:199	batch id:60	 loss:1.294580
[Test] epoch:199	batch id:70	 loss:1.460638
[Test] epoch:199	batch id:80	 loss:1.722033
[Test] epoch:199	batch id:90	 loss:1.461576
[Test] epoch:199	batch id:100	 loss:1.980373
[Test] epoch:199	batch id:110	 loss:1.367568
[Test] epoch:199	batch id:120	 loss:1.359405
[Test] epoch:199	batch id:130	 loss:1.389194
[Test] epoch:199	batch id:140	 loss:1.374220
[Test] epoch:199	batch id:150	 loss:1.804375
[Test] 199, loss: 1.497329, test acc: 0.908023,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:200	batch id:0	 lr:0.038625 loss:1.432384
[Train] epoch:200	batch id:10	 lr:0.038625 loss:1.409398
[Train] epoch:200	batch id:20	 lr:0.038625 loss:1.369203
[Train] epoch:200	batch id:30	 lr:0.038625 loss:1.346340
[Train] epoch:200	batch id:40	 lr:0.038625 loss:1.523707
[Train] epoch:200	batch id:50	 lr:0.038625 loss:1.390670
[Train] epoch:200	batch id:60	 lr:0.038625 loss:1.412393
[Train] epoch:200	batch id:70	 lr:0.038625 loss:1.438831
[Train] epoch:200	batch id:80	 lr:0.038625 loss:1.542214
[Train] epoch:200	batch id:90	 lr:0.038625 loss:1.509542
[Train] epoch:200	batch id:100	 lr:0.038625 loss:1.449468
[Train] epoch:200	batch id:110	 lr:0.038625 loss:1.525321
[Train] epoch:200	batch id:120	 lr:0.038625 loss:1.430782
[Train] epoch:200	batch id:130	 lr:0.038625 loss:1.577808
[Train] epoch:200	batch id:140	 lr:0.038625 loss:1.659226
[Train] epoch:200	batch id:150	 lr:0.038625 loss:1.533366
[Train] epoch:200	batch id:160	 lr:0.038625 loss:1.391561
[Train] epoch:200	batch id:170	 lr:0.038625 loss:1.454832
[Train] epoch:200	batch id:180	 lr:0.038625 loss:1.619380
[Train] epoch:200	batch id:190	 lr:0.038625 loss:1.549447
[Train] epoch:200	batch id:200	 lr:0.038625 loss:1.448272
[Train] epoch:200	batch id:210	 lr:0.038625 loss:1.420324
[Train] epoch:200	batch id:220	 lr:0.038625 loss:1.371624
[Train] epoch:200	batch id:230	 lr:0.038625 loss:1.616742
[Train] epoch:200	batch id:240	 lr:0.038625 loss:1.460221
[Train] epoch:200	batch id:250	 lr:0.038625 loss:1.376967
[Train] epoch:200	batch id:260	 lr:0.038625 loss:1.453839
[Train] epoch:200	batch id:270	 lr:0.038625 loss:1.510317
[Train] epoch:200	batch id:280	 lr:0.038625 loss:1.586648
[Train] epoch:200	batch id:290	 lr:0.038625 loss:1.421295
[Train] epoch:200	batch id:300	 lr:0.038625 loss:1.486172
[Train] 200, loss: 1.467556, train acc: 0.946050, 
[Test] epoch:200	batch id:0	 loss:1.298871
[Test] epoch:200	batch id:10	 loss:1.408558
[Test] epoch:200	batch id:20	 loss:1.367256
[Test] epoch:200	batch id:30	 loss:1.457553
[Test] epoch:200	batch id:40	 loss:1.359719
[Test] epoch:200	batch id:50	 loss:1.580790
[Test] epoch:200	batch id:60	 loss:1.309383
[Test] epoch:200	batch id:70	 loss:1.491598
[Test] epoch:200	batch id:80	 loss:1.682812
[Test] epoch:200	batch id:90	 loss:1.521611
[Test] epoch:200	batch id:100	 loss:1.839335
[Test] epoch:200	batch id:110	 loss:1.401336
[Test] epoch:200	batch id:120	 loss:1.403000
[Test] epoch:200	batch id:130	 loss:1.475807
[Test] epoch:200	batch id:140	 loss:1.519248
[Test] epoch:200	batch id:150	 loss:1.851857
[Test] 200, loss: 1.499373, test acc: 0.904376,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:201	batch id:0	 lr:0.038194 loss:1.460698
[Train] epoch:201	batch id:10	 lr:0.038194 loss:1.439205
[Train] epoch:201	batch id:20	 lr:0.038194 loss:1.447666
[Train] epoch:201	batch id:30	 lr:0.038194 loss:1.489754
[Train] epoch:201	batch id:40	 lr:0.038194 loss:1.378781
[Train] epoch:201	batch id:50	 lr:0.038194 loss:1.425419
[Train] epoch:201	batch id:60	 lr:0.038194 loss:1.372168
[Train] epoch:201	batch id:70	 lr:0.038194 loss:1.423667
[Train] epoch:201	batch id:80	 lr:0.038194 loss:1.490507
[Train] epoch:201	batch id:90	 lr:0.038194 loss:1.454436
[Train] epoch:201	batch id:100	 lr:0.038194 loss:1.574062
[Train] epoch:201	batch id:110	 lr:0.038194 loss:1.430821
[Train] epoch:201	batch id:120	 lr:0.038194 loss:1.585621
[Train] epoch:201	batch id:130	 lr:0.038194 loss:1.411162
[Train] epoch:201	batch id:140	 lr:0.038194 loss:1.413228
[Train] epoch:201	batch id:150	 lr:0.038194 loss:1.380911
[Train] epoch:201	batch id:160	 lr:0.038194 loss:1.457466
[Train] epoch:201	batch id:170	 lr:0.038194 loss:1.632481
[Train] epoch:201	batch id:180	 lr:0.038194 loss:1.443943
[Train] epoch:201	batch id:190	 lr:0.038194 loss:1.450879
[Train] epoch:201	batch id:200	 lr:0.038194 loss:1.413303
[Train] epoch:201	batch id:210	 lr:0.038194 loss:1.596256
[Train] epoch:201	batch id:220	 lr:0.038194 loss:1.385440
[Train] epoch:201	batch id:230	 lr:0.038194 loss:1.391921
[Train] epoch:201	batch id:240	 lr:0.038194 loss:1.402255
[Train] epoch:201	batch id:250	 lr:0.038194 loss:1.422886
[Train] epoch:201	batch id:260	 lr:0.038194 loss:1.593675
[Train] epoch:201	batch id:270	 lr:0.038194 loss:1.393989
[Train] epoch:201	batch id:280	 lr:0.038194 loss:1.576057
[Train] epoch:201	batch id:290	 lr:0.038194 loss:1.394903
[Train] epoch:201	batch id:300	 lr:0.038194 loss:1.449175
[Train] 201, loss: 1.463175, train acc: 0.948290, 
[Test] epoch:201	batch id:0	 loss:1.302794
[Test] epoch:201	batch id:10	 loss:1.433478
[Test] epoch:201	batch id:20	 loss:1.324739
[Test] epoch:201	batch id:30	 loss:1.420065
[Test] epoch:201	batch id:40	 loss:1.450087
[Test] epoch:201	batch id:50	 loss:1.470390
[Test] epoch:201	batch id:60	 loss:1.312152
[Test] epoch:201	batch id:70	 loss:1.516333
[Test] epoch:201	batch id:80	 loss:1.590832
[Test] epoch:201	batch id:90	 loss:1.675627
[Test] epoch:201	batch id:100	 loss:1.757556
[Test] epoch:201	batch id:110	 loss:1.440775
[Test] epoch:201	batch id:120	 loss:1.367097
[Test] epoch:201	batch id:130	 loss:1.483620
[Test] epoch:201	batch id:140	 loss:1.340788
[Test] epoch:201	batch id:150	 loss:1.745585
[Test] 201, loss: 1.500822, test acc: 0.905186,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:202	batch id:0	 lr:0.037764 loss:1.411098
[Train] epoch:202	batch id:10	 lr:0.037764 loss:1.537619
[Train] epoch:202	batch id:20	 lr:0.037764 loss:1.398407
[Train] epoch:202	batch id:30	 lr:0.037764 loss:1.405345
[Train] epoch:202	batch id:40	 lr:0.037764 loss:1.450989
[Train] epoch:202	batch id:50	 lr:0.037764 loss:1.403978
[Train] epoch:202	batch id:60	 lr:0.037764 loss:1.433503
[Train] epoch:202	batch id:70	 lr:0.037764 loss:1.409831
[Train] epoch:202	batch id:80	 lr:0.037764 loss:1.472860
[Train] epoch:202	batch id:90	 lr:0.037764 loss:1.335304
[Train] epoch:202	batch id:100	 lr:0.037764 loss:1.425534
[Train] epoch:202	batch id:110	 lr:0.037764 loss:1.387869
[Train] epoch:202	batch id:120	 lr:0.037764 loss:1.404008
[Train] epoch:202	batch id:130	 lr:0.037764 loss:1.428889
[Train] epoch:202	batch id:140	 lr:0.037764 loss:1.603479
[Train] epoch:202	batch id:150	 lr:0.037764 loss:1.417775
[Train] epoch:202	batch id:160	 lr:0.037764 loss:1.378926
[Train] epoch:202	batch id:170	 lr:0.037764 loss:1.407473
[Train] epoch:202	batch id:180	 lr:0.037764 loss:1.524220
[Train] epoch:202	batch id:190	 lr:0.037764 loss:1.484652
[Train] epoch:202	batch id:200	 lr:0.037764 loss:1.486048
[Train] epoch:202	batch id:210	 lr:0.037764 loss:1.517421
[Train] epoch:202	batch id:220	 lr:0.037764 loss:1.598346
[Train] epoch:202	batch id:230	 lr:0.037764 loss:1.404731
[Train] epoch:202	batch id:240	 lr:0.037764 loss:1.510817
[Train] epoch:202	batch id:250	 lr:0.037764 loss:1.388131
[Train] epoch:202	batch id:260	 lr:0.037764 loss:1.466325
[Train] epoch:202	batch id:270	 lr:0.037764 loss:1.445572
[Train] epoch:202	batch id:280	 lr:0.037764 loss:1.570841
[Train] epoch:202	batch id:290	 lr:0.037764 loss:1.449658
[Train] epoch:202	batch id:300	 lr:0.037764 loss:1.369352
[Train] 202, loss: 1.457622, train acc: 0.952871, 
[Test] epoch:202	batch id:0	 loss:1.290777
[Test] epoch:202	batch id:10	 loss:1.475988
[Test] epoch:202	batch id:20	 loss:1.470374
[Test] epoch:202	batch id:30	 loss:1.373601
[Test] epoch:202	batch id:40	 loss:1.380161
[Test] epoch:202	batch id:50	 loss:1.446533
[Test] epoch:202	batch id:60	 loss:1.289552
[Test] epoch:202	batch id:70	 loss:1.386713
[Test] epoch:202	batch id:80	 loss:1.549128
[Test] epoch:202	batch id:90	 loss:1.423926
[Test] epoch:202	batch id:100	 loss:1.825827
[Test] epoch:202	batch id:110	 loss:1.304779
[Test] epoch:202	batch id:120	 loss:1.299252
[Test] epoch:202	batch id:130	 loss:1.406989
[Test] epoch:202	batch id:140	 loss:1.400145
[Test] epoch:202	batch id:150	 loss:1.679546
[Test] 202, loss: 1.469852, test acc: 0.918963,
Max Acc:0.918963
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:203	batch id:0	 lr:0.037335 loss:1.463453
[Train] epoch:203	batch id:10	 lr:0.037335 loss:1.461795
[Train] epoch:203	batch id:20	 lr:0.037335 loss:1.439641
[Train] epoch:203	batch id:30	 lr:0.037335 loss:1.481732
[Train] epoch:203	batch id:40	 lr:0.037335 loss:1.430761
[Train] epoch:203	batch id:50	 lr:0.037335 loss:1.473740
[Train] epoch:203	batch id:60	 lr:0.037335 loss:1.391557
[Train] epoch:203	batch id:70	 lr:0.037335 loss:1.471140
[Train] epoch:203	batch id:80	 lr:0.037335 loss:1.460530
[Train] epoch:203	batch id:90	 lr:0.037335 loss:1.398301
[Train] epoch:203	batch id:100	 lr:0.037335 loss:1.474162
[Train] epoch:203	batch id:110	 lr:0.037335 loss:1.421027
[Train] epoch:203	batch id:120	 lr:0.037335 loss:1.429764
[Train] epoch:203	batch id:130	 lr:0.037335 loss:1.371649
[Train] epoch:203	batch id:140	 lr:0.037335 loss:1.487351
[Train] epoch:203	batch id:150	 lr:0.037335 loss:1.480140
[Train] epoch:203	batch id:160	 lr:0.037335 loss:1.401536
[Train] epoch:203	batch id:170	 lr:0.037335 loss:1.375021
[Train] epoch:203	batch id:180	 lr:0.037335 loss:1.464360
[Train] epoch:203	batch id:190	 lr:0.037335 loss:1.407002
[Train] epoch:203	batch id:200	 lr:0.037335 loss:1.613298
[Train] epoch:203	batch id:210	 lr:0.037335 loss:1.454653
[Train] epoch:203	batch id:220	 lr:0.037335 loss:1.483312
[Train] epoch:203	batch id:230	 lr:0.037335 loss:1.404525
[Train] epoch:203	batch id:240	 lr:0.037335 loss:1.411278
[Train] epoch:203	batch id:250	 lr:0.037335 loss:1.388823
[Train] epoch:203	batch id:260	 lr:0.037335 loss:1.456509
[Train] epoch:203	batch id:270	 lr:0.037335 loss:1.502582
[Train] epoch:203	batch id:280	 lr:0.037335 loss:1.490822
[Train] epoch:203	batch id:290	 lr:0.037335 loss:1.471758
[Train] epoch:203	batch id:300	 lr:0.037335 loss:1.440871
[Train] 203, loss: 1.463202, train acc: 0.947985, 
[Test] epoch:203	batch id:0	 loss:1.296256
[Test] epoch:203	batch id:10	 loss:1.508796
[Test] epoch:203	batch id:20	 loss:1.420804
[Test] epoch:203	batch id:30	 loss:1.510664
[Test] epoch:203	batch id:40	 loss:1.362867
[Test] epoch:203	batch id:50	 loss:1.547038
[Test] epoch:203	batch id:60	 loss:1.272284
[Test] epoch:203	batch id:70	 loss:1.423396
[Test] epoch:203	batch id:80	 loss:1.699368
[Test] epoch:203	batch id:90	 loss:1.480381
[Test] epoch:203	batch id:100	 loss:2.015193
[Test] epoch:203	batch id:110	 loss:1.333317
[Test] epoch:203	batch id:120	 loss:1.351832
[Test] epoch:203	batch id:130	 loss:1.379802
[Test] epoch:203	batch id:140	 loss:1.456748
[Test] epoch:203	batch id:150	 loss:1.645734
[Test] 203, loss: 1.486827, test acc: 0.903566,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:204	batch id:0	 lr:0.036907 loss:1.504201
[Train] epoch:204	batch id:10	 lr:0.036907 loss:1.446659
[Train] epoch:204	batch id:20	 lr:0.036907 loss:1.477593
[Train] epoch:204	batch id:30	 lr:0.036907 loss:1.419887
[Train] epoch:204	batch id:40	 lr:0.036907 loss:1.382974
[Train] epoch:204	batch id:50	 lr:0.036907 loss:1.455050
[Train] epoch:204	batch id:60	 lr:0.036907 loss:1.546358
[Train] epoch:204	batch id:70	 lr:0.036907 loss:1.453707
[Train] epoch:204	batch id:80	 lr:0.036907 loss:1.384090
[Train] epoch:204	batch id:90	 lr:0.036907 loss:1.554034
[Train] epoch:204	batch id:100	 lr:0.036907 loss:1.374372
[Train] epoch:204	batch id:110	 lr:0.036907 loss:1.554088
[Train] epoch:204	batch id:120	 lr:0.036907 loss:1.385551
[Train] epoch:204	batch id:130	 lr:0.036907 loss:1.377671
[Train] epoch:204	batch id:140	 lr:0.036907 loss:1.497584
[Train] epoch:204	batch id:150	 lr:0.036907 loss:1.388214
[Train] epoch:204	batch id:160	 lr:0.036907 loss:1.568869
[Train] epoch:204	batch id:170	 lr:0.036907 loss:1.479917
[Train] epoch:204	batch id:180	 lr:0.036907 loss:1.522182
[Train] epoch:204	batch id:190	 lr:0.036907 loss:1.627072
[Train] epoch:204	batch id:200	 lr:0.036907 loss:1.498204
[Train] epoch:204	batch id:210	 lr:0.036907 loss:1.672838
[Train] epoch:204	batch id:220	 lr:0.036907 loss:1.432850
[Train] epoch:204	batch id:230	 lr:0.036907 loss:1.424727
[Train] epoch:204	batch id:240	 lr:0.036907 loss:1.447701
[Train] epoch:204	batch id:250	 lr:0.036907 loss:1.391936
[Train] epoch:204	batch id:260	 lr:0.036907 loss:1.482804
[Train] epoch:204	batch id:270	 lr:0.036907 loss:1.482012
[Train] epoch:204	batch id:280	 lr:0.036907 loss:1.393674
[Train] epoch:204	batch id:290	 lr:0.036907 loss:1.528498
[Train] epoch:204	batch id:300	 lr:0.036907 loss:1.386030
[Train] 204, loss: 1.462580, train acc: 0.951140, 
[Test] epoch:204	batch id:0	 loss:1.312049
[Test] epoch:204	batch id:10	 loss:1.362425
[Test] epoch:204	batch id:20	 loss:1.633140
[Test] epoch:204	batch id:30	 loss:1.426126
[Test] epoch:204	batch id:40	 loss:1.311691
[Test] epoch:204	batch id:50	 loss:1.452170
[Test] epoch:204	batch id:60	 loss:1.307683
[Test] epoch:204	batch id:70	 loss:1.460475
[Test] epoch:204	batch id:80	 loss:1.737179
[Test] epoch:204	batch id:90	 loss:1.542699
[Test] epoch:204	batch id:100	 loss:1.838717
[Test] epoch:204	batch id:110	 loss:1.436943
[Test] epoch:204	batch id:120	 loss:1.337932
[Test] epoch:204	batch id:130	 loss:1.553479
[Test] epoch:204	batch id:140	 loss:1.307997
[Test] epoch:204	batch id:150	 loss:1.801716
[Test] 204, loss: 1.510239, test acc: 0.903971,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:205	batch id:0	 lr:0.036481 loss:1.536943
[Train] epoch:205	batch id:10	 lr:0.036481 loss:1.428334
[Train] epoch:205	batch id:20	 lr:0.036481 loss:1.438556
[Train] epoch:205	batch id:30	 lr:0.036481 loss:1.511316
[Train] epoch:205	batch id:40	 lr:0.036481 loss:1.484339
[Train] epoch:205	batch id:50	 lr:0.036481 loss:1.446307
[Train] epoch:205	batch id:60	 lr:0.036481 loss:1.384827
[Train] epoch:205	batch id:70	 lr:0.036481 loss:1.459189
[Train] epoch:205	batch id:80	 lr:0.036481 loss:1.443249
[Train] epoch:205	batch id:90	 lr:0.036481 loss:1.385592
[Train] epoch:205	batch id:100	 lr:0.036481 loss:1.517006
[Train] epoch:205	batch id:110	 lr:0.036481 loss:1.431236
[Train] epoch:205	batch id:120	 lr:0.036481 loss:1.394057
[Train] epoch:205	batch id:130	 lr:0.036481 loss:1.514996
[Train] epoch:205	batch id:140	 lr:0.036481 loss:1.442498
[Train] epoch:205	batch id:150	 lr:0.036481 loss:1.463527
[Train] epoch:205	batch id:160	 lr:0.036481 loss:1.433000
[Train] epoch:205	batch id:170	 lr:0.036481 loss:1.419835
[Train] epoch:205	batch id:180	 lr:0.036481 loss:1.453204
[Train] epoch:205	batch id:190	 lr:0.036481 loss:1.472867
[Train] epoch:205	batch id:200	 lr:0.036481 loss:1.351672
[Train] epoch:205	batch id:210	 lr:0.036481 loss:1.414178
[Train] epoch:205	batch id:220	 lr:0.036481 loss:1.441435
[Train] epoch:205	batch id:230	 lr:0.036481 loss:1.552377
[Train] epoch:205	batch id:240	 lr:0.036481 loss:1.469177
[Train] epoch:205	batch id:250	 lr:0.036481 loss:1.511237
[Train] epoch:205	batch id:260	 lr:0.036481 loss:1.397773
[Train] epoch:205	batch id:270	 lr:0.036481 loss:1.425023
[Train] epoch:205	batch id:280	 lr:0.036481 loss:1.475320
[Train] epoch:205	batch id:290	 lr:0.036481 loss:1.589055
[Train] epoch:205	batch id:300	 lr:0.036481 loss:1.432523
[Train] 205, loss: 1.458438, train acc: 0.953074, 
[Test] epoch:205	batch id:0	 loss:1.337194
[Test] epoch:205	batch id:10	 loss:1.401926
[Test] epoch:205	batch id:20	 loss:1.474200
[Test] epoch:205	batch id:30	 loss:1.438884
[Test] epoch:205	batch id:40	 loss:1.354852
[Test] epoch:205	batch id:50	 loss:1.528170
[Test] epoch:205	batch id:60	 loss:1.291042
[Test] epoch:205	batch id:70	 loss:1.444241
[Test] epoch:205	batch id:80	 loss:1.681851
[Test] epoch:205	batch id:90	 loss:1.546982
[Test] epoch:205	batch id:100	 loss:1.826704
[Test] epoch:205	batch id:110	 loss:1.338166
[Test] epoch:205	batch id:120	 loss:1.370329
[Test] epoch:205	batch id:130	 loss:1.463983
[Test] epoch:205	batch id:140	 loss:1.421318
[Test] epoch:205	batch id:150	 loss:1.769277
[Test] 205, loss: 1.504471, test acc: 0.902755,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:206	batch id:0	 lr:0.036055 loss:1.490189
[Train] epoch:206	batch id:10	 lr:0.036055 loss:1.398431
[Train] epoch:206	batch id:20	 lr:0.036055 loss:1.437747
[Train] epoch:206	batch id:30	 lr:0.036055 loss:1.386972
[Train] epoch:206	batch id:40	 lr:0.036055 loss:1.466101
[Train] epoch:206	batch id:50	 lr:0.036055 loss:1.364892
[Train] epoch:206	batch id:60	 lr:0.036055 loss:1.413331
[Train] epoch:206	batch id:70	 lr:0.036055 loss:1.355654
[Train] epoch:206	batch id:80	 lr:0.036055 loss:1.450102
[Train] epoch:206	batch id:90	 lr:0.036055 loss:1.397001
[Train] epoch:206	batch id:100	 lr:0.036055 loss:1.459775
[Train] epoch:206	batch id:110	 lr:0.036055 loss:1.457538
[Train] epoch:206	batch id:120	 lr:0.036055 loss:1.553736
[Train] epoch:206	batch id:130	 lr:0.036055 loss:1.461197
[Train] epoch:206	batch id:140	 lr:0.036055 loss:1.541324
[Train] epoch:206	batch id:150	 lr:0.036055 loss:1.380438
[Train] epoch:206	batch id:160	 lr:0.036055 loss:1.507641
[Train] epoch:206	batch id:170	 lr:0.036055 loss:1.394967
[Train] epoch:206	batch id:180	 lr:0.036055 loss:1.393770
[Train] epoch:206	batch id:190	 lr:0.036055 loss:1.455281
[Train] epoch:206	batch id:200	 lr:0.036055 loss:1.451254
[Train] epoch:206	batch id:210	 lr:0.036055 loss:1.489888
[Train] epoch:206	batch id:220	 lr:0.036055 loss:1.482275
[Train] epoch:206	batch id:230	 lr:0.036055 loss:1.438567
[Train] epoch:206	batch id:240	 lr:0.036055 loss:1.407762
[Train] epoch:206	batch id:250	 lr:0.036055 loss:1.423764
[Train] epoch:206	batch id:260	 lr:0.036055 loss:1.459298
[Train] epoch:206	batch id:270	 lr:0.036055 loss:1.444925
[Train] epoch:206	batch id:280	 lr:0.036055 loss:1.473439
[Train] epoch:206	batch id:290	 lr:0.036055 loss:1.443492
[Train] epoch:206	batch id:300	 lr:0.036055 loss:1.386893
[Train] 206, loss: 1.458195, train acc: 0.950835, 
[Test] epoch:206	batch id:0	 loss:1.292269
[Test] epoch:206	batch id:10	 loss:1.470031
[Test] epoch:206	batch id:20	 loss:1.428036
[Test] epoch:206	batch id:30	 loss:1.408111
[Test] epoch:206	batch id:40	 loss:1.296372
[Test] epoch:206	batch id:50	 loss:1.514398
[Test] epoch:206	batch id:60	 loss:1.268414
[Test] epoch:206	batch id:70	 loss:1.425144
[Test] epoch:206	batch id:80	 loss:1.731757
[Test] epoch:206	batch id:90	 loss:1.514273
[Test] epoch:206	batch id:100	 loss:1.916272
[Test] epoch:206	batch id:110	 loss:1.324098
[Test] epoch:206	batch id:120	 loss:1.401727
[Test] epoch:206	batch id:130	 loss:1.357901
[Test] epoch:206	batch id:140	 loss:1.497737
[Test] epoch:206	batch id:150	 loss:1.720518
[Test] 206, loss: 1.483286, test acc: 0.913290,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:207	batch id:0	 lr:0.035631 loss:1.411257
[Train] epoch:207	batch id:10	 lr:0.035631 loss:1.361089
[Train] epoch:207	batch id:20	 lr:0.035631 loss:1.431488
[Train] epoch:207	batch id:30	 lr:0.035631 loss:1.425513
[Train] epoch:207	batch id:40	 lr:0.035631 loss:1.415186
[Train] epoch:207	batch id:50	 lr:0.035631 loss:1.350015
[Train] epoch:207	batch id:60	 lr:0.035631 loss:1.459687
[Train] epoch:207	batch id:70	 lr:0.035631 loss:1.474256
[Train] epoch:207	batch id:80	 lr:0.035631 loss:1.406627
[Train] epoch:207	batch id:90	 lr:0.035631 loss:1.432020
[Train] epoch:207	batch id:100	 lr:0.035631 loss:1.377451
[Train] epoch:207	batch id:110	 lr:0.035631 loss:1.435759
[Train] epoch:207	batch id:120	 lr:0.035631 loss:1.372202
[Train] epoch:207	batch id:130	 lr:0.035631 loss:1.426971
[Train] epoch:207	batch id:140	 lr:0.035631 loss:1.447748
[Train] epoch:207	batch id:150	 lr:0.035631 loss:1.432245
[Train] epoch:207	batch id:160	 lr:0.035631 loss:1.383852
[Train] epoch:207	batch id:170	 lr:0.035631 loss:1.411747
[Train] epoch:207	batch id:180	 lr:0.035631 loss:1.359475
[Train] epoch:207	batch id:190	 lr:0.035631 loss:1.452088
[Train] epoch:207	batch id:200	 lr:0.035631 loss:1.354034
[Train] epoch:207	batch id:210	 lr:0.035631 loss:1.483978
[Train] epoch:207	batch id:220	 lr:0.035631 loss:1.536070
[Train] epoch:207	batch id:230	 lr:0.035631 loss:1.447063
[Train] epoch:207	batch id:240	 lr:0.035631 loss:1.477472
[Train] epoch:207	batch id:250	 lr:0.035631 loss:1.393960
[Train] epoch:207	batch id:260	 lr:0.035631 loss:1.434326
[Train] epoch:207	batch id:270	 lr:0.035631 loss:1.495481
[Train] epoch:207	batch id:280	 lr:0.035631 loss:1.606086
[Train] epoch:207	batch id:290	 lr:0.035631 loss:1.544404
[Train] epoch:207	batch id:300	 lr:0.035631 loss:1.562492
[Train] 207, loss: 1.445631, train acc: 0.957757, 
[Test] epoch:207	batch id:0	 loss:1.321264
[Test] epoch:207	batch id:10	 loss:1.394021
[Test] epoch:207	batch id:20	 loss:1.384602
[Test] epoch:207	batch id:30	 loss:1.455745
[Test] epoch:207	batch id:40	 loss:1.354656
[Test] epoch:207	batch id:50	 loss:1.474533
[Test] epoch:207	batch id:60	 loss:1.278706
[Test] epoch:207	batch id:70	 loss:1.456490
[Test] epoch:207	batch id:80	 loss:1.618845
[Test] epoch:207	batch id:90	 loss:1.449450
[Test] epoch:207	batch id:100	 loss:1.901011
[Test] epoch:207	batch id:110	 loss:1.492294
[Test] epoch:207	batch id:120	 loss:1.304419
[Test] epoch:207	batch id:130	 loss:1.451791
[Test] epoch:207	batch id:140	 loss:1.313189
[Test] epoch:207	batch id:150	 loss:1.665354
[Test] 207, loss: 1.471165, test acc: 0.914506,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:208	batch id:0	 lr:0.035208 loss:1.558837
[Train] epoch:208	batch id:10	 lr:0.035208 loss:1.412081
[Train] epoch:208	batch id:20	 lr:0.035208 loss:1.476701
[Train] epoch:208	batch id:30	 lr:0.035208 loss:1.379881
[Train] epoch:208	batch id:40	 lr:0.035208 loss:1.427133
[Train] epoch:208	batch id:50	 lr:0.035208 loss:1.473884
[Train] epoch:208	batch id:60	 lr:0.035208 loss:1.531377
[Train] epoch:208	batch id:70	 lr:0.035208 loss:1.479305
[Train] epoch:208	batch id:80	 lr:0.035208 loss:1.510113
[Train] epoch:208	batch id:90	 lr:0.035208 loss:1.446810
[Train] epoch:208	batch id:100	 lr:0.035208 loss:1.480746
[Train] epoch:208	batch id:110	 lr:0.035208 loss:1.508573
[Train] epoch:208	batch id:120	 lr:0.035208 loss:1.430220
[Train] epoch:208	batch id:130	 lr:0.035208 loss:1.371437
[Train] epoch:208	batch id:140	 lr:0.035208 loss:1.389270
[Train] epoch:208	batch id:150	 lr:0.035208 loss:1.455150
[Train] epoch:208	batch id:160	 lr:0.035208 loss:1.439387
[Train] epoch:208	batch id:170	 lr:0.035208 loss:1.455227
[Train] epoch:208	batch id:180	 lr:0.035208 loss:1.501626
[Train] epoch:208	batch id:190	 lr:0.035208 loss:1.540340
[Train] epoch:208	batch id:200	 lr:0.035208 loss:1.516042
[Train] epoch:208	batch id:210	 lr:0.035208 loss:1.470600
[Train] epoch:208	batch id:220	 lr:0.035208 loss:1.516038
[Train] epoch:208	batch id:230	 lr:0.035208 loss:1.480944
[Train] epoch:208	batch id:240	 lr:0.035208 loss:1.365096
[Train] epoch:208	batch id:250	 lr:0.035208 loss:1.541891
[Train] epoch:208	batch id:260	 lr:0.035208 loss:1.357272
[Train] epoch:208	batch id:270	 lr:0.035208 loss:1.640413
[Train] epoch:208	batch id:280	 lr:0.035208 loss:1.534778
[Train] epoch:208	batch id:290	 lr:0.035208 loss:1.473449
[Train] epoch:208	batch id:300	 lr:0.035208 loss:1.467115
[Train] 208, loss: 1.452209, train acc: 0.955314, 
[Test] epoch:208	batch id:0	 loss:1.322045
[Test] epoch:208	batch id:10	 loss:1.427357
[Test] epoch:208	batch id:20	 loss:1.373882
[Test] epoch:208	batch id:30	 loss:1.449749
[Test] epoch:208	batch id:40	 loss:1.360080
[Test] epoch:208	batch id:50	 loss:1.405523
[Test] epoch:208	batch id:60	 loss:1.301637
[Test] epoch:208	batch id:70	 loss:1.461742
[Test] epoch:208	batch id:80	 loss:1.717601
[Test] epoch:208	batch id:90	 loss:1.553082
[Test] epoch:208	batch id:100	 loss:1.961459
[Test] epoch:208	batch id:110	 loss:1.433999
[Test] epoch:208	batch id:120	 loss:1.339588
[Test] epoch:208	batch id:130	 loss:1.393214
[Test] epoch:208	batch id:140	 loss:1.401891
[Test] epoch:208	batch id:150	 loss:1.821186
[Test] 208, loss: 1.502643, test acc: 0.909238,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:209	batch id:0	 lr:0.034786 loss:1.387240
[Train] epoch:209	batch id:10	 lr:0.034786 loss:1.424055
[Train] epoch:209	batch id:20	 lr:0.034786 loss:1.413659
[Train] epoch:209	batch id:30	 lr:0.034786 loss:1.433042
[Train] epoch:209	batch id:40	 lr:0.034786 loss:1.388021
[Train] epoch:209	batch id:50	 lr:0.034786 loss:1.474482
[Train] epoch:209	batch id:60	 lr:0.034786 loss:1.536813
[Train] epoch:209	batch id:70	 lr:0.034786 loss:1.452626
[Train] epoch:209	batch id:80	 lr:0.034786 loss:1.456062
[Train] epoch:209	batch id:90	 lr:0.034786 loss:1.352653
[Train] epoch:209	batch id:100	 lr:0.034786 loss:1.457279
[Train] epoch:209	batch id:110	 lr:0.034786 loss:1.439883
[Train] epoch:209	batch id:120	 lr:0.034786 loss:1.436239
[Train] epoch:209	batch id:130	 lr:0.034786 loss:1.489118
[Train] epoch:209	batch id:140	 lr:0.034786 loss:1.579349
[Train] epoch:209	batch id:150	 lr:0.034786 loss:1.632964
[Train] epoch:209	batch id:160	 lr:0.034786 loss:1.505010
[Train] epoch:209	batch id:170	 lr:0.034786 loss:1.461720
[Train] epoch:209	batch id:180	 lr:0.034786 loss:1.483445
[Train] epoch:209	batch id:190	 lr:0.034786 loss:1.484187
[Train] epoch:209	batch id:200	 lr:0.034786 loss:1.549344
[Train] epoch:209	batch id:210	 lr:0.034786 loss:1.405160
[Train] epoch:209	batch id:220	 lr:0.034786 loss:1.417215
[Train] epoch:209	batch id:230	 lr:0.034786 loss:1.385452
[Train] epoch:209	batch id:240	 lr:0.034786 loss:1.455385
[Train] epoch:209	batch id:250	 lr:0.034786 loss:1.404301
[Train] epoch:209	batch id:260	 lr:0.034786 loss:1.433398
[Train] epoch:209	batch id:270	 lr:0.034786 loss:1.394719
[Train] epoch:209	batch id:280	 lr:0.034786 loss:1.390620
[Train] epoch:209	batch id:290	 lr:0.034786 loss:1.482586
[Train] epoch:209	batch id:300	 lr:0.034786 loss:1.493419
[Train] 209, loss: 1.447633, train acc: 0.958164, 
[Test] epoch:209	batch id:0	 loss:1.321305
[Test] epoch:209	batch id:10	 loss:1.485757
[Test] epoch:209	batch id:20	 loss:1.433123
[Test] epoch:209	batch id:30	 loss:1.428383
[Test] epoch:209	batch id:40	 loss:1.499249
[Test] epoch:209	batch id:50	 loss:1.536885
[Test] epoch:209	batch id:60	 loss:1.264265
[Test] epoch:209	batch id:70	 loss:1.322558
[Test] epoch:209	batch id:80	 loss:1.606830
[Test] epoch:209	batch id:90	 loss:1.508517
[Test] epoch:209	batch id:100	 loss:1.811206
[Test] epoch:209	batch id:110	 loss:1.387762
[Test] epoch:209	batch id:120	 loss:1.331660
[Test] epoch:209	batch id:130	 loss:1.426979
[Test] epoch:209	batch id:140	 loss:1.398693
[Test] epoch:209	batch id:150	 loss:1.708863
[Test] 209, loss: 1.500711, test acc: 0.900729,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:210	batch id:0	 lr:0.034365 loss:1.418103
[Train] epoch:210	batch id:10	 lr:0.034365 loss:1.573147
[Train] epoch:210	batch id:20	 lr:0.034365 loss:1.378557
[Train] epoch:210	batch id:30	 lr:0.034365 loss:1.431156
[Train] epoch:210	batch id:40	 lr:0.034365 loss:1.337400
[Train] epoch:210	batch id:50	 lr:0.034365 loss:1.466691
[Train] epoch:210	batch id:60	 lr:0.034365 loss:1.470994
[Train] epoch:210	batch id:70	 lr:0.034365 loss:1.379912
[Train] epoch:210	batch id:80	 lr:0.034365 loss:1.414656
[Train] epoch:210	batch id:90	 lr:0.034365 loss:1.382702
[Train] epoch:210	batch id:100	 lr:0.034365 loss:1.383616
[Train] epoch:210	batch id:110	 lr:0.034365 loss:1.415109
[Train] epoch:210	batch id:120	 lr:0.034365 loss:1.400830
[Train] epoch:210	batch id:130	 lr:0.034365 loss:1.460415
[Train] epoch:210	batch id:140	 lr:0.034365 loss:1.508231
[Train] epoch:210	batch id:150	 lr:0.034365 loss:1.794013
[Train] epoch:210	batch id:160	 lr:0.034365 loss:1.450034
[Train] epoch:210	batch id:170	 lr:0.034365 loss:1.455189
[Train] epoch:210	batch id:180	 lr:0.034365 loss:1.412929
[Train] epoch:210	batch id:190	 lr:0.034365 loss:1.497454
[Train] epoch:210	batch id:200	 lr:0.034365 loss:1.426968
[Train] epoch:210	batch id:210	 lr:0.034365 loss:1.390711
[Train] epoch:210	batch id:220	 lr:0.034365 loss:1.508198
[Train] epoch:210	batch id:230	 lr:0.034365 loss:1.455925
[Train] epoch:210	batch id:240	 lr:0.034365 loss:1.625059
[Train] epoch:210	batch id:250	 lr:0.034365 loss:1.461910
[Train] epoch:210	batch id:260	 lr:0.034365 loss:1.435061
[Train] epoch:210	batch id:270	 lr:0.034365 loss:1.363873
[Train] epoch:210	batch id:280	 lr:0.034365 loss:1.599646
[Train] epoch:210	batch id:290	 lr:0.034365 loss:1.383762
[Train] epoch:210	batch id:300	 lr:0.034365 loss:1.588298
[Train] 210, loss: 1.455404, train acc: 0.952565, 
[Test] epoch:210	batch id:0	 loss:1.305734
[Test] epoch:210	batch id:10	 loss:1.481278
[Test] epoch:210	batch id:20	 loss:1.475343
[Test] epoch:210	batch id:30	 loss:1.393699
[Test] epoch:210	batch id:40	 loss:1.393000
[Test] epoch:210	batch id:50	 loss:1.472505
[Test] epoch:210	batch id:60	 loss:1.282577
[Test] epoch:210	batch id:70	 loss:1.425467
[Test] epoch:210	batch id:80	 loss:1.598975
[Test] epoch:210	batch id:90	 loss:1.457726
[Test] epoch:210	batch id:100	 loss:2.012915
[Test] epoch:210	batch id:110	 loss:1.378621
[Test] epoch:210	batch id:120	 loss:1.392612
[Test] epoch:210	batch id:130	 loss:1.362744
[Test] epoch:210	batch id:140	 loss:1.379382
[Test] epoch:210	batch id:150	 loss:1.727601
[Test] 210, loss: 1.497998, test acc: 0.905186,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:211	batch id:0	 lr:0.033946 loss:1.438540
[Train] epoch:211	batch id:10	 lr:0.033946 loss:1.425452
[Train] epoch:211	batch id:20	 lr:0.033946 loss:1.464527
[Train] epoch:211	batch id:30	 lr:0.033946 loss:1.433396
[Train] epoch:211	batch id:40	 lr:0.033946 loss:1.417222
[Train] epoch:211	batch id:50	 lr:0.033946 loss:1.442995
[Train] epoch:211	batch id:60	 lr:0.033946 loss:1.421104
[Train] epoch:211	batch id:70	 lr:0.033946 loss:1.355010
[Train] epoch:211	batch id:80	 lr:0.033946 loss:1.462076
[Train] epoch:211	batch id:90	 lr:0.033946 loss:1.488843
[Train] epoch:211	batch id:100	 lr:0.033946 loss:1.547696
[Train] epoch:211	batch id:110	 lr:0.033946 loss:1.394881
[Train] epoch:211	batch id:120	 lr:0.033946 loss:1.465335
[Train] epoch:211	batch id:130	 lr:0.033946 loss:1.483435
[Train] epoch:211	batch id:140	 lr:0.033946 loss:1.411537
[Train] epoch:211	batch id:150	 lr:0.033946 loss:1.435836
[Train] epoch:211	batch id:160	 lr:0.033946 loss:1.384416
[Train] epoch:211	batch id:170	 lr:0.033946 loss:1.392135
[Train] epoch:211	batch id:180	 lr:0.033946 loss:1.502417
[Train] epoch:211	batch id:190	 lr:0.033946 loss:1.442610
[Train] epoch:211	batch id:200	 lr:0.033946 loss:1.385294
[Train] epoch:211	batch id:210	 lr:0.033946 loss:1.486812
[Train] epoch:211	batch id:220	 lr:0.033946 loss:1.376320
[Train] epoch:211	batch id:230	 lr:0.033946 loss:1.445833
[Train] epoch:211	batch id:240	 lr:0.033946 loss:1.450981
[Train] epoch:211	batch id:250	 lr:0.033946 loss:1.389899
[Train] epoch:211	batch id:260	 lr:0.033946 loss:1.561655
[Train] epoch:211	batch id:270	 lr:0.033946 loss:1.437096
[Train] epoch:211	batch id:280	 lr:0.033946 loss:1.524261
[Train] epoch:211	batch id:290	 lr:0.033946 loss:1.457395
[Train] epoch:211	batch id:300	 lr:0.033946 loss:1.504416
[Train] 211, loss: 1.446398, train acc: 0.956433, 
[Test] epoch:211	batch id:0	 loss:1.296787
[Test] epoch:211	batch id:10	 loss:1.471903
[Test] epoch:211	batch id:20	 loss:1.373707
[Test] epoch:211	batch id:30	 loss:1.397600
[Test] epoch:211	batch id:40	 loss:1.389686
[Test] epoch:211	batch id:50	 loss:1.496679
[Test] epoch:211	batch id:60	 loss:1.265923
[Test] epoch:211	batch id:70	 loss:1.600836
[Test] epoch:211	batch id:80	 loss:1.631330
[Test] epoch:211	batch id:90	 loss:1.534348
[Test] epoch:211	batch id:100	 loss:1.906314
[Test] epoch:211	batch id:110	 loss:1.381837
[Test] epoch:211	batch id:120	 loss:1.324684
[Test] epoch:211	batch id:130	 loss:1.407803
[Test] epoch:211	batch id:140	 loss:1.370059
[Test] epoch:211	batch id:150	 loss:1.740732
[Test] 211, loss: 1.474635, test acc: 0.907618,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:212	batch id:0	 lr:0.033528 loss:1.413140
[Train] epoch:212	batch id:10	 lr:0.033528 loss:1.341959
[Train] epoch:212	batch id:20	 lr:0.033528 loss:1.450261
[Train] epoch:212	batch id:30	 lr:0.033528 loss:1.469233
[Train] epoch:212	batch id:40	 lr:0.033528 loss:1.569983
[Train] epoch:212	batch id:50	 lr:0.033528 loss:1.430147
[Train] epoch:212	batch id:60	 lr:0.033528 loss:1.498850
[Train] epoch:212	batch id:70	 lr:0.033528 loss:1.368963
[Train] epoch:212	batch id:80	 lr:0.033528 loss:1.353583
[Train] epoch:212	batch id:90	 lr:0.033528 loss:1.462485
[Train] epoch:212	batch id:100	 lr:0.033528 loss:1.455291
[Train] epoch:212	batch id:110	 lr:0.033528 loss:1.375691
[Train] epoch:212	batch id:120	 lr:0.033528 loss:1.412535
[Train] epoch:212	batch id:130	 lr:0.033528 loss:1.393930
[Train] epoch:212	batch id:140	 lr:0.033528 loss:1.494117
[Train] epoch:212	batch id:150	 lr:0.033528 loss:1.382426
[Train] epoch:212	batch id:160	 lr:0.033528 loss:1.487985
[Train] epoch:212	batch id:170	 lr:0.033528 loss:1.349801
[Train] epoch:212	batch id:180	 lr:0.033528 loss:1.432038
[Train] epoch:212	batch id:190	 lr:0.033528 loss:1.393064
[Train] epoch:212	batch id:200	 lr:0.033528 loss:1.417063
[Train] epoch:212	batch id:210	 lr:0.033528 loss:1.476322
[Train] epoch:212	batch id:220	 lr:0.033528 loss:1.447653
[Train] epoch:212	batch id:230	 lr:0.033528 loss:1.442953
[Train] epoch:212	batch id:240	 lr:0.033528 loss:1.425138
[Train] epoch:212	batch id:250	 lr:0.033528 loss:1.339979
[Train] epoch:212	batch id:260	 lr:0.033528 loss:1.442507
[Train] epoch:212	batch id:270	 lr:0.033528 loss:1.423655
[Train] epoch:212	batch id:280	 lr:0.033528 loss:1.422367
[Train] epoch:212	batch id:290	 lr:0.033528 loss:1.420639
[Train] epoch:212	batch id:300	 lr:0.033528 loss:1.322567
[Train] 212, loss: 1.441002, train acc: 0.959080, 
[Test] epoch:212	batch id:0	 loss:1.330886
[Test] epoch:212	batch id:10	 loss:1.486650
[Test] epoch:212	batch id:20	 loss:1.375737
[Test] epoch:212	batch id:30	 loss:1.385946
[Test] epoch:212	batch id:40	 loss:1.578802
[Test] epoch:212	batch id:50	 loss:1.573066
[Test] epoch:212	batch id:60	 loss:1.270753
[Test] epoch:212	batch id:70	 loss:1.388751
[Test] epoch:212	batch id:80	 loss:1.601558
[Test] epoch:212	batch id:90	 loss:1.521299
[Test] epoch:212	batch id:100	 loss:1.999131
[Test] epoch:212	batch id:110	 loss:1.516953
[Test] epoch:212	batch id:120	 loss:1.357901
[Test] epoch:212	batch id:130	 loss:1.359131
[Test] epoch:212	batch id:140	 loss:1.383603
[Test] epoch:212	batch id:150	 loss:1.811610
[Test] 212, loss: 1.511490, test acc: 0.901135,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:213	batch id:0	 lr:0.033111 loss:1.403989
[Train] epoch:213	batch id:10	 lr:0.033111 loss:1.413591
[Train] epoch:213	batch id:20	 lr:0.033111 loss:1.449204
[Train] epoch:213	batch id:30	 lr:0.033111 loss:1.397160
[Train] epoch:213	batch id:40	 lr:0.033111 loss:1.610389
[Train] epoch:213	batch id:50	 lr:0.033111 loss:1.473318
[Train] epoch:213	batch id:60	 lr:0.033111 loss:1.336571
[Train] epoch:213	batch id:70	 lr:0.033111 loss:1.423951
[Train] epoch:213	batch id:80	 lr:0.033111 loss:1.392926
[Train] epoch:213	batch id:90	 lr:0.033111 loss:1.366956
[Train] epoch:213	batch id:100	 lr:0.033111 loss:1.565292
[Train] epoch:213	batch id:110	 lr:0.033111 loss:1.482707
[Train] epoch:213	batch id:120	 lr:0.033111 loss:1.378247
[Train] epoch:213	batch id:130	 lr:0.033111 loss:1.530629
[Train] epoch:213	batch id:140	 lr:0.033111 loss:1.363082
[Train] epoch:213	batch id:150	 lr:0.033111 loss:1.473031
[Train] epoch:213	batch id:160	 lr:0.033111 loss:1.489927
[Train] epoch:213	batch id:170	 lr:0.033111 loss:1.487934
[Train] epoch:213	batch id:180	 lr:0.033111 loss:1.485434
[Train] epoch:213	batch id:190	 lr:0.033111 loss:1.476017
[Train] epoch:213	batch id:200	 lr:0.033111 loss:1.493170
[Train] epoch:213	batch id:210	 lr:0.033111 loss:1.389478
[Train] epoch:213	batch id:220	 lr:0.033111 loss:1.457499
[Train] epoch:213	batch id:230	 lr:0.033111 loss:1.437196
[Train] epoch:213	batch id:240	 lr:0.033111 loss:1.489523
[Train] epoch:213	batch id:250	 lr:0.033111 loss:1.456635
[Train] epoch:213	batch id:260	 lr:0.033111 loss:1.494038
[Train] epoch:213	batch id:270	 lr:0.033111 loss:1.371130
[Train] epoch:213	batch id:280	 lr:0.033111 loss:1.652720
[Train] epoch:213	batch id:290	 lr:0.033111 loss:1.384186
[Train] epoch:213	batch id:300	 lr:0.033111 loss:1.485980
[Train] 213, loss: 1.452640, train acc: 0.954092, 
[Test] epoch:213	batch id:0	 loss:1.323867
[Test] epoch:213	batch id:10	 loss:1.462126
[Test] epoch:213	batch id:20	 loss:1.388280
[Test] epoch:213	batch id:30	 loss:1.391195
[Test] epoch:213	batch id:40	 loss:1.429072
[Test] epoch:213	batch id:50	 loss:1.529941
[Test] epoch:213	batch id:60	 loss:1.272679
[Test] epoch:213	batch id:70	 loss:1.448759
[Test] epoch:213	batch id:80	 loss:1.618912
[Test] epoch:213	batch id:90	 loss:1.601614
[Test] epoch:213	batch id:100	 loss:1.944269
[Test] epoch:213	batch id:110	 loss:1.410220
[Test] epoch:213	batch id:120	 loss:1.419281
[Test] epoch:213	batch id:130	 loss:1.454271
[Test] epoch:213	batch id:140	 loss:1.436474
[Test] epoch:213	batch id:150	 loss:1.540050
[Test] 213, loss: 1.490545, test acc: 0.908833,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:214	batch id:0	 lr:0.032696 loss:1.414622
[Train] epoch:214	batch id:10	 lr:0.032696 loss:1.428098
[Train] epoch:214	batch id:20	 lr:0.032696 loss:1.393010
[Train] epoch:214	batch id:30	 lr:0.032696 loss:1.385152
[Train] epoch:214	batch id:40	 lr:0.032696 loss:1.382228
[Train] epoch:214	batch id:50	 lr:0.032696 loss:1.444221
[Train] epoch:214	batch id:60	 lr:0.032696 loss:1.343515
[Train] epoch:214	batch id:70	 lr:0.032696 loss:1.492871
[Train] epoch:214	batch id:80	 lr:0.032696 loss:1.373137
[Train] epoch:214	batch id:90	 lr:0.032696 loss:1.547636
[Train] epoch:214	batch id:100	 lr:0.032696 loss:1.423526
[Train] epoch:214	batch id:110	 lr:0.032696 loss:1.538647
[Train] epoch:214	batch id:120	 lr:0.032696 loss:1.346306
[Train] epoch:214	batch id:130	 lr:0.032696 loss:1.493418
[Train] epoch:214	batch id:140	 lr:0.032696 loss:1.529757
[Train] epoch:214	batch id:150	 lr:0.032696 loss:1.429446
[Train] epoch:214	batch id:160	 lr:0.032696 loss:1.385729
[Train] epoch:214	batch id:170	 lr:0.032696 loss:1.482452
[Train] epoch:214	batch id:180	 lr:0.032696 loss:1.552822
[Train] epoch:214	batch id:190	 lr:0.032696 loss:1.397681
[Train] epoch:214	batch id:200	 lr:0.032696 loss:1.443625
[Train] epoch:214	batch id:210	 lr:0.032696 loss:1.476471
[Train] epoch:214	batch id:220	 lr:0.032696 loss:1.386150
[Train] epoch:214	batch id:230	 lr:0.032696 loss:1.530790
[Train] epoch:214	batch id:240	 lr:0.032696 loss:1.416173
[Train] epoch:214	batch id:250	 lr:0.032696 loss:1.419164
[Train] epoch:214	batch id:260	 lr:0.032696 loss:1.545416
[Train] epoch:214	batch id:270	 lr:0.032696 loss:1.497919
[Train] epoch:214	batch id:280	 lr:0.032696 loss:1.350372
[Train] epoch:214	batch id:290	 lr:0.032696 loss:1.399195
[Train] epoch:214	batch id:300	 lr:0.032696 loss:1.394511
[Train] 214, loss: 1.442575, train acc: 0.957146, 
[Test] epoch:214	batch id:0	 loss:1.295140
[Test] epoch:214	batch id:10	 loss:1.524553
[Test] epoch:214	batch id:20	 loss:1.494578
[Test] epoch:214	batch id:30	 loss:1.413578
[Test] epoch:214	batch id:40	 loss:1.409084
[Test] epoch:214	batch id:50	 loss:1.677668
[Test] epoch:214	batch id:60	 loss:1.296301
[Test] epoch:214	batch id:70	 loss:1.367923
[Test] epoch:214	batch id:80	 loss:1.677049
[Test] epoch:214	batch id:90	 loss:1.434740
[Test] epoch:214	batch id:100	 loss:1.957722
[Test] epoch:214	batch id:110	 loss:1.430778
[Test] epoch:214	batch id:120	 loss:1.333810
[Test] epoch:214	batch id:130	 loss:1.478191
[Test] epoch:214	batch id:140	 loss:1.410444
[Test] epoch:214	batch id:150	 loss:1.784389
[Test] 214, loss: 1.494092, test acc: 0.904781,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:215	batch id:0	 lr:0.032282 loss:1.463232
[Train] epoch:215	batch id:10	 lr:0.032282 loss:1.441363
[Train] epoch:215	batch id:20	 lr:0.032282 loss:1.379336
[Train] epoch:215	batch id:30	 lr:0.032282 loss:1.490619
[Train] epoch:215	batch id:40	 lr:0.032282 loss:1.391095
[Train] epoch:215	batch id:50	 lr:0.032282 loss:1.392082
[Train] epoch:215	batch id:60	 lr:0.032282 loss:1.495362
[Train] epoch:215	batch id:70	 lr:0.032282 loss:1.574073
[Train] epoch:215	batch id:80	 lr:0.032282 loss:1.474281
[Train] epoch:215	batch id:90	 lr:0.032282 loss:1.477927
[Train] epoch:215	batch id:100	 lr:0.032282 loss:1.385775
[Train] epoch:215	batch id:110	 lr:0.032282 loss:1.405740
[Train] epoch:215	batch id:120	 lr:0.032282 loss:1.481227
[Train] epoch:215	batch id:130	 lr:0.032282 loss:1.370960
[Train] epoch:215	batch id:140	 lr:0.032282 loss:1.441050
[Train] epoch:215	batch id:150	 lr:0.032282 loss:1.421233
[Train] epoch:215	batch id:160	 lr:0.032282 loss:1.361583
[Train] epoch:215	batch id:170	 lr:0.032282 loss:1.441346
[Train] epoch:215	batch id:180	 lr:0.032282 loss:1.463983
[Train] epoch:215	batch id:190	 lr:0.032282 loss:1.351286
[Train] epoch:215	batch id:200	 lr:0.032282 loss:1.468829
[Train] epoch:215	batch id:210	 lr:0.032282 loss:1.373312
[Train] epoch:215	batch id:220	 lr:0.032282 loss:1.421213
[Train] epoch:215	batch id:230	 lr:0.032282 loss:1.473327
[Train] epoch:215	batch id:240	 lr:0.032282 loss:1.384788
[Train] epoch:215	batch id:250	 lr:0.032282 loss:1.390405
[Train] epoch:215	batch id:260	 lr:0.032282 loss:1.491888
[Train] epoch:215	batch id:270	 lr:0.032282 loss:1.567963
[Train] epoch:215	batch id:280	 lr:0.032282 loss:1.404235
[Train] epoch:215	batch id:290	 lr:0.032282 loss:1.454128
[Train] epoch:215	batch id:300	 lr:0.032282 loss:1.469875
[Train] 215, loss: 1.448220, train acc: 0.955619, 
[Test] epoch:215	batch id:0	 loss:1.317601
[Test] epoch:215	batch id:10	 loss:1.402027
[Test] epoch:215	batch id:20	 loss:1.415077
[Test] epoch:215	batch id:30	 loss:1.412293
[Test] epoch:215	batch id:40	 loss:1.475973
[Test] epoch:215	batch id:50	 loss:1.559471
[Test] epoch:215	batch id:60	 loss:1.272297
[Test] epoch:215	batch id:70	 loss:1.424596
[Test] epoch:215	batch id:80	 loss:1.690018
[Test] epoch:215	batch id:90	 loss:1.571420
[Test] epoch:215	batch id:100	 loss:2.000079
[Test] epoch:215	batch id:110	 loss:1.304362
[Test] epoch:215	batch id:120	 loss:1.326566
[Test] epoch:215	batch id:130	 loss:1.380972
[Test] epoch:215	batch id:140	 loss:1.380129
[Test] epoch:215	batch id:150	 loss:1.700521
[Test] 215, loss: 1.484463, test acc: 0.910049,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:216	batch id:0	 lr:0.031869 loss:1.424948
[Train] epoch:216	batch id:10	 lr:0.031869 loss:1.478243
[Train] epoch:216	batch id:20	 lr:0.031869 loss:1.363265
[Train] epoch:216	batch id:30	 lr:0.031869 loss:1.472205
[Train] epoch:216	batch id:40	 lr:0.031869 loss:1.381738
[Train] epoch:216	batch id:50	 lr:0.031869 loss:1.397453
[Train] epoch:216	batch id:60	 lr:0.031869 loss:1.505450
[Train] epoch:216	batch id:70	 lr:0.031869 loss:1.385794
[Train] epoch:216	batch id:80	 lr:0.031869 loss:1.491215
[Train] epoch:216	batch id:90	 lr:0.031869 loss:1.479074
[Train] epoch:216	batch id:100	 lr:0.031869 loss:1.557863
[Train] epoch:216	batch id:110	 lr:0.031869 loss:1.433876
[Train] epoch:216	batch id:120	 lr:0.031869 loss:1.438828
[Train] epoch:216	batch id:130	 lr:0.031869 loss:1.511581
[Train] epoch:216	batch id:140	 lr:0.031869 loss:1.482563
[Train] epoch:216	batch id:150	 lr:0.031869 loss:1.364030
[Train] epoch:216	batch id:160	 lr:0.031869 loss:1.372171
[Train] epoch:216	batch id:170	 lr:0.031869 loss:1.354230
[Train] epoch:216	batch id:180	 lr:0.031869 loss:1.517486
[Train] epoch:216	batch id:190	 lr:0.031869 loss:1.440145
[Train] epoch:216	batch id:200	 lr:0.031869 loss:1.458614
[Train] epoch:216	batch id:210	 lr:0.031869 loss:1.363938
[Train] epoch:216	batch id:220	 lr:0.031869 loss:1.583354
[Train] epoch:216	batch id:230	 lr:0.031869 loss:1.435807
[Train] epoch:216	batch id:240	 lr:0.031869 loss:1.464376
[Train] epoch:216	batch id:250	 lr:0.031869 loss:1.408124
[Train] epoch:216	batch id:260	 lr:0.031869 loss:1.399396
[Train] epoch:216	batch id:270	 lr:0.031869 loss:1.416561
[Train] epoch:216	batch id:280	 lr:0.031869 loss:1.439233
[Train] epoch:216	batch id:290	 lr:0.031869 loss:1.366607
[Train] epoch:216	batch id:300	 lr:0.031869 loss:1.541909
[Train] 216, loss: 1.445646, train acc: 0.957146, 
[Test] epoch:216	batch id:0	 loss:1.304918
[Test] epoch:216	batch id:10	 loss:1.340727
[Test] epoch:216	batch id:20	 loss:1.419359
[Test] epoch:216	batch id:30	 loss:1.373042
[Test] epoch:216	batch id:40	 loss:1.425724
[Test] epoch:216	batch id:50	 loss:1.520083
[Test] epoch:216	batch id:60	 loss:1.281037
[Test] epoch:216	batch id:70	 loss:1.479576
[Test] epoch:216	batch id:80	 loss:1.691130
[Test] epoch:216	batch id:90	 loss:1.619035
[Test] epoch:216	batch id:100	 loss:2.009660
[Test] epoch:216	batch id:110	 loss:1.347026
[Test] epoch:216	batch id:120	 loss:1.308875
[Test] epoch:216	batch id:130	 loss:1.385268
[Test] epoch:216	batch id:140	 loss:1.324817
[Test] epoch:216	batch id:150	 loss:1.799783
[Test] 216, loss: 1.480882, test acc: 0.912885,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:217	batch id:0	 lr:0.031459 loss:1.497358
[Train] epoch:217	batch id:10	 lr:0.031459 loss:1.394351
[Train] epoch:217	batch id:20	 lr:0.031459 loss:1.448729
[Train] epoch:217	batch id:30	 lr:0.031459 loss:1.499965
[Train] epoch:217	batch id:40	 lr:0.031459 loss:1.386260
[Train] epoch:217	batch id:50	 lr:0.031459 loss:1.407391
[Train] epoch:217	batch id:60	 lr:0.031459 loss:1.445083
[Train] epoch:217	batch id:70	 lr:0.031459 loss:1.429929
[Train] epoch:217	batch id:80	 lr:0.031459 loss:1.609703
[Train] epoch:217	batch id:90	 lr:0.031459 loss:1.516181
[Train] epoch:217	batch id:100	 lr:0.031459 loss:1.436095
[Train] epoch:217	batch id:110	 lr:0.031459 loss:1.391580
[Train] epoch:217	batch id:120	 lr:0.031459 loss:1.454071
[Train] epoch:217	batch id:130	 lr:0.031459 loss:1.405382
[Train] epoch:217	batch id:140	 lr:0.031459 loss:1.493195
[Train] epoch:217	batch id:150	 lr:0.031459 loss:1.373125
[Train] epoch:217	batch id:160	 lr:0.031459 loss:1.370305
[Train] epoch:217	batch id:170	 lr:0.031459 loss:1.649393
[Train] epoch:217	batch id:180	 lr:0.031459 loss:1.542017
[Train] epoch:217	batch id:190	 lr:0.031459 loss:1.448161
[Train] epoch:217	batch id:200	 lr:0.031459 loss:1.542374
[Train] epoch:217	batch id:210	 lr:0.031459 loss:1.476627
[Train] epoch:217	batch id:220	 lr:0.031459 loss:1.402184
[Train] epoch:217	batch id:230	 lr:0.031459 loss:1.426908
[Train] epoch:217	batch id:240	 lr:0.031459 loss:1.482053
[Train] epoch:217	batch id:250	 lr:0.031459 loss:1.381635
[Train] epoch:217	batch id:260	 lr:0.031459 loss:1.415615
[Train] epoch:217	batch id:270	 lr:0.031459 loss:1.378931
[Train] epoch:217	batch id:280	 lr:0.031459 loss:1.490117
[Train] epoch:217	batch id:290	 lr:0.031459 loss:1.466239
[Train] epoch:217	batch id:300	 lr:0.031459 loss:1.443717
[Train] 217, loss: 1.445969, train acc: 0.956840, 
[Test] epoch:217	batch id:0	 loss:1.286095
[Test] epoch:217	batch id:10	 loss:1.519459
[Test] epoch:217	batch id:20	 loss:1.329904
[Test] epoch:217	batch id:30	 loss:1.393128
[Test] epoch:217	batch id:40	 loss:1.419427
[Test] epoch:217	batch id:50	 loss:1.420681
[Test] epoch:217	batch id:60	 loss:1.267630
[Test] epoch:217	batch id:70	 loss:1.365254
[Test] epoch:217	batch id:80	 loss:1.552075
[Test] epoch:217	batch id:90	 loss:1.561887
[Test] epoch:217	batch id:100	 loss:1.918110
[Test] epoch:217	batch id:110	 loss:1.387858
[Test] epoch:217	batch id:120	 loss:1.363406
[Test] epoch:217	batch id:130	 loss:1.425346
[Test] epoch:217	batch id:140	 loss:1.359049
[Test] epoch:217	batch id:150	 loss:1.758673
[Test] 217, loss: 1.479437, test acc: 0.913695,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:218	batch id:0	 lr:0.031049 loss:1.369365
[Train] epoch:218	batch id:10	 lr:0.031049 loss:1.455894
[Train] epoch:218	batch id:20	 lr:0.031049 loss:1.451839
[Train] epoch:218	batch id:30	 lr:0.031049 loss:1.589637
[Train] epoch:218	batch id:40	 lr:0.031049 loss:1.443616
[Train] epoch:218	batch id:50	 lr:0.031049 loss:1.353094
[Train] epoch:218	batch id:60	 lr:0.031049 loss:1.416514
[Train] epoch:218	batch id:70	 lr:0.031049 loss:1.454411
[Train] epoch:218	batch id:80	 lr:0.031049 loss:1.405526
[Train] epoch:218	batch id:90	 lr:0.031049 loss:1.485934
[Train] epoch:218	batch id:100	 lr:0.031049 loss:1.430831
[Train] epoch:218	batch id:110	 lr:0.031049 loss:1.408837
[Train] epoch:218	batch id:120	 lr:0.031049 loss:1.623914
[Train] epoch:218	batch id:130	 lr:0.031049 loss:1.636332
[Train] epoch:218	batch id:140	 lr:0.031049 loss:1.396247
[Train] epoch:218	batch id:150	 lr:0.031049 loss:1.519968
[Train] epoch:218	batch id:160	 lr:0.031049 loss:1.552647
[Train] epoch:218	batch id:170	 lr:0.031049 loss:1.383624
[Train] epoch:218	batch id:180	 lr:0.031049 loss:1.502856
[Train] epoch:218	batch id:190	 lr:0.031049 loss:1.376304
[Train] epoch:218	batch id:200	 lr:0.031049 loss:1.391688
[Train] epoch:218	batch id:210	 lr:0.031049 loss:1.385473
[Train] epoch:218	batch id:220	 lr:0.031049 loss:1.364145
[Train] epoch:218	batch id:230	 lr:0.031049 loss:1.484568
[Train] epoch:218	batch id:240	 lr:0.031049 loss:1.427148
[Train] epoch:218	batch id:250	 lr:0.031049 loss:1.421739
[Train] epoch:218	batch id:260	 lr:0.031049 loss:1.381496
[Train] epoch:218	batch id:270	 lr:0.031049 loss:1.443729
[Train] epoch:218	batch id:280	 lr:0.031049 loss:1.418362
[Train] epoch:218	batch id:290	 lr:0.031049 loss:1.418192
[Train] epoch:218	batch id:300	 lr:0.031049 loss:1.425987
[Train] 218, loss: 1.442474, train acc: 0.960912, 
[Test] epoch:218	batch id:0	 loss:1.362040
[Test] epoch:218	batch id:10	 loss:1.356918
[Test] epoch:218	batch id:20	 loss:1.522617
[Test] epoch:218	batch id:30	 loss:1.439063
[Test] epoch:218	batch id:40	 loss:1.476308
[Test] epoch:218	batch id:50	 loss:1.536319
[Test] epoch:218	batch id:60	 loss:1.281049
[Test] epoch:218	batch id:70	 loss:1.459980
[Test] epoch:218	batch id:80	 loss:1.581808
[Test] epoch:218	batch id:90	 loss:1.512974
[Test] epoch:218	batch id:100	 loss:1.820893
[Test] epoch:218	batch id:110	 loss:1.410861
[Test] epoch:218	batch id:120	 loss:1.349500
[Test] epoch:218	batch id:130	 loss:1.561647
[Test] epoch:218	batch id:140	 loss:1.395271
[Test] epoch:218	batch id:150	 loss:1.805556
[Test] 218, loss: 1.504400, test acc: 0.901540,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:219	batch id:0	 lr:0.030641 loss:1.426005
[Train] epoch:219	batch id:10	 lr:0.030641 loss:1.481053
[Train] epoch:219	batch id:20	 lr:0.030641 loss:1.405141
[Train] epoch:219	batch id:30	 lr:0.030641 loss:1.434713
[Train] epoch:219	batch id:40	 lr:0.030641 loss:1.390872
[Train] epoch:219	batch id:50	 lr:0.030641 loss:1.566332
[Train] epoch:219	batch id:60	 lr:0.030641 loss:1.388481
[Train] epoch:219	batch id:70	 lr:0.030641 loss:1.420171
[Train] epoch:219	batch id:80	 lr:0.030641 loss:1.441963
[Train] epoch:219	batch id:90	 lr:0.030641 loss:1.415076
[Train] epoch:219	batch id:100	 lr:0.030641 loss:1.398716
[Train] epoch:219	batch id:110	 lr:0.030641 loss:1.373516
[Train] epoch:219	batch id:120	 lr:0.030641 loss:1.397064
[Train] epoch:219	batch id:130	 lr:0.030641 loss:1.498802
[Train] epoch:219	batch id:140	 lr:0.030641 loss:1.464570
[Train] epoch:219	batch id:150	 lr:0.030641 loss:1.466624
[Train] epoch:219	batch id:160	 lr:0.030641 loss:1.461180
[Train] epoch:219	batch id:170	 lr:0.030641 loss:1.467803
[Train] epoch:219	batch id:180	 lr:0.030641 loss:1.469497
[Train] epoch:219	batch id:190	 lr:0.030641 loss:1.527870
[Train] epoch:219	batch id:200	 lr:0.030641 loss:1.462493
[Train] epoch:219	batch id:210	 lr:0.030641 loss:1.479856
[Train] epoch:219	batch id:220	 lr:0.030641 loss:1.433156
[Train] epoch:219	batch id:230	 lr:0.030641 loss:1.385290
[Train] epoch:219	batch id:240	 lr:0.030641 loss:1.460249
[Train] epoch:219	batch id:250	 lr:0.030641 loss:1.566270
[Train] epoch:219	batch id:260	 lr:0.030641 loss:1.497564
[Train] epoch:219	batch id:270	 lr:0.030641 loss:1.472546
[Train] epoch:219	batch id:280	 lr:0.030641 loss:1.395396
[Train] epoch:219	batch id:290	 lr:0.030641 loss:1.442091
[Train] epoch:219	batch id:300	 lr:0.030641 loss:1.477162
[Train] 219, loss: 1.438328, train acc: 0.959487, 
[Test] epoch:219	batch id:0	 loss:1.294601
[Test] epoch:219	batch id:10	 loss:1.376122
[Test] epoch:219	batch id:20	 loss:1.563951
[Test] epoch:219	batch id:30	 loss:1.408091
[Test] epoch:219	batch id:40	 loss:1.409661
[Test] epoch:219	batch id:50	 loss:1.632954
[Test] epoch:219	batch id:60	 loss:1.273053
[Test] epoch:219	batch id:70	 loss:1.435871
[Test] epoch:219	batch id:80	 loss:1.651690
[Test] epoch:219	batch id:90	 loss:1.445470
[Test] epoch:219	batch id:100	 loss:2.009858
[Test] epoch:219	batch id:110	 loss:1.336701
[Test] epoch:219	batch id:120	 loss:1.338444
[Test] epoch:219	batch id:130	 loss:1.309722
[Test] epoch:219	batch id:140	 loss:1.419844
[Test] epoch:219	batch id:150	 loss:1.804107
[Test] 219, loss: 1.498302, test acc: 0.901135,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:220	batch id:0	 lr:0.030235 loss:1.411199
[Train] epoch:220	batch id:10	 lr:0.030235 loss:1.369498
[Train] epoch:220	batch id:20	 lr:0.030235 loss:1.442686
[Train] epoch:220	batch id:30	 lr:0.030235 loss:1.376184
[Train] epoch:220	batch id:40	 lr:0.030235 loss:1.430685
[Train] epoch:220	batch id:50	 lr:0.030235 loss:1.355308
[Train] epoch:220	batch id:60	 lr:0.030235 loss:1.450972
[Train] epoch:220	batch id:70	 lr:0.030235 loss:1.425461
[Train] epoch:220	batch id:80	 lr:0.030235 loss:1.453616
[Train] epoch:220	batch id:90	 lr:0.030235 loss:1.398183
[Train] epoch:220	batch id:100	 lr:0.030235 loss:1.433235
[Train] epoch:220	batch id:110	 lr:0.030235 loss:1.440161
[Train] epoch:220	batch id:120	 lr:0.030235 loss:1.459713
[Train] epoch:220	batch id:130	 lr:0.030235 loss:1.488005
[Train] epoch:220	batch id:140	 lr:0.030235 loss:1.383594
[Train] epoch:220	batch id:150	 lr:0.030235 loss:1.494492
[Train] epoch:220	batch id:160	 lr:0.030235 loss:1.485740
[Train] epoch:220	batch id:170	 lr:0.030235 loss:1.441981
[Train] epoch:220	batch id:180	 lr:0.030235 loss:1.408987
[Train] epoch:220	batch id:190	 lr:0.030235 loss:1.404497
[Train] epoch:220	batch id:200	 lr:0.030235 loss:1.381507
[Train] epoch:220	batch id:210	 lr:0.030235 loss:1.429630
[Train] epoch:220	batch id:220	 lr:0.030235 loss:1.473510
[Train] epoch:220	batch id:230	 lr:0.030235 loss:1.594228
[Train] epoch:220	batch id:240	 lr:0.030235 loss:1.431603
[Train] epoch:220	batch id:250	 lr:0.030235 loss:1.391166
[Train] epoch:220	batch id:260	 lr:0.030235 loss:1.497254
[Train] epoch:220	batch id:270	 lr:0.030235 loss:1.374842
[Train] epoch:220	batch id:280	 lr:0.030235 loss:1.349360
[Train] epoch:220	batch id:290	 lr:0.030235 loss:1.399573
[Train] epoch:220	batch id:300	 lr:0.030235 loss:1.480876
[Train] 220, loss: 1.438706, train acc: 0.960200, 
[Test] epoch:220	batch id:0	 loss:1.293579
[Test] epoch:220	batch id:10	 loss:1.359143
[Test] epoch:220	batch id:20	 loss:1.390086
[Test] epoch:220	batch id:30	 loss:1.432786
[Test] epoch:220	batch id:40	 loss:1.541768
[Test] epoch:220	batch id:50	 loss:1.565040
[Test] epoch:220	batch id:60	 loss:1.314170
[Test] epoch:220	batch id:70	 loss:1.396160
[Test] epoch:220	batch id:80	 loss:1.546075
[Test] epoch:220	batch id:90	 loss:1.461825
[Test] epoch:220	batch id:100	 loss:2.041475
[Test] epoch:220	batch id:110	 loss:1.408283
[Test] epoch:220	batch id:120	 loss:1.391573
[Test] epoch:220	batch id:130	 loss:1.346720
[Test] epoch:220	batch id:140	 loss:1.344657
[Test] epoch:220	batch id:150	 loss:1.704426
[Test] 220, loss: 1.480178, test acc: 0.907212,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:221	batch id:0	 lr:0.029831 loss:1.368715
[Train] epoch:221	batch id:10	 lr:0.029831 loss:1.493851
[Train] epoch:221	batch id:20	 lr:0.029831 loss:1.500419
[Train] epoch:221	batch id:30	 lr:0.029831 loss:1.476710
[Train] epoch:221	batch id:40	 lr:0.029831 loss:1.457887
[Train] epoch:221	batch id:50	 lr:0.029831 loss:1.448653
[Train] epoch:221	batch id:60	 lr:0.029831 loss:1.391156
[Train] epoch:221	batch id:70	 lr:0.029831 loss:1.407955
[Train] epoch:221	batch id:80	 lr:0.029831 loss:1.400524
[Train] epoch:221	batch id:90	 lr:0.029831 loss:1.428624
[Train] epoch:221	batch id:100	 lr:0.029831 loss:1.401825
[Train] epoch:221	batch id:110	 lr:0.029831 loss:1.445148
[Train] epoch:221	batch id:120	 lr:0.029831 loss:1.598652
[Train] epoch:221	batch id:130	 lr:0.029831 loss:1.419537
[Train] epoch:221	batch id:140	 lr:0.029831 loss:1.403610
[Train] epoch:221	batch id:150	 lr:0.029831 loss:1.369345
[Train] epoch:221	batch id:160	 lr:0.029831 loss:1.434781
[Train] epoch:221	batch id:170	 lr:0.029831 loss:1.437075
[Train] epoch:221	batch id:180	 lr:0.029831 loss:1.433960
[Train] epoch:221	batch id:190	 lr:0.029831 loss:1.470189
[Train] epoch:221	batch id:200	 lr:0.029831 loss:1.382457
[Train] epoch:221	batch id:210	 lr:0.029831 loss:1.371414
[Train] epoch:221	batch id:220	 lr:0.029831 loss:1.345721
[Train] epoch:221	batch id:230	 lr:0.029831 loss:1.382509
[Train] epoch:221	batch id:240	 lr:0.029831 loss:1.387216
[Train] epoch:221	batch id:250	 lr:0.029831 loss:1.329199
[Train] epoch:221	batch id:260	 lr:0.029831 loss:1.463464
[Train] epoch:221	batch id:270	 lr:0.029831 loss:1.419266
[Train] epoch:221	batch id:280	 lr:0.029831 loss:1.497852
[Train] epoch:221	batch id:290	 lr:0.029831 loss:1.524770
[Train] epoch:221	batch id:300	 lr:0.029831 loss:1.494793
[Train] 221, loss: 1.430381, train acc: 0.963050, 
[Test] epoch:221	batch id:0	 loss:1.266478
[Test] epoch:221	batch id:10	 loss:1.534096
[Test] epoch:221	batch id:20	 loss:1.384259
[Test] epoch:221	batch id:30	 loss:1.392293
[Test] epoch:221	batch id:40	 loss:1.529813
[Test] epoch:221	batch id:50	 loss:1.667941
[Test] epoch:221	batch id:60	 loss:1.263795
[Test] epoch:221	batch id:70	 loss:1.316651
[Test] epoch:221	batch id:80	 loss:1.569251
[Test] epoch:221	batch id:90	 loss:1.362759
[Test] epoch:221	batch id:100	 loss:2.012102
[Test] epoch:221	batch id:110	 loss:1.444669
[Test] epoch:221	batch id:120	 loss:1.421724
[Test] epoch:221	batch id:130	 loss:1.298288
[Test] epoch:221	batch id:140	 loss:1.314271
[Test] epoch:221	batch id:150	 loss:1.773617
[Test] 221, loss: 1.477014, test acc: 0.904376,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:222	batch id:0	 lr:0.029428 loss:1.359933
[Train] epoch:222	batch id:10	 lr:0.029428 loss:1.526280
[Train] epoch:222	batch id:20	 lr:0.029428 loss:1.559909
[Train] epoch:222	batch id:30	 lr:0.029428 loss:1.458452
[Train] epoch:222	batch id:40	 lr:0.029428 loss:1.450615
[Train] epoch:222	batch id:50	 lr:0.029428 loss:1.449750
[Train] epoch:222	batch id:60	 lr:0.029428 loss:1.414330
[Train] epoch:222	batch id:70	 lr:0.029428 loss:1.429711
[Train] epoch:222	batch id:80	 lr:0.029428 loss:1.362460
[Train] epoch:222	batch id:90	 lr:0.029428 loss:1.496731
[Train] epoch:222	batch id:100	 lr:0.029428 loss:1.448215
[Train] epoch:222	batch id:110	 lr:0.029428 loss:1.431136
[Train] epoch:222	batch id:120	 lr:0.029428 loss:1.441648
[Train] epoch:222	batch id:130	 lr:0.029428 loss:1.400782
[Train] epoch:222	batch id:140	 lr:0.029428 loss:1.417293
[Train] epoch:222	batch id:150	 lr:0.029428 loss:1.477218
[Train] epoch:222	batch id:160	 lr:0.029428 loss:1.379380
[Train] epoch:222	batch id:170	 lr:0.029428 loss:1.414694
[Train] epoch:222	batch id:180	 lr:0.029428 loss:1.390124
[Train] epoch:222	batch id:190	 lr:0.029428 loss:1.367378
[Train] epoch:222	batch id:200	 lr:0.029428 loss:1.329643
[Train] epoch:222	batch id:210	 lr:0.029428 loss:1.414362
[Train] epoch:222	batch id:220	 lr:0.029428 loss:1.390216
[Train] epoch:222	batch id:230	 lr:0.029428 loss:1.451899
[Train] epoch:222	batch id:240	 lr:0.029428 loss:1.529966
[Train] epoch:222	batch id:250	 lr:0.029428 loss:1.384164
[Train] epoch:222	batch id:260	 lr:0.029428 loss:1.451549
[Train] epoch:222	batch id:270	 lr:0.029428 loss:1.476738
[Train] epoch:222	batch id:280	 lr:0.029428 loss:1.370573
[Train] epoch:222	batch id:290	 lr:0.029428 loss:1.409706
[Train] epoch:222	batch id:300	 lr:0.029428 loss:1.381771
[Train] 222, loss: 1.429429, train acc: 0.963050, 
[Test] epoch:222	batch id:0	 loss:1.293447
[Test] epoch:222	batch id:10	 loss:1.430029
[Test] epoch:222	batch id:20	 loss:1.354386
[Test] epoch:222	batch id:30	 loss:1.354258
[Test] epoch:222	batch id:40	 loss:1.466544
[Test] epoch:222	batch id:50	 loss:1.536321
[Test] epoch:222	batch id:60	 loss:1.259349
[Test] epoch:222	batch id:70	 loss:1.383449
[Test] epoch:222	batch id:80	 loss:1.639446
[Test] epoch:222	batch id:90	 loss:1.455857
[Test] epoch:222	batch id:100	 loss:1.838703
[Test] epoch:222	batch id:110	 loss:1.341851
[Test] epoch:222	batch id:120	 loss:1.351426
[Test] epoch:222	batch id:130	 loss:1.421234
[Test] epoch:222	batch id:140	 loss:1.355843
[Test] epoch:222	batch id:150	 loss:1.776569
[Test] 222, loss: 1.470904, test acc: 0.905186,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:223	batch id:0	 lr:0.029027 loss:1.344312
[Train] epoch:223	batch id:10	 lr:0.029027 loss:1.347803
[Train] epoch:223	batch id:20	 lr:0.029027 loss:1.503851
[Train] epoch:223	batch id:30	 lr:0.029027 loss:1.483171
[Train] epoch:223	batch id:40	 lr:0.029027 loss:1.364838
[Train] epoch:223	batch id:50	 lr:0.029027 loss:1.404348
[Train] epoch:223	batch id:60	 lr:0.029027 loss:1.513630
[Train] epoch:223	batch id:70	 lr:0.029027 loss:1.396701
[Train] epoch:223	batch id:80	 lr:0.029027 loss:1.410995
[Train] epoch:223	batch id:90	 lr:0.029027 loss:1.368742
[Train] epoch:223	batch id:100	 lr:0.029027 loss:1.443873
[Train] epoch:223	batch id:110	 lr:0.029027 loss:1.397673
[Train] epoch:223	batch id:120	 lr:0.029027 loss:1.382375
[Train] epoch:223	batch id:130	 lr:0.029027 loss:1.430589
[Train] epoch:223	batch id:140	 lr:0.029027 loss:1.367908
[Train] epoch:223	batch id:150	 lr:0.029027 loss:1.348053
[Train] epoch:223	batch id:160	 lr:0.029027 loss:1.468489
[Train] epoch:223	batch id:170	 lr:0.029027 loss:1.450738
[Train] epoch:223	batch id:180	 lr:0.029027 loss:1.364902
[Train] epoch:223	batch id:190	 lr:0.029027 loss:1.402217
[Train] epoch:223	batch id:200	 lr:0.029027 loss:1.511767
[Train] epoch:223	batch id:210	 lr:0.029027 loss:1.406947
[Train] epoch:223	batch id:220	 lr:0.029027 loss:1.343099
[Train] epoch:223	batch id:230	 lr:0.029027 loss:1.445315
[Train] epoch:223	batch id:240	 lr:0.029027 loss:1.365519
[Train] epoch:223	batch id:250	 lr:0.029027 loss:1.374595
[Train] epoch:223	batch id:260	 lr:0.029027 loss:1.533356
[Train] epoch:223	batch id:270	 lr:0.029027 loss:1.504020
[Train] epoch:223	batch id:280	 lr:0.029027 loss:1.419185
[Train] epoch:223	batch id:290	 lr:0.029027 loss:1.445360
[Train] epoch:223	batch id:300	 lr:0.029027 loss:1.413120
[Train] 223, loss: 1.430049, train acc: 0.963762, 
[Test] epoch:223	batch id:0	 loss:1.278440
[Test] epoch:223	batch id:10	 loss:1.469693
[Test] epoch:223	batch id:20	 loss:1.443431
[Test] epoch:223	batch id:30	 loss:1.459277
[Test] epoch:223	batch id:40	 loss:1.440650
[Test] epoch:223	batch id:50	 loss:1.585583
[Test] epoch:223	batch id:60	 loss:1.254969
[Test] epoch:223	batch id:70	 loss:1.457987
[Test] epoch:223	batch id:80	 loss:1.698502
[Test] epoch:223	batch id:90	 loss:1.509829
[Test] epoch:223	batch id:100	 loss:1.991962
[Test] epoch:223	batch id:110	 loss:1.406000
[Test] epoch:223	batch id:120	 loss:1.397683
[Test] epoch:223	batch id:130	 loss:1.354880
[Test] epoch:223	batch id:140	 loss:1.480618
[Test] epoch:223	batch id:150	 loss:1.728562
[Test] 223, loss: 1.479677, test acc: 0.908023,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:224	batch id:0	 lr:0.028627 loss:1.447291
[Train] epoch:224	batch id:10	 lr:0.028627 loss:1.393711
[Train] epoch:224	batch id:20	 lr:0.028627 loss:1.382925
[Train] epoch:224	batch id:30	 lr:0.028627 loss:1.477305
[Train] epoch:224	batch id:40	 lr:0.028627 loss:1.374274
[Train] epoch:224	batch id:50	 lr:0.028627 loss:1.400041
[Train] epoch:224	batch id:60	 lr:0.028627 loss:1.398028
[Train] epoch:224	batch id:70	 lr:0.028627 loss:1.397034
[Train] epoch:224	batch id:80	 lr:0.028627 loss:1.487885
[Train] epoch:224	batch id:90	 lr:0.028627 loss:1.351640
[Train] epoch:224	batch id:100	 lr:0.028627 loss:1.521681
[Train] epoch:224	batch id:110	 lr:0.028627 loss:1.544035
[Train] epoch:224	batch id:120	 lr:0.028627 loss:1.567175
[Train] epoch:224	batch id:130	 lr:0.028627 loss:1.365214
[Train] epoch:224	batch id:140	 lr:0.028627 loss:1.415845
[Train] epoch:224	batch id:150	 lr:0.028627 loss:1.396420
[Train] epoch:224	batch id:160	 lr:0.028627 loss:1.480921
[Train] epoch:224	batch id:170	 lr:0.028627 loss:1.451806
[Train] epoch:224	batch id:180	 lr:0.028627 loss:1.472125
[Train] epoch:224	batch id:190	 lr:0.028627 loss:1.388914
[Train] epoch:224	batch id:200	 lr:0.028627 loss:1.373348
[Train] epoch:224	batch id:210	 lr:0.028627 loss:1.354779
[Train] epoch:224	batch id:220	 lr:0.028627 loss:1.406658
[Train] epoch:224	batch id:230	 lr:0.028627 loss:1.450075
[Train] epoch:224	batch id:240	 lr:0.028627 loss:1.444006
[Train] epoch:224	batch id:250	 lr:0.028627 loss:1.446557
[Train] epoch:224	batch id:260	 lr:0.028627 loss:1.372563
[Train] epoch:224	batch id:270	 lr:0.028627 loss:1.358223
[Train] epoch:224	batch id:280	 lr:0.028627 loss:1.413229
[Train] epoch:224	batch id:290	 lr:0.028627 loss:1.358765
[Train] epoch:224	batch id:300	 lr:0.028627 loss:1.419065
[Train] 224, loss: 1.427828, train acc: 0.964169, 
[Test] epoch:224	batch id:0	 loss:1.292737
[Test] epoch:224	batch id:10	 loss:1.438842
[Test] epoch:224	batch id:20	 loss:1.420892
[Test] epoch:224	batch id:30	 loss:1.422457
[Test] epoch:224	batch id:40	 loss:1.622194
[Test] epoch:224	batch id:50	 loss:1.585787
[Test] epoch:224	batch id:60	 loss:1.270788
[Test] epoch:224	batch id:70	 loss:1.513218
[Test] epoch:224	batch id:80	 loss:1.619490
[Test] epoch:224	batch id:90	 loss:1.510513
[Test] epoch:224	batch id:100	 loss:2.017709
[Test] epoch:224	batch id:110	 loss:1.410595
[Test] epoch:224	batch id:120	 loss:1.451615
[Test] epoch:224	batch id:130	 loss:1.344557
[Test] epoch:224	batch id:140	 loss:1.474454
[Test] epoch:224	batch id:150	 loss:1.934947
[Test] 224, loss: 1.502646, test acc: 0.904781,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:225	batch id:0	 lr:0.028230 loss:1.482745
[Train] epoch:225	batch id:10	 lr:0.028230 loss:1.386605
[Train] epoch:225	batch id:20	 lr:0.028230 loss:1.472504
[Train] epoch:225	batch id:30	 lr:0.028230 loss:1.405282
[Train] epoch:225	batch id:40	 lr:0.028230 loss:1.397873
[Train] epoch:225	batch id:50	 lr:0.028230 loss:1.476834
[Train] epoch:225	batch id:60	 lr:0.028230 loss:1.505594
[Train] epoch:225	batch id:70	 lr:0.028230 loss:1.505543
[Train] epoch:225	batch id:80	 lr:0.028230 loss:1.593479
[Train] epoch:225	batch id:90	 lr:0.028230 loss:1.357654
[Train] epoch:225	batch id:100	 lr:0.028230 loss:1.408653
[Train] epoch:225	batch id:110	 lr:0.028230 loss:1.414634
[Train] epoch:225	batch id:120	 lr:0.028230 loss:1.373970
[Train] epoch:225	batch id:130	 lr:0.028230 loss:1.409914
[Train] epoch:225	batch id:140	 lr:0.028230 loss:1.513533
[Train] epoch:225	batch id:150	 lr:0.028230 loss:1.374852
[Train] epoch:225	batch id:160	 lr:0.028230 loss:1.374211
[Train] epoch:225	batch id:170	 lr:0.028230 loss:1.401800
[Train] epoch:225	batch id:180	 lr:0.028230 loss:1.432549
[Train] epoch:225	batch id:190	 lr:0.028230 loss:1.387950
[Train] epoch:225	batch id:200	 lr:0.028230 loss:1.329515
[Train] epoch:225	batch id:210	 lr:0.028230 loss:1.463571
[Train] epoch:225	batch id:220	 lr:0.028230 loss:1.403516
[Train] epoch:225	batch id:230	 lr:0.028230 loss:1.434416
[Train] epoch:225	batch id:240	 lr:0.028230 loss:1.440907
[Train] epoch:225	batch id:250	 lr:0.028230 loss:1.493533
[Train] epoch:225	batch id:260	 lr:0.028230 loss:1.437374
[Train] epoch:225	batch id:270	 lr:0.028230 loss:1.463020
[Train] epoch:225	batch id:280	 lr:0.028230 loss:1.421409
[Train] epoch:225	batch id:290	 lr:0.028230 loss:1.417474
[Train] epoch:225	batch id:300	 lr:0.028230 loss:1.414967
[Train] 225, loss: 1.431735, train acc: 0.962846, 
[Test] epoch:225	batch id:0	 loss:1.287377
[Test] epoch:225	batch id:10	 loss:1.492667
[Test] epoch:225	batch id:20	 loss:1.457793
[Test] epoch:225	batch id:30	 loss:1.358475
[Test] epoch:225	batch id:40	 loss:1.580993
[Test] epoch:225	batch id:50	 loss:1.467519
[Test] epoch:225	batch id:60	 loss:1.269996
[Test] epoch:225	batch id:70	 loss:1.499239
[Test] epoch:225	batch id:80	 loss:1.649641
[Test] epoch:225	batch id:90	 loss:1.432536
[Test] epoch:225	batch id:100	 loss:2.099078
[Test] epoch:225	batch id:110	 loss:1.346319
[Test] epoch:225	batch id:120	 loss:1.336941
[Test] epoch:225	batch id:130	 loss:1.396800
[Test] epoch:225	batch id:140	 loss:1.452329
[Test] epoch:225	batch id:150	 loss:1.715282
[Test] 225, loss: 1.488170, test acc: 0.907212,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:226	batch id:0	 lr:0.027834 loss:1.463644
[Train] epoch:226	batch id:10	 lr:0.027834 loss:1.480817
[Train] epoch:226	batch id:20	 lr:0.027834 loss:1.446314
[Train] epoch:226	batch id:30	 lr:0.027834 loss:1.393260
[Train] epoch:226	batch id:40	 lr:0.027834 loss:1.436862
[Train] epoch:226	batch id:50	 lr:0.027834 loss:1.385063
[Train] epoch:226	batch id:60	 lr:0.027834 loss:1.478212
[Train] epoch:226	batch id:70	 lr:0.027834 loss:1.399894
[Train] epoch:226	batch id:80	 lr:0.027834 loss:1.537955
[Train] epoch:226	batch id:90	 lr:0.027834 loss:1.411934
[Train] epoch:226	batch id:100	 lr:0.027834 loss:1.440722
[Train] epoch:226	batch id:110	 lr:0.027834 loss:1.460777
[Train] epoch:226	batch id:120	 lr:0.027834 loss:1.466928
[Train] epoch:226	batch id:130	 lr:0.027834 loss:1.536968
[Train] epoch:226	batch id:140	 lr:0.027834 loss:1.415696
[Train] epoch:226	batch id:150	 lr:0.027834 loss:1.347321
[Train] epoch:226	batch id:160	 lr:0.027834 loss:1.360679
[Train] epoch:226	batch id:170	 lr:0.027834 loss:1.399435
[Train] epoch:226	batch id:180	 lr:0.027834 loss:1.444890
[Train] epoch:226	batch id:190	 lr:0.027834 loss:1.452865
[Train] epoch:226	batch id:200	 lr:0.027834 loss:1.443790
[Train] epoch:226	batch id:210	 lr:0.027834 loss:1.407100
[Train] epoch:226	batch id:220	 lr:0.027834 loss:1.381871
[Train] epoch:226	batch id:230	 lr:0.027834 loss:1.394382
[Train] epoch:226	batch id:240	 lr:0.027834 loss:1.368615
[Train] epoch:226	batch id:250	 lr:0.027834 loss:1.368129
[Train] epoch:226	batch id:260	 lr:0.027834 loss:1.459437
[Train] epoch:226	batch id:270	 lr:0.027834 loss:1.373415
[Train] epoch:226	batch id:280	 lr:0.027834 loss:1.398459
[Train] epoch:226	batch id:290	 lr:0.027834 loss:1.443953
[Train] epoch:226	batch id:300	 lr:0.027834 loss:1.468023
[Train] 226, loss: 1.425641, train acc: 0.966205, 
[Test] epoch:226	batch id:0	 loss:1.263017
[Test] epoch:226	batch id:10	 loss:1.407478
[Test] epoch:226	batch id:20	 loss:1.424832
[Test] epoch:226	batch id:30	 loss:1.344601
[Test] epoch:226	batch id:40	 loss:1.422908
[Test] epoch:226	batch id:50	 loss:1.540063
[Test] epoch:226	batch id:60	 loss:1.256468
[Test] epoch:226	batch id:70	 loss:1.451569
[Test] epoch:226	batch id:80	 loss:1.694386
[Test] epoch:226	batch id:90	 loss:1.459805
[Test] epoch:226	batch id:100	 loss:1.882230
[Test] epoch:226	batch id:110	 loss:1.463706
[Test] epoch:226	batch id:120	 loss:1.424064
[Test] epoch:226	batch id:130	 loss:1.364788
[Test] epoch:226	batch id:140	 loss:1.407781
[Test] epoch:226	batch id:150	 loss:1.682245
[Test] 226, loss: 1.471872, test acc: 0.910049,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:227	batch id:0	 lr:0.027440 loss:1.356716
[Train] epoch:227	batch id:10	 lr:0.027440 loss:1.418896
[Train] epoch:227	batch id:20	 lr:0.027440 loss:1.493662
[Train] epoch:227	batch id:30	 lr:0.027440 loss:1.340552
[Train] epoch:227	batch id:40	 lr:0.027440 loss:1.556673
[Train] epoch:227	batch id:50	 lr:0.027440 loss:1.495081
[Train] epoch:227	batch id:60	 lr:0.027440 loss:1.423098
[Train] epoch:227	batch id:70	 lr:0.027440 loss:1.427887
[Train] epoch:227	batch id:80	 lr:0.027440 loss:1.432577
[Train] epoch:227	batch id:90	 lr:0.027440 loss:1.453794
[Train] epoch:227	batch id:100	 lr:0.027440 loss:1.375267
[Train] epoch:227	batch id:110	 lr:0.027440 loss:1.353736
[Train] epoch:227	batch id:120	 lr:0.027440 loss:1.402313
[Train] epoch:227	batch id:130	 lr:0.027440 loss:1.482691
[Train] epoch:227	batch id:140	 lr:0.027440 loss:1.447738
[Train] epoch:227	batch id:150	 lr:0.027440 loss:1.440353
[Train] epoch:227	batch id:160	 lr:0.027440 loss:1.371171
[Train] epoch:227	batch id:170	 lr:0.027440 loss:1.456064
[Train] epoch:227	batch id:180	 lr:0.027440 loss:1.412637
[Train] epoch:227	batch id:190	 lr:0.027440 loss:1.413805
[Train] epoch:227	batch id:200	 lr:0.027440 loss:1.543608
[Train] epoch:227	batch id:210	 lr:0.027440 loss:1.352839
[Train] epoch:227	batch id:220	 lr:0.027440 loss:1.355365
[Train] epoch:227	batch id:230	 lr:0.027440 loss:1.510782
[Train] epoch:227	batch id:240	 lr:0.027440 loss:1.383418
[Train] epoch:227	batch id:250	 lr:0.027440 loss:1.359438
[Train] epoch:227	batch id:260	 lr:0.027440 loss:1.394979
[Train] epoch:227	batch id:270	 lr:0.027440 loss:1.355964
[Train] epoch:227	batch id:280	 lr:0.027440 loss:1.420971
[Train] epoch:227	batch id:290	 lr:0.027440 loss:1.374689
[Train] epoch:227	batch id:300	 lr:0.027440 loss:1.460154
[Train] 227, loss: 1.426747, train acc: 0.966409, 
[Test] epoch:227	batch id:0	 loss:1.311675
[Test] epoch:227	batch id:10	 loss:1.522524
[Test] epoch:227	batch id:20	 loss:1.378842
[Test] epoch:227	batch id:30	 loss:1.377552
[Test] epoch:227	batch id:40	 loss:1.483749
[Test] epoch:227	batch id:50	 loss:1.431036
[Test] epoch:227	batch id:60	 loss:1.283121
[Test] epoch:227	batch id:70	 loss:1.400067
[Test] epoch:227	batch id:80	 loss:1.650596
[Test] epoch:227	batch id:90	 loss:1.538438
[Test] epoch:227	batch id:100	 loss:1.870437
[Test] epoch:227	batch id:110	 loss:1.308243
[Test] epoch:227	batch id:120	 loss:1.313441
[Test] epoch:227	batch id:130	 loss:1.366986
[Test] epoch:227	batch id:140	 loss:1.344770
[Test] epoch:227	batch id:150	 loss:1.677392
[Test] 227, loss: 1.471065, test acc: 0.917342,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:228	batch id:0	 lr:0.027047 loss:1.437359
[Train] epoch:228	batch id:10	 lr:0.027047 loss:1.453079
[Train] epoch:228	batch id:20	 lr:0.027047 loss:1.487947
[Train] epoch:228	batch id:30	 lr:0.027047 loss:1.356840
[Train] epoch:228	batch id:40	 lr:0.027047 loss:1.469498
[Train] epoch:228	batch id:50	 lr:0.027047 loss:1.408580
[Train] epoch:228	batch id:60	 lr:0.027047 loss:1.398901
[Train] epoch:228	batch id:70	 lr:0.027047 loss:1.343409
[Train] epoch:228	batch id:80	 lr:0.027047 loss:1.428016
[Train] epoch:228	batch id:90	 lr:0.027047 loss:1.399283
[Train] epoch:228	batch id:100	 lr:0.027047 loss:1.409476
[Train] epoch:228	batch id:110	 lr:0.027047 loss:1.365446
[Train] epoch:228	batch id:120	 lr:0.027047 loss:1.502490
[Train] epoch:228	batch id:130	 lr:0.027047 loss:1.355221
[Train] epoch:228	batch id:140	 lr:0.027047 loss:1.508077
[Train] epoch:228	batch id:150	 lr:0.027047 loss:1.410049
[Train] epoch:228	batch id:160	 lr:0.027047 loss:1.385402
[Train] epoch:228	batch id:170	 lr:0.027047 loss:1.351388
[Train] epoch:228	batch id:180	 lr:0.027047 loss:1.410887
[Train] epoch:228	batch id:190	 lr:0.027047 loss:1.585614
[Train] epoch:228	batch id:200	 lr:0.027047 loss:1.453210
[Train] epoch:228	batch id:210	 lr:0.027047 loss:1.456413
[Train] epoch:228	batch id:220	 lr:0.027047 loss:1.374414
[Train] epoch:228	batch id:230	 lr:0.027047 loss:1.485299
[Train] epoch:228	batch id:240	 lr:0.027047 loss:1.417318
[Train] epoch:228	batch id:250	 lr:0.027047 loss:1.414328
[Train] epoch:228	batch id:260	 lr:0.027047 loss:1.365449
[Train] epoch:228	batch id:270	 lr:0.027047 loss:1.493642
[Train] epoch:228	batch id:280	 lr:0.027047 loss:1.365263
[Train] epoch:228	batch id:290	 lr:0.027047 loss:1.640879
[Train] epoch:228	batch id:300	 lr:0.027047 loss:1.366053
[Train] 228, loss: 1.422739, train acc: 0.967325, 
[Test] epoch:228	batch id:0	 loss:1.332817
[Test] epoch:228	batch id:10	 loss:1.497139
[Test] epoch:228	batch id:20	 loss:1.494685
[Test] epoch:228	batch id:30	 loss:1.395509
[Test] epoch:228	batch id:40	 loss:1.482686
[Test] epoch:228	batch id:50	 loss:1.437180
[Test] epoch:228	batch id:60	 loss:1.285030
[Test] epoch:228	batch id:70	 loss:1.429842
[Test] epoch:228	batch id:80	 loss:1.686840
[Test] epoch:228	batch id:90	 loss:1.468273
[Test] epoch:228	batch id:100	 loss:1.910034
[Test] epoch:228	batch id:110	 loss:1.312027
[Test] epoch:228	batch id:120	 loss:1.347418
[Test] epoch:228	batch id:130	 loss:1.359513
[Test] epoch:228	batch id:140	 loss:1.472370
[Test] epoch:228	batch id:150	 loss:1.631814
[Test] 228, loss: 1.481873, test acc: 0.910049,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:229	batch id:0	 lr:0.026657 loss:1.505989
[Train] epoch:229	batch id:10	 lr:0.026657 loss:1.434612
[Train] epoch:229	batch id:20	 lr:0.026657 loss:1.459309
[Train] epoch:229	batch id:30	 lr:0.026657 loss:1.331634
[Train] epoch:229	batch id:40	 lr:0.026657 loss:1.439001
[Train] epoch:229	batch id:50	 lr:0.026657 loss:1.400307
[Train] epoch:229	batch id:60	 lr:0.026657 loss:1.451280
[Train] epoch:229	batch id:70	 lr:0.026657 loss:1.356953
[Train] epoch:229	batch id:80	 lr:0.026657 loss:1.418352
[Train] epoch:229	batch id:90	 lr:0.026657 loss:1.351973
[Train] epoch:229	batch id:100	 lr:0.026657 loss:1.398441
[Train] epoch:229	batch id:110	 lr:0.026657 loss:1.507907
[Train] epoch:229	batch id:120	 lr:0.026657 loss:1.604408
[Train] epoch:229	batch id:130	 lr:0.026657 loss:1.388014
[Train] epoch:229	batch id:140	 lr:0.026657 loss:1.378920
[Train] epoch:229	batch id:150	 lr:0.026657 loss:1.511179
[Train] epoch:229	batch id:160	 lr:0.026657 loss:1.493573
[Train] epoch:229	batch id:170	 lr:0.026657 loss:1.404506
[Train] epoch:229	batch id:180	 lr:0.026657 loss:1.370699
[Train] epoch:229	batch id:190	 lr:0.026657 loss:1.474465
[Train] epoch:229	batch id:200	 lr:0.026657 loss:1.501230
[Train] epoch:229	batch id:210	 lr:0.026657 loss:1.398397
[Train] epoch:229	batch id:220	 lr:0.026657 loss:1.358401
[Train] epoch:229	batch id:230	 lr:0.026657 loss:1.393341
[Train] epoch:229	batch id:240	 lr:0.026657 loss:1.354151
[Train] epoch:229	batch id:250	 lr:0.026657 loss:1.413570
[Train] epoch:229	batch id:260	 lr:0.026657 loss:1.392495
[Train] epoch:229	batch id:270	 lr:0.026657 loss:1.379122
[Train] epoch:229	batch id:280	 lr:0.026657 loss:1.367166
[Train] epoch:229	batch id:290	 lr:0.026657 loss:1.372001
[Train] epoch:229	batch id:300	 lr:0.026657 loss:1.402467
[Train] 229, loss: 1.423159, train acc: 0.965289, 
[Test] epoch:229	batch id:0	 loss:1.290460
[Test] epoch:229	batch id:10	 loss:1.556855
[Test] epoch:229	batch id:20	 loss:1.366482
[Test] epoch:229	batch id:30	 loss:1.399992
[Test] epoch:229	batch id:40	 loss:1.427845
[Test] epoch:229	batch id:50	 loss:1.598983
[Test] epoch:229	batch id:60	 loss:1.267514
[Test] epoch:229	batch id:70	 loss:1.426407
[Test] epoch:229	batch id:80	 loss:1.673714
[Test] epoch:229	batch id:90	 loss:1.605540
[Test] epoch:229	batch id:100	 loss:2.086072
[Test] epoch:229	batch id:110	 loss:1.357862
[Test] epoch:229	batch id:120	 loss:1.349102
[Test] epoch:229	batch id:130	 loss:1.341420
[Test] epoch:229	batch id:140	 loss:1.367476
[Test] epoch:229	batch id:150	 loss:1.790120
[Test] 229, loss: 1.500605, test acc: 0.903160,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:230	batch id:0	 lr:0.026269 loss:1.393248
[Train] epoch:230	batch id:10	 lr:0.026269 loss:1.428149
[Train] epoch:230	batch id:20	 lr:0.026269 loss:1.405470
[Train] epoch:230	batch id:30	 lr:0.026269 loss:1.413973
[Train] epoch:230	batch id:40	 lr:0.026269 loss:1.467653
[Train] epoch:230	batch id:50	 lr:0.026269 loss:1.467045
[Train] epoch:230	batch id:60	 lr:0.026269 loss:1.414779
[Train] epoch:230	batch id:70	 lr:0.026269 loss:1.430216
[Train] epoch:230	batch id:80	 lr:0.026269 loss:1.379549
[Train] epoch:230	batch id:90	 lr:0.026269 loss:1.371173
[Train] epoch:230	batch id:100	 lr:0.026269 loss:1.354876
[Train] epoch:230	batch id:110	 lr:0.026269 loss:1.450662
[Train] epoch:230	batch id:120	 lr:0.026269 loss:1.374743
[Train] epoch:230	batch id:130	 lr:0.026269 loss:1.386853
[Train] epoch:230	batch id:140	 lr:0.026269 loss:1.421797
[Train] epoch:230	batch id:150	 lr:0.026269 loss:1.468084
[Train] epoch:230	batch id:160	 lr:0.026269 loss:1.375327
[Train] epoch:230	batch id:170	 lr:0.026269 loss:1.533006
[Train] epoch:230	batch id:180	 lr:0.026269 loss:1.520384
[Train] epoch:230	batch id:190	 lr:0.026269 loss:1.446320
[Train] epoch:230	batch id:200	 lr:0.026269 loss:1.432697
[Train] epoch:230	batch id:210	 lr:0.026269 loss:1.478041
[Train] epoch:230	batch id:220	 lr:0.026269 loss:1.378108
[Train] epoch:230	batch id:230	 lr:0.026269 loss:1.354935
[Train] epoch:230	batch id:240	 lr:0.026269 loss:1.402034
[Train] epoch:230	batch id:250	 lr:0.026269 loss:1.505111
[Train] epoch:230	batch id:260	 lr:0.026269 loss:1.372988
[Train] epoch:230	batch id:270	 lr:0.026269 loss:1.419360
[Train] epoch:230	batch id:280	 lr:0.026269 loss:1.396065
[Train] epoch:230	batch id:290	 lr:0.026269 loss:1.382452
[Train] epoch:230	batch id:300	 lr:0.026269 loss:1.422409
[Train] 230, loss: 1.418548, train acc: 0.967325, 
[Test] epoch:230	batch id:0	 loss:1.338432
[Test] epoch:230	batch id:10	 loss:1.500068
[Test] epoch:230	batch id:20	 loss:1.481989
[Test] epoch:230	batch id:30	 loss:1.426944
[Test] epoch:230	batch id:40	 loss:1.431981
[Test] epoch:230	batch id:50	 loss:1.400014
[Test] epoch:230	batch id:60	 loss:1.277809
[Test] epoch:230	batch id:70	 loss:1.434406
[Test] epoch:230	batch id:80	 loss:1.774087
[Test] epoch:230	batch id:90	 loss:1.619735
[Test] epoch:230	batch id:100	 loss:1.952982
[Test] epoch:230	batch id:110	 loss:1.353529
[Test] epoch:230	batch id:120	 loss:1.338259
[Test] epoch:230	batch id:130	 loss:1.469351
[Test] epoch:230	batch id:140	 loss:1.394114
[Test] epoch:230	batch id:150	 loss:1.726792
[Test] 230, loss: 1.498286, test acc: 0.910049,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:231	batch id:0	 lr:0.025882 loss:1.460341
[Train] epoch:231	batch id:10	 lr:0.025882 loss:1.412174
[Train] epoch:231	batch id:20	 lr:0.025882 loss:1.498857
[Train] epoch:231	batch id:30	 lr:0.025882 loss:1.372514
[Train] epoch:231	batch id:40	 lr:0.025882 loss:1.461230
[Train] epoch:231	batch id:50	 lr:0.025882 loss:1.366581
[Train] epoch:231	batch id:60	 lr:0.025882 loss:1.407338
[Train] epoch:231	batch id:70	 lr:0.025882 loss:1.437900
[Train] epoch:231	batch id:80	 lr:0.025882 loss:1.436002
[Train] epoch:231	batch id:90	 lr:0.025882 loss:1.541105
[Train] epoch:231	batch id:100	 lr:0.025882 loss:1.377223
[Train] epoch:231	batch id:110	 lr:0.025882 loss:1.359720
[Train] epoch:231	batch id:120	 lr:0.025882 loss:1.390891
[Train] epoch:231	batch id:130	 lr:0.025882 loss:1.390605
[Train] epoch:231	batch id:140	 lr:0.025882 loss:1.420048
[Train] epoch:231	batch id:150	 lr:0.025882 loss:1.485309
[Train] epoch:231	batch id:160	 lr:0.025882 loss:1.387785
[Train] epoch:231	batch id:170	 lr:0.025882 loss:1.363233
[Train] epoch:231	batch id:180	 lr:0.025882 loss:1.448195
[Train] epoch:231	batch id:190	 lr:0.025882 loss:1.405192
[Train] epoch:231	batch id:200	 lr:0.025882 loss:1.378519
[Train] epoch:231	batch id:210	 lr:0.025882 loss:1.430283
[Train] epoch:231	batch id:220	 lr:0.025882 loss:1.376125
[Train] epoch:231	batch id:230	 lr:0.025882 loss:1.516913
[Train] epoch:231	batch id:240	 lr:0.025882 loss:1.435504
[Train] epoch:231	batch id:250	 lr:0.025882 loss:1.445363
[Train] epoch:231	batch id:260	 lr:0.025882 loss:1.407897
[Train] epoch:231	batch id:270	 lr:0.025882 loss:1.414135
[Train] epoch:231	batch id:280	 lr:0.025882 loss:1.444558
[Train] epoch:231	batch id:290	 lr:0.025882 loss:1.426625
[Train] epoch:231	batch id:300	 lr:0.025882 loss:1.394003
[Train] 231, loss: 1.420653, train acc: 0.966612, 
[Test] epoch:231	batch id:0	 loss:1.348709
[Test] epoch:231	batch id:10	 loss:1.501191
[Test] epoch:231	batch id:20	 loss:1.590457
[Test] epoch:231	batch id:30	 loss:1.431800
[Test] epoch:231	batch id:40	 loss:1.461346
[Test] epoch:231	batch id:50	 loss:1.513108
[Test] epoch:231	batch id:60	 loss:1.284030
[Test] epoch:231	batch id:70	 loss:1.411089
[Test] epoch:231	batch id:80	 loss:1.669643
[Test] epoch:231	batch id:90	 loss:1.473804
[Test] epoch:231	batch id:100	 loss:1.854071
[Test] epoch:231	batch id:110	 loss:1.418539
[Test] epoch:231	batch id:120	 loss:1.341383
[Test] epoch:231	batch id:130	 loss:1.506227
[Test] epoch:231	batch id:140	 loss:1.356643
[Test] epoch:231	batch id:150	 loss:1.738282
[Test] 231, loss: 1.494560, test acc: 0.907212,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:232	batch id:0	 lr:0.025498 loss:1.399672
[Train] epoch:232	batch id:10	 lr:0.025498 loss:1.433415
[Train] epoch:232	batch id:20	 lr:0.025498 loss:1.471683
[Train] epoch:232	batch id:30	 lr:0.025498 loss:1.429372
[Train] epoch:232	batch id:40	 lr:0.025498 loss:1.380475
[Train] epoch:232	batch id:50	 lr:0.025498 loss:1.449310
[Train] epoch:232	batch id:60	 lr:0.025498 loss:1.360408
[Train] epoch:232	batch id:70	 lr:0.025498 loss:1.371824
[Train] epoch:232	batch id:80	 lr:0.025498 loss:1.428743
[Train] epoch:232	batch id:90	 lr:0.025498 loss:1.369163
[Train] epoch:232	batch id:100	 lr:0.025498 loss:1.339199
[Train] epoch:232	batch id:110	 lr:0.025498 loss:1.411664
[Train] epoch:232	batch id:120	 lr:0.025498 loss:1.479205
[Train] epoch:232	batch id:130	 lr:0.025498 loss:1.367862
[Train] epoch:232	batch id:140	 lr:0.025498 loss:1.392004
[Train] epoch:232	batch id:150	 lr:0.025498 loss:1.357799
[Train] epoch:232	batch id:160	 lr:0.025498 loss:1.435746
[Train] epoch:232	batch id:170	 lr:0.025498 loss:1.451057
[Train] epoch:232	batch id:180	 lr:0.025498 loss:1.450577
[Train] epoch:232	batch id:190	 lr:0.025498 loss:1.371121
[Train] epoch:232	batch id:200	 lr:0.025498 loss:1.456676
[Train] epoch:232	batch id:210	 lr:0.025498 loss:1.413877
[Train] epoch:232	batch id:220	 lr:0.025498 loss:1.474342
[Train] epoch:232	batch id:230	 lr:0.025498 loss:1.362039
[Train] epoch:232	batch id:240	 lr:0.025498 loss:1.465228
[Train] epoch:232	batch id:250	 lr:0.025498 loss:1.347644
[Train] epoch:232	batch id:260	 lr:0.025498 loss:1.510030
[Train] epoch:232	batch id:270	 lr:0.025498 loss:1.395455
[Train] epoch:232	batch id:280	 lr:0.025498 loss:1.400652
[Train] epoch:232	batch id:290	 lr:0.025498 loss:1.420496
[Train] epoch:232	batch id:300	 lr:0.025498 loss:1.411946
[Train] 232, loss: 1.426520, train acc: 0.965289, 
[Test] epoch:232	batch id:0	 loss:1.331957
[Test] epoch:232	batch id:10	 loss:1.478757
[Test] epoch:232	batch id:20	 loss:1.572658
[Test] epoch:232	batch id:30	 loss:1.477818
[Test] epoch:232	batch id:40	 loss:1.500883
[Test] epoch:232	batch id:50	 loss:1.474850
[Test] epoch:232	batch id:60	 loss:1.298238
[Test] epoch:232	batch id:70	 loss:1.454006
[Test] epoch:232	batch id:80	 loss:1.766366
[Test] epoch:232	batch id:90	 loss:1.459859
[Test] epoch:232	batch id:100	 loss:2.087677
[Test] epoch:232	batch id:110	 loss:1.466952
[Test] epoch:232	batch id:120	 loss:1.440879
[Test] epoch:232	batch id:130	 loss:1.452242
[Test] epoch:232	batch id:140	 loss:1.486433
[Test] epoch:232	batch id:150	 loss:1.791468
[Test] 232, loss: 1.509686, test acc: 0.903160,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:233	batch id:0	 lr:0.025115 loss:1.363651
[Train] epoch:233	batch id:10	 lr:0.025115 loss:1.370804
[Train] epoch:233	batch id:20	 lr:0.025115 loss:1.429056
[Train] epoch:233	batch id:30	 lr:0.025115 loss:1.400023
[Train] epoch:233	batch id:40	 lr:0.025115 loss:1.459997
[Train] epoch:233	batch id:50	 lr:0.025115 loss:1.477018
[Train] epoch:233	batch id:60	 lr:0.025115 loss:1.371608
[Train] epoch:233	batch id:70	 lr:0.025115 loss:1.380821
[Train] epoch:233	batch id:80	 lr:0.025115 loss:1.389813
[Train] epoch:233	batch id:90	 lr:0.025115 loss:1.469385
[Train] epoch:233	batch id:100	 lr:0.025115 loss:1.508978
[Train] epoch:233	batch id:110	 lr:0.025115 loss:1.458190
[Train] epoch:233	batch id:120	 lr:0.025115 loss:1.483446
[Train] epoch:233	batch id:130	 lr:0.025115 loss:1.488107
[Train] epoch:233	batch id:140	 lr:0.025115 loss:1.416844
[Train] epoch:233	batch id:150	 lr:0.025115 loss:1.707482
[Train] epoch:233	batch id:160	 lr:0.025115 loss:1.482953
[Train] epoch:233	batch id:170	 lr:0.025115 loss:1.443282
[Train] epoch:233	batch id:180	 lr:0.025115 loss:1.538253
[Train] epoch:233	batch id:190	 lr:0.025115 loss:1.406113
[Train] epoch:233	batch id:200	 lr:0.025115 loss:1.518451
[Train] epoch:233	batch id:210	 lr:0.025115 loss:1.318410
[Train] epoch:233	batch id:220	 lr:0.025115 loss:1.480515
[Train] epoch:233	batch id:230	 lr:0.025115 loss:1.430157
[Train] epoch:233	batch id:240	 lr:0.025115 loss:1.391801
[Train] epoch:233	batch id:250	 lr:0.025115 loss:1.412689
[Train] epoch:233	batch id:260	 lr:0.025115 loss:1.403073
[Train] epoch:233	batch id:270	 lr:0.025115 loss:1.418208
[Train] epoch:233	batch id:280	 lr:0.025115 loss:1.566267
[Train] epoch:233	batch id:290	 lr:0.025115 loss:1.414507
[Train] epoch:233	batch id:300	 lr:0.025115 loss:1.498556
[Train] 233, loss: 1.416068, train acc: 0.968241, 
[Test] epoch:233	batch id:0	 loss:1.276194
[Test] epoch:233	batch id:10	 loss:1.511073
[Test] epoch:233	batch id:20	 loss:1.517033
[Test] epoch:233	batch id:30	 loss:1.405850
[Test] epoch:233	batch id:40	 loss:1.526052
[Test] epoch:233	batch id:50	 loss:1.502607
[Test] epoch:233	batch id:60	 loss:1.303011
[Test] epoch:233	batch id:70	 loss:1.480524
[Test] epoch:233	batch id:80	 loss:1.609810
[Test] epoch:233	batch id:90	 loss:1.425259
[Test] epoch:233	batch id:100	 loss:2.167030
[Test] epoch:233	batch id:110	 loss:1.317202
[Test] epoch:233	batch id:120	 loss:1.337113
[Test] epoch:233	batch id:130	 loss:1.359597
[Test] epoch:233	batch id:140	 loss:1.401084
[Test] epoch:233	batch id:150	 loss:1.539121
[Test] 233, loss: 1.500464, test acc: 0.914911,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:234	batch id:0	 lr:0.024735 loss:1.417274
[Train] epoch:234	batch id:10	 lr:0.024735 loss:1.407261
[Train] epoch:234	batch id:20	 lr:0.024735 loss:1.442896
[Train] epoch:234	batch id:30	 lr:0.024735 loss:1.403871
[Train] epoch:234	batch id:40	 lr:0.024735 loss:1.440869
[Train] epoch:234	batch id:50	 lr:0.024735 loss:1.439230
[Train] epoch:234	batch id:60	 lr:0.024735 loss:1.440401
[Train] epoch:234	batch id:70	 lr:0.024735 loss:1.350644
[Train] epoch:234	batch id:80	 lr:0.024735 loss:1.401087
[Train] epoch:234	batch id:90	 lr:0.024735 loss:1.425422
[Train] epoch:234	batch id:100	 lr:0.024735 loss:1.439183
[Train] epoch:234	batch id:110	 lr:0.024735 loss:1.452058
[Train] epoch:234	batch id:120	 lr:0.024735 loss:1.557395
[Train] epoch:234	batch id:130	 lr:0.024735 loss:1.378274
[Train] epoch:234	batch id:140	 lr:0.024735 loss:1.384176
[Train] epoch:234	batch id:150	 lr:0.024735 loss:1.453313
[Train] epoch:234	batch id:160	 lr:0.024735 loss:1.361134
[Train] epoch:234	batch id:170	 lr:0.024735 loss:1.373917
[Train] epoch:234	batch id:180	 lr:0.024735 loss:1.495749
[Train] epoch:234	batch id:190	 lr:0.024735 loss:1.354829
[Train] epoch:234	batch id:200	 lr:0.024735 loss:1.331147
[Train] epoch:234	batch id:210	 lr:0.024735 loss:1.495162
[Train] epoch:234	batch id:220	 lr:0.024735 loss:1.376411
[Train] epoch:234	batch id:230	 lr:0.024735 loss:1.461264
[Train] epoch:234	batch id:240	 lr:0.024735 loss:1.431103
[Train] epoch:234	batch id:250	 lr:0.024735 loss:1.448891
[Train] epoch:234	batch id:260	 lr:0.024735 loss:1.503125
[Train] epoch:234	batch id:270	 lr:0.024735 loss:1.439734
[Train] epoch:234	batch id:280	 lr:0.024735 loss:1.357384
[Train] epoch:234	batch id:290	 lr:0.024735 loss:1.419480
[Train] epoch:234	batch id:300	 lr:0.024735 loss:1.506070
[Train] 234, loss: 1.422380, train acc: 0.967630, 
[Test] epoch:234	batch id:0	 loss:1.284546
[Test] epoch:234	batch id:10	 loss:1.464235
[Test] epoch:234	batch id:20	 loss:1.389216
[Test] epoch:234	batch id:30	 loss:1.359638
[Test] epoch:234	batch id:40	 loss:1.388486
[Test] epoch:234	batch id:50	 loss:1.519335
[Test] epoch:234	batch id:60	 loss:1.276721
[Test] epoch:234	batch id:70	 loss:1.441719
[Test] epoch:234	batch id:80	 loss:1.602488
[Test] epoch:234	batch id:90	 loss:1.492504
[Test] epoch:234	batch id:100	 loss:1.964293
[Test] epoch:234	batch id:110	 loss:1.372298
[Test] epoch:234	batch id:120	 loss:1.324992
[Test] epoch:234	batch id:130	 loss:1.384657
[Test] epoch:234	batch id:140	 loss:1.418366
[Test] epoch:234	batch id:150	 loss:1.705359
[Test] 234, loss: 1.479166, test acc: 0.910049,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:235	batch id:0	 lr:0.024357 loss:1.438966
[Train] epoch:235	batch id:10	 lr:0.024357 loss:1.397049
[Train] epoch:235	batch id:20	 lr:0.024357 loss:1.395491
[Train] epoch:235	batch id:30	 lr:0.024357 loss:1.392515
[Train] epoch:235	batch id:40	 lr:0.024357 loss:1.408657
[Train] epoch:235	batch id:50	 lr:0.024357 loss:1.380428
[Train] epoch:235	batch id:60	 lr:0.024357 loss:1.392274
[Train] epoch:235	batch id:70	 lr:0.024357 loss:1.451561
[Train] epoch:235	batch id:80	 lr:0.024357 loss:1.356093
[Train] epoch:235	batch id:90	 lr:0.024357 loss:1.420601
[Train] epoch:235	batch id:100	 lr:0.024357 loss:1.414860
[Train] epoch:235	batch id:110	 lr:0.024357 loss:1.406141
[Train] epoch:235	batch id:120	 lr:0.024357 loss:1.409854
[Train] epoch:235	batch id:130	 lr:0.024357 loss:1.385132
[Train] epoch:235	batch id:140	 lr:0.024357 loss:1.499107
[Train] epoch:235	batch id:150	 lr:0.024357 loss:1.388667
[Train] epoch:235	batch id:160	 lr:0.024357 loss:1.470013
[Train] epoch:235	batch id:170	 lr:0.024357 loss:1.387908
[Train] epoch:235	batch id:180	 lr:0.024357 loss:1.462685
[Train] epoch:235	batch id:190	 lr:0.024357 loss:1.409316
[Train] epoch:235	batch id:200	 lr:0.024357 loss:1.461046
[Train] epoch:235	batch id:210	 lr:0.024357 loss:1.466264
[Train] epoch:235	batch id:220	 lr:0.024357 loss:1.405573
[Train] epoch:235	batch id:230	 lr:0.024357 loss:1.408388
[Train] epoch:235	batch id:240	 lr:0.024357 loss:1.574234
[Train] epoch:235	batch id:250	 lr:0.024357 loss:1.361539
[Train] epoch:235	batch id:260	 lr:0.024357 loss:1.439917
[Train] epoch:235	batch id:270	 lr:0.024357 loss:1.495651
[Train] epoch:235	batch id:280	 lr:0.024357 loss:1.399283
[Train] epoch:235	batch id:290	 lr:0.024357 loss:1.421813
[Train] epoch:235	batch id:300	 lr:0.024357 loss:1.506053
[Train] 235, loss: 1.415868, train acc: 0.968954, 
[Test] epoch:235	batch id:0	 loss:1.302543
[Test] epoch:235	batch id:10	 loss:1.405603
[Test] epoch:235	batch id:20	 loss:1.401798
[Test] epoch:235	batch id:30	 loss:1.387633
[Test] epoch:235	batch id:40	 loss:1.350152
[Test] epoch:235	batch id:50	 loss:1.585964
[Test] epoch:235	batch id:60	 loss:1.261222
[Test] epoch:235	batch id:70	 loss:1.482577
[Test] epoch:235	batch id:80	 loss:1.571986
[Test] epoch:235	batch id:90	 loss:1.535956
[Test] epoch:235	batch id:100	 loss:1.821233
[Test] epoch:235	batch id:110	 loss:1.431876
[Test] epoch:235	batch id:120	 loss:1.331366
[Test] epoch:235	batch id:130	 loss:1.458847
[Test] epoch:235	batch id:140	 loss:1.375367
[Test] epoch:235	batch id:150	 loss:1.742028
[Test] 235, loss: 1.476579, test acc: 0.915721,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:236	batch id:0	 lr:0.023980 loss:1.448635
[Train] epoch:236	batch id:10	 lr:0.023980 loss:1.387701
[Train] epoch:236	batch id:20	 lr:0.023980 loss:1.370468
[Train] epoch:236	batch id:30	 lr:0.023980 loss:1.392896
[Train] epoch:236	batch id:40	 lr:0.023980 loss:1.393290
[Train] epoch:236	batch id:50	 lr:0.023980 loss:1.390515
[Train] epoch:236	batch id:60	 lr:0.023980 loss:1.322671
[Train] epoch:236	batch id:70	 lr:0.023980 loss:1.506131
[Train] epoch:236	batch id:80	 lr:0.023980 loss:1.395214
[Train] epoch:236	batch id:90	 lr:0.023980 loss:1.402616
[Train] epoch:236	batch id:100	 lr:0.023980 loss:1.512886
[Train] epoch:236	batch id:110	 lr:0.023980 loss:1.376642
[Train] epoch:236	batch id:120	 lr:0.023980 loss:1.424380
[Train] epoch:236	batch id:130	 lr:0.023980 loss:1.371615
[Train] epoch:236	batch id:140	 lr:0.023980 loss:1.388460
[Train] epoch:236	batch id:150	 lr:0.023980 loss:1.474094
[Train] epoch:236	batch id:160	 lr:0.023980 loss:1.443363
[Train] epoch:236	batch id:170	 lr:0.023980 loss:1.437541
[Train] epoch:236	batch id:180	 lr:0.023980 loss:1.429246
[Train] epoch:236	batch id:190	 lr:0.023980 loss:1.422038
[Train] epoch:236	batch id:200	 lr:0.023980 loss:1.438498
[Train] epoch:236	batch id:210	 lr:0.023980 loss:1.400361
[Train] epoch:236	batch id:220	 lr:0.023980 loss:1.410355
[Train] epoch:236	batch id:230	 lr:0.023980 loss:1.484570
[Train] epoch:236	batch id:240	 lr:0.023980 loss:1.409605
[Train] epoch:236	batch id:250	 lr:0.023980 loss:1.395804
[Train] epoch:236	batch id:260	 lr:0.023980 loss:1.490052
[Train] epoch:236	batch id:270	 lr:0.023980 loss:1.364428
[Train] epoch:236	batch id:280	 lr:0.023980 loss:1.460255
[Train] epoch:236	batch id:290	 lr:0.023980 loss:1.465218
[Train] epoch:236	batch id:300	 lr:0.023980 loss:1.368976
[Train] 236, loss: 1.410237, train acc: 0.969157, 
[Test] epoch:236	batch id:0	 loss:1.286246
[Test] epoch:236	batch id:10	 loss:1.381510
[Test] epoch:236	batch id:20	 loss:1.491236
[Test] epoch:236	batch id:30	 loss:1.426009
[Test] epoch:236	batch id:40	 loss:1.509292
[Test] epoch:236	batch id:50	 loss:1.552322
[Test] epoch:236	batch id:60	 loss:1.256038
[Test] epoch:236	batch id:70	 loss:1.467168
[Test] epoch:236	batch id:80	 loss:1.623661
[Test] epoch:236	batch id:90	 loss:1.492610
[Test] epoch:236	batch id:100	 loss:1.980104
[Test] epoch:236	batch id:110	 loss:1.360022
[Test] epoch:236	batch id:120	 loss:1.372413
[Test] epoch:236	batch id:130	 loss:1.362262
[Test] epoch:236	batch id:140	 loss:1.439788
[Test] epoch:236	batch id:150	 loss:1.688812
[Test] 236, loss: 1.493701, test acc: 0.908023,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:237	batch id:0	 lr:0.023606 loss:1.552824
[Train] epoch:237	batch id:10	 lr:0.023606 loss:1.474285
[Train] epoch:237	batch id:20	 lr:0.023606 loss:1.371148
[Train] epoch:237	batch id:30	 lr:0.023606 loss:1.369034
[Train] epoch:237	batch id:40	 lr:0.023606 loss:1.488637
[Train] epoch:237	batch id:50	 lr:0.023606 loss:1.389669
[Train] epoch:237	batch id:60	 lr:0.023606 loss:1.414501
[Train] epoch:237	batch id:70	 lr:0.023606 loss:1.363716
[Train] epoch:237	batch id:80	 lr:0.023606 loss:1.485658
[Train] epoch:237	batch id:90	 lr:0.023606 loss:1.387065
[Train] epoch:237	batch id:100	 lr:0.023606 loss:1.412102
[Train] epoch:237	batch id:110	 lr:0.023606 loss:1.514445
[Train] epoch:237	batch id:120	 lr:0.023606 loss:1.429543
[Train] epoch:237	batch id:130	 lr:0.023606 loss:1.396520
[Train] epoch:237	batch id:140	 lr:0.023606 loss:1.398714
[Train] epoch:237	batch id:150	 lr:0.023606 loss:1.378302
[Train] epoch:237	batch id:160	 lr:0.023606 loss:1.403498
[Train] epoch:237	batch id:170	 lr:0.023606 loss:1.473262
[Train] epoch:237	batch id:180	 lr:0.023606 loss:1.418613
[Train] epoch:237	batch id:190	 lr:0.023606 loss:1.407931
[Train] epoch:237	batch id:200	 lr:0.023606 loss:1.350200
[Train] epoch:237	batch id:210	 lr:0.023606 loss:1.394220
[Train] epoch:237	batch id:220	 lr:0.023606 loss:1.512734
[Train] epoch:237	batch id:230	 lr:0.023606 loss:1.488901
[Train] epoch:237	batch id:240	 lr:0.023606 loss:1.440768
[Train] epoch:237	batch id:250	 lr:0.023606 loss:1.344759
[Train] epoch:237	batch id:260	 lr:0.023606 loss:1.398196
[Train] epoch:237	batch id:270	 lr:0.023606 loss:1.362779
[Train] epoch:237	batch id:280	 lr:0.023606 loss:1.591130
[Train] epoch:237	batch id:290	 lr:0.023606 loss:1.422806
[Train] epoch:237	batch id:300	 lr:0.023606 loss:1.366596
[Train] 237, loss: 1.415466, train acc: 0.968343, 
[Test] epoch:237	batch id:0	 loss:1.300122
[Test] epoch:237	batch id:10	 loss:1.425444
[Test] epoch:237	batch id:20	 loss:1.513166
[Test] epoch:237	batch id:30	 loss:1.440531
[Test] epoch:237	batch id:40	 loss:1.425167
[Test] epoch:237	batch id:50	 loss:1.463098
[Test] epoch:237	batch id:60	 loss:1.276704
[Test] epoch:237	batch id:70	 loss:1.492120
[Test] epoch:237	batch id:80	 loss:1.592898
[Test] epoch:237	batch id:90	 loss:1.462710
[Test] epoch:237	batch id:100	 loss:1.750942
[Test] epoch:237	batch id:110	 loss:1.418091
[Test] epoch:237	batch id:120	 loss:1.324764
[Test] epoch:237	batch id:130	 loss:1.375674
[Test] epoch:237	batch id:140	 loss:1.318520
[Test] epoch:237	batch id:150	 loss:1.821698
[Test] 237, loss: 1.483072, test acc: 0.908023,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:238	batch id:0	 lr:0.023235 loss:1.395854
[Train] epoch:238	batch id:10	 lr:0.023235 loss:1.495834
[Train] epoch:238	batch id:20	 lr:0.023235 loss:1.417153
[Train] epoch:238	batch id:30	 lr:0.023235 loss:1.481928
[Train] epoch:238	batch id:40	 lr:0.023235 loss:1.370189
[Train] epoch:238	batch id:50	 lr:0.023235 loss:1.378943
[Train] epoch:238	batch id:60	 lr:0.023235 loss:1.392688
[Train] epoch:238	batch id:70	 lr:0.023235 loss:1.331718
[Train] epoch:238	batch id:80	 lr:0.023235 loss:1.418688
[Train] epoch:238	batch id:90	 lr:0.023235 loss:1.455278
[Train] epoch:238	batch id:100	 lr:0.023235 loss:1.551655
[Train] epoch:238	batch id:110	 lr:0.023235 loss:1.385700
[Train] epoch:238	batch id:120	 lr:0.023235 loss:1.390763
[Train] epoch:238	batch id:130	 lr:0.023235 loss:1.404729
[Train] epoch:238	batch id:140	 lr:0.023235 loss:1.406702
[Train] epoch:238	batch id:150	 lr:0.023235 loss:1.383367
[Train] epoch:238	batch id:160	 lr:0.023235 loss:1.448486
[Train] epoch:238	batch id:170	 lr:0.023235 loss:1.464990
[Train] epoch:238	batch id:180	 lr:0.023235 loss:1.389749
[Train] epoch:238	batch id:190	 lr:0.023235 loss:1.357653
[Train] epoch:238	batch id:200	 lr:0.023235 loss:1.503875
[Train] epoch:238	batch id:210	 lr:0.023235 loss:1.382940
[Train] epoch:238	batch id:220	 lr:0.023235 loss:1.361100
[Train] epoch:238	batch id:230	 lr:0.023235 loss:1.427207
[Train] epoch:238	batch id:240	 lr:0.023235 loss:1.464708
[Train] epoch:238	batch id:250	 lr:0.023235 loss:1.364047
[Train] epoch:238	batch id:260	 lr:0.023235 loss:1.376343
[Train] epoch:238	batch id:270	 lr:0.023235 loss:1.531843
[Train] epoch:238	batch id:280	 lr:0.023235 loss:1.414039
[Train] epoch:238	batch id:290	 lr:0.023235 loss:1.346355
[Train] epoch:238	batch id:300	 lr:0.023235 loss:1.447655
[Train] 238, loss: 1.409166, train acc: 0.974552, 
[Test] epoch:238	batch id:0	 loss:1.409338
[Test] epoch:238	batch id:10	 loss:1.457719
[Test] epoch:238	batch id:20	 loss:1.372964
[Test] epoch:238	batch id:30	 loss:1.616611
[Test] epoch:238	batch id:40	 loss:1.468133
[Test] epoch:238	batch id:50	 loss:1.514853
[Test] epoch:238	batch id:60	 loss:1.266466
[Test] epoch:238	batch id:70	 loss:1.510530
[Test] epoch:238	batch id:80	 loss:1.579886
[Test] epoch:238	batch id:90	 loss:1.558325
[Test] epoch:238	batch id:100	 loss:2.090073
[Test] epoch:238	batch id:110	 loss:1.361997
[Test] epoch:238	batch id:120	 loss:1.367918
[Test] epoch:238	batch id:130	 loss:1.380600
[Test] epoch:238	batch id:140	 loss:1.324760
[Test] epoch:238	batch id:150	 loss:1.882900
[Test] 238, loss: 1.497193, test acc: 0.901945,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:239	batch id:0	 lr:0.022865 loss:1.456696
[Train] epoch:239	batch id:10	 lr:0.022865 loss:1.369507
[Train] epoch:239	batch id:20	 lr:0.022865 loss:1.387442
[Train] epoch:239	batch id:30	 lr:0.022865 loss:1.366166
[Train] epoch:239	batch id:40	 lr:0.022865 loss:1.383274
[Train] epoch:239	batch id:50	 lr:0.022865 loss:1.339523
[Train] epoch:239	batch id:60	 lr:0.022865 loss:1.390851
[Train] epoch:239	batch id:70	 lr:0.022865 loss:1.371329
[Train] epoch:239	batch id:80	 lr:0.022865 loss:1.338681
[Train] epoch:239	batch id:90	 lr:0.022865 loss:1.397968
[Train] epoch:239	batch id:100	 lr:0.022865 loss:1.362545
[Train] epoch:239	batch id:110	 lr:0.022865 loss:1.409051
[Train] epoch:239	batch id:120	 lr:0.022865 loss:1.418966
[Train] epoch:239	batch id:130	 lr:0.022865 loss:1.476664
[Train] epoch:239	batch id:140	 lr:0.022865 loss:1.431645
[Train] epoch:239	batch id:150	 lr:0.022865 loss:1.398962
[Train] epoch:239	batch id:160	 lr:0.022865 loss:1.457915
[Train] epoch:239	batch id:170	 lr:0.022865 loss:1.398557
[Train] epoch:239	batch id:180	 lr:0.022865 loss:1.380303
[Train] epoch:239	batch id:190	 lr:0.022865 loss:1.410482
[Train] epoch:239	batch id:200	 lr:0.022865 loss:1.454717
[Train] epoch:239	batch id:210	 lr:0.022865 loss:1.416474
[Train] epoch:239	batch id:220	 lr:0.022865 loss:1.400202
[Train] epoch:239	batch id:230	 lr:0.022865 loss:1.377041
[Train] epoch:239	batch id:240	 lr:0.022865 loss:1.344775
[Train] epoch:239	batch id:250	 lr:0.022865 loss:1.415293
[Train] epoch:239	batch id:260	 lr:0.022865 loss:1.384144
[Train] epoch:239	batch id:270	 lr:0.022865 loss:1.474511
[Train] epoch:239	batch id:280	 lr:0.022865 loss:1.347620
[Train] epoch:239	batch id:290	 lr:0.022865 loss:1.339092
[Train] epoch:239	batch id:300	 lr:0.022865 loss:1.427608
[Train] 239, loss: 1.410519, train acc: 0.970989, 
[Test] epoch:239	batch id:0	 loss:1.293966
[Test] epoch:239	batch id:10	 loss:1.357059
[Test] epoch:239	batch id:20	 loss:1.494388
[Test] epoch:239	batch id:30	 loss:1.381827
[Test] epoch:239	batch id:40	 loss:1.479326
[Test] epoch:239	batch id:50	 loss:1.454334
[Test] epoch:239	batch id:60	 loss:1.268683
[Test] epoch:239	batch id:70	 loss:1.369485
[Test] epoch:239	batch id:80	 loss:1.577338
[Test] epoch:239	batch id:90	 loss:1.493884
[Test] epoch:239	batch id:100	 loss:2.158489
[Test] epoch:239	batch id:110	 loss:1.372153
[Test] epoch:239	batch id:120	 loss:1.317037
[Test] epoch:239	batch id:130	 loss:1.316617
[Test] epoch:239	batch id:140	 loss:1.285217
[Test] epoch:239	batch id:150	 loss:1.745194
[Test] 239, loss: 1.462706, test acc: 0.925041,
Max Acc:0.925041
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:240	batch id:0	 lr:0.022497 loss:1.459805
[Train] epoch:240	batch id:10	 lr:0.022497 loss:1.388806
[Train] epoch:240	batch id:20	 lr:0.022497 loss:1.492193
[Train] epoch:240	batch id:30	 lr:0.022497 loss:1.425349
[Train] epoch:240	batch id:40	 lr:0.022497 loss:1.361228
[Train] epoch:240	batch id:50	 lr:0.022497 loss:1.411442
[Train] epoch:240	batch id:60	 lr:0.022497 loss:1.398495
[Train] epoch:240	batch id:70	 lr:0.022497 loss:1.452289
[Train] epoch:240	batch id:80	 lr:0.022497 loss:1.363042
[Train] epoch:240	batch id:90	 lr:0.022497 loss:1.390035
[Train] epoch:240	batch id:100	 lr:0.022497 loss:1.359788
[Train] epoch:240	batch id:110	 lr:0.022497 loss:1.503346
[Train] epoch:240	batch id:120	 lr:0.022497 loss:1.383099
[Train] epoch:240	batch id:130	 lr:0.022497 loss:1.398241
[Train] epoch:240	batch id:140	 lr:0.022497 loss:1.438096
[Train] epoch:240	batch id:150	 lr:0.022497 loss:1.468814
[Train] epoch:240	batch id:160	 lr:0.022497 loss:1.378408
[Train] epoch:240	batch id:170	 lr:0.022497 loss:1.430182
[Train] epoch:240	batch id:180	 lr:0.022497 loss:1.427210
[Train] epoch:240	batch id:190	 lr:0.022497 loss:1.347597
[Train] epoch:240	batch id:200	 lr:0.022497 loss:1.382612
[Train] epoch:240	batch id:210	 lr:0.022497 loss:1.399643
[Train] epoch:240	batch id:220	 lr:0.022497 loss:1.349967
[Train] epoch:240	batch id:230	 lr:0.022497 loss:1.414821
[Train] epoch:240	batch id:240	 lr:0.022497 loss:1.484371
[Train] epoch:240	batch id:250	 lr:0.022497 loss:1.383434
[Train] epoch:240	batch id:260	 lr:0.022497 loss:1.369254
[Train] epoch:240	batch id:270	 lr:0.022497 loss:1.431442
[Train] epoch:240	batch id:280	 lr:0.022497 loss:1.388467
[Train] epoch:240	batch id:290	 lr:0.022497 loss:1.330050
[Train] epoch:240	batch id:300	 lr:0.022497 loss:1.439239
[Train] 240, loss: 1.410566, train acc: 0.969870, 
[Test] epoch:240	batch id:0	 loss:1.321814
[Test] epoch:240	batch id:10	 loss:1.381986
[Test] epoch:240	batch id:20	 loss:1.445283
[Test] epoch:240	batch id:30	 loss:1.515137
[Test] epoch:240	batch id:40	 loss:1.419599
[Test] epoch:240	batch id:50	 loss:1.540406
[Test] epoch:240	batch id:60	 loss:1.298132
[Test] epoch:240	batch id:70	 loss:1.489108
[Test] epoch:240	batch id:80	 loss:1.627547
[Test] epoch:240	batch id:90	 loss:1.696654
[Test] epoch:240	batch id:100	 loss:2.063117
[Test] epoch:240	batch id:110	 loss:1.371984
[Test] epoch:240	batch id:120	 loss:1.353254
[Test] epoch:240	batch id:130	 loss:1.354095
[Test] epoch:240	batch id:140	 loss:1.356850
[Test] epoch:240	batch id:150	 loss:1.937879
[Test] 240, loss: 1.493992, test acc: 0.920178,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:241	batch id:0	 lr:0.022132 loss:1.383577
[Train] epoch:241	batch id:10	 lr:0.022132 loss:1.460314
[Train] epoch:241	batch id:20	 lr:0.022132 loss:1.324859
[Train] epoch:241	batch id:30	 lr:0.022132 loss:1.501731
[Train] epoch:241	batch id:40	 lr:0.022132 loss:1.367072
[Train] epoch:241	batch id:50	 lr:0.022132 loss:1.450057
[Train] epoch:241	batch id:60	 lr:0.022132 loss:1.340762
[Train] epoch:241	batch id:70	 lr:0.022132 loss:1.402152
[Train] epoch:241	batch id:80	 lr:0.022132 loss:1.344860
[Train] epoch:241	batch id:90	 lr:0.022132 loss:1.384015
[Train] epoch:241	batch id:100	 lr:0.022132 loss:1.421681
[Train] epoch:241	batch id:110	 lr:0.022132 loss:1.391772
[Train] epoch:241	batch id:120	 lr:0.022132 loss:1.399318
[Train] epoch:241	batch id:130	 lr:0.022132 loss:1.418414
[Train] epoch:241	batch id:140	 lr:0.022132 loss:1.430290
[Train] epoch:241	batch id:150	 lr:0.022132 loss:1.456880
[Train] epoch:241	batch id:160	 lr:0.022132 loss:1.431357
[Train] epoch:241	batch id:170	 lr:0.022132 loss:1.376527
[Train] epoch:241	batch id:180	 lr:0.022132 loss:1.361459
[Train] epoch:241	batch id:190	 lr:0.022132 loss:1.375998
[Train] epoch:241	batch id:200	 lr:0.022132 loss:1.400263
[Train] epoch:241	batch id:210	 lr:0.022132 loss:1.384655
[Train] epoch:241	batch id:220	 lr:0.022132 loss:1.472273
[Train] epoch:241	batch id:230	 lr:0.022132 loss:1.420300
[Train] epoch:241	batch id:240	 lr:0.022132 loss:1.461162
[Train] epoch:241	batch id:250	 lr:0.022132 loss:1.409914
[Train] epoch:241	batch id:260	 lr:0.022132 loss:1.382248
[Train] epoch:241	batch id:270	 lr:0.022132 loss:1.389019
[Train] epoch:241	batch id:280	 lr:0.022132 loss:1.366722
[Train] epoch:241	batch id:290	 lr:0.022132 loss:1.377930
[Train] epoch:241	batch id:300	 lr:0.022132 loss:1.351711
[Train] 241, loss: 1.408293, train acc: 0.973127, 
[Test] epoch:241	batch id:0	 loss:1.270235
[Test] epoch:241	batch id:10	 loss:1.363315
[Test] epoch:241	batch id:20	 loss:1.481633
[Test] epoch:241	batch id:30	 loss:1.436523
[Test] epoch:241	batch id:40	 loss:1.472888
[Test] epoch:241	batch id:50	 loss:1.433336
[Test] epoch:241	batch id:60	 loss:1.264741
[Test] epoch:241	batch id:70	 loss:1.440160
[Test] epoch:241	batch id:80	 loss:1.613539
[Test] epoch:241	batch id:90	 loss:1.426783
[Test] epoch:241	batch id:100	 loss:1.867902
[Test] epoch:241	batch id:110	 loss:1.346147
[Test] epoch:241	batch id:120	 loss:1.312727
[Test] epoch:241	batch id:130	 loss:1.384522
[Test] epoch:241	batch id:140	 loss:1.383516
[Test] epoch:241	batch id:150	 loss:1.879086
[Test] 241, loss: 1.472868, test acc: 0.914911,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:242	batch id:0	 lr:0.021769 loss:1.359662
[Train] epoch:242	batch id:10	 lr:0.021769 loss:1.400519
[Train] epoch:242	batch id:20	 lr:0.021769 loss:1.421374
[Train] epoch:242	batch id:30	 lr:0.021769 loss:1.355222
[Train] epoch:242	batch id:40	 lr:0.021769 loss:1.338537
[Train] epoch:242	batch id:50	 lr:0.021769 loss:1.358518
[Train] epoch:242	batch id:60	 lr:0.021769 loss:1.483388
[Train] epoch:242	batch id:70	 lr:0.021769 loss:1.473235
[Train] epoch:242	batch id:80	 lr:0.021769 loss:1.396775
[Train] epoch:242	batch id:90	 lr:0.021769 loss:1.356512
[Train] epoch:242	batch id:100	 lr:0.021769 loss:1.393956
[Train] epoch:242	batch id:110	 lr:0.021769 loss:1.433896
[Train] epoch:242	batch id:120	 lr:0.021769 loss:1.405662
[Train] epoch:242	batch id:130	 lr:0.021769 loss:1.338304
[Train] epoch:242	batch id:140	 lr:0.021769 loss:1.333987
[Train] epoch:242	batch id:150	 lr:0.021769 loss:1.338559
[Train] epoch:242	batch id:160	 lr:0.021769 loss:1.416139
[Train] epoch:242	batch id:170	 lr:0.021769 loss:1.446261
[Train] epoch:242	batch id:180	 lr:0.021769 loss:1.451810
[Train] epoch:242	batch id:190	 lr:0.021769 loss:1.378229
[Train] epoch:242	batch id:200	 lr:0.021769 loss:1.415363
[Train] epoch:242	batch id:210	 lr:0.021769 loss:1.375171
[Train] epoch:242	batch id:220	 lr:0.021769 loss:1.434018
[Train] epoch:242	batch id:230	 lr:0.021769 loss:1.337051
[Train] epoch:242	batch id:240	 lr:0.021769 loss:1.421928
[Train] epoch:242	batch id:250	 lr:0.021769 loss:1.445271
[Train] epoch:242	batch id:260	 lr:0.021769 loss:1.482287
[Train] epoch:242	batch id:270	 lr:0.021769 loss:1.452639
[Train] epoch:242	batch id:280	 lr:0.021769 loss:1.392119
[Train] epoch:242	batch id:290	 lr:0.021769 loss:1.386297
[Train] epoch:242	batch id:300	 lr:0.021769 loss:1.414091
[Train] 242, loss: 1.404834, train acc: 0.973229, 
[Test] epoch:242	batch id:0	 loss:1.279005
[Test] epoch:242	batch id:10	 loss:1.533620
[Test] epoch:242	batch id:20	 loss:1.499364
[Test] epoch:242	batch id:30	 loss:1.467043
[Test] epoch:242	batch id:40	 loss:1.418384
[Test] epoch:242	batch id:50	 loss:1.414836
[Test] epoch:242	batch id:60	 loss:1.274198
[Test] epoch:242	batch id:70	 loss:1.530976
[Test] epoch:242	batch id:80	 loss:1.669991
[Test] epoch:242	batch id:90	 loss:1.446268
[Test] epoch:242	batch id:100	 loss:2.134232
[Test] epoch:242	batch id:110	 loss:1.312899
[Test] epoch:242	batch id:120	 loss:1.308996
[Test] epoch:242	batch id:130	 loss:1.321960
[Test] epoch:242	batch id:140	 loss:1.360460
[Test] epoch:242	batch id:150	 loss:1.749090
[Test] 242, loss: 1.469086, test acc: 0.920178,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:243	batch id:0	 lr:0.021409 loss:1.454376
[Train] epoch:243	batch id:10	 lr:0.021409 loss:1.389014
[Train] epoch:243	batch id:20	 lr:0.021409 loss:1.382479
[Train] epoch:243	batch id:30	 lr:0.021409 loss:1.378227
[Train] epoch:243	batch id:40	 lr:0.021409 loss:1.448987
[Train] epoch:243	batch id:50	 lr:0.021409 loss:1.414129
[Train] epoch:243	batch id:60	 lr:0.021409 loss:1.466465
[Train] epoch:243	batch id:70	 lr:0.021409 loss:1.407601
[Train] epoch:243	batch id:80	 lr:0.021409 loss:1.430129
[Train] epoch:243	batch id:90	 lr:0.021409 loss:1.384373
[Train] epoch:243	batch id:100	 lr:0.021409 loss:1.351939
[Train] epoch:243	batch id:110	 lr:0.021409 loss:1.431540
[Train] epoch:243	batch id:120	 lr:0.021409 loss:1.433813
[Train] epoch:243	batch id:130	 lr:0.021409 loss:1.397665
[Train] epoch:243	batch id:140	 lr:0.021409 loss:1.395738
[Train] epoch:243	batch id:150	 lr:0.021409 loss:1.375972
[Train] epoch:243	batch id:160	 lr:0.021409 loss:1.371161
[Train] epoch:243	batch id:170	 lr:0.021409 loss:1.355752
[Train] epoch:243	batch id:180	 lr:0.021409 loss:1.502853
[Train] epoch:243	batch id:190	 lr:0.021409 loss:1.343941
[Train] epoch:243	batch id:200	 lr:0.021409 loss:1.447881
[Train] epoch:243	batch id:210	 lr:0.021409 loss:1.408981
[Train] epoch:243	batch id:220	 lr:0.021409 loss:1.475567
[Train] epoch:243	batch id:230	 lr:0.021409 loss:1.475974
[Train] epoch:243	batch id:240	 lr:0.021409 loss:1.393291
[Train] epoch:243	batch id:250	 lr:0.021409 loss:1.428511
[Train] epoch:243	batch id:260	 lr:0.021409 loss:1.489430
[Train] epoch:243	batch id:270	 lr:0.021409 loss:1.362926
[Train] epoch:243	batch id:280	 lr:0.021409 loss:1.502727
[Train] epoch:243	batch id:290	 lr:0.021409 loss:1.385724
[Train] epoch:243	batch id:300	 lr:0.021409 loss:1.396213
[Train] 243, loss: 1.405586, train acc: 0.973941, 
[Test] epoch:243	batch id:0	 loss:1.283119
[Test] epoch:243	batch id:10	 loss:1.375901
[Test] epoch:243	batch id:20	 loss:1.490445
[Test] epoch:243	batch id:30	 loss:1.556509
[Test] epoch:243	batch id:40	 loss:1.554850
[Test] epoch:243	batch id:50	 loss:1.530781
[Test] epoch:243	batch id:60	 loss:1.273027
[Test] epoch:243	batch id:70	 loss:1.418720
[Test] epoch:243	batch id:80	 loss:1.665845
[Test] epoch:243	batch id:90	 loss:1.606007
[Test] epoch:243	batch id:100	 loss:2.000626
[Test] epoch:243	batch id:110	 loss:1.381447
[Test] epoch:243	batch id:120	 loss:1.316947
[Test] epoch:243	batch id:130	 loss:1.340184
[Test] epoch:243	batch id:140	 loss:1.379304
[Test] epoch:243	batch id:150	 loss:1.776629
[Test] 243, loss: 1.486539, test acc: 0.910859,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:244	batch id:0	 lr:0.021050 loss:1.332976
[Train] epoch:244	batch id:10	 lr:0.021050 loss:1.363724
[Train] epoch:244	batch id:20	 lr:0.021050 loss:1.435311
[Train] epoch:244	batch id:30	 lr:0.021050 loss:1.362700
[Train] epoch:244	batch id:40	 lr:0.021050 loss:1.388102
[Train] epoch:244	batch id:50	 lr:0.021050 loss:1.507178
[Train] epoch:244	batch id:60	 lr:0.021050 loss:1.438216
[Train] epoch:244	batch id:70	 lr:0.021050 loss:1.454505
[Train] epoch:244	batch id:80	 lr:0.021050 loss:1.437995
[Train] epoch:244	batch id:90	 lr:0.021050 loss:1.467273
[Train] epoch:244	batch id:100	 lr:0.021050 loss:1.392196
[Train] epoch:244	batch id:110	 lr:0.021050 loss:1.359317
[Train] epoch:244	batch id:120	 lr:0.021050 loss:1.432296
[Train] epoch:244	batch id:130	 lr:0.021050 loss:1.433167
[Train] epoch:244	batch id:140	 lr:0.021050 loss:1.501669
[Train] epoch:244	batch id:150	 lr:0.021050 loss:1.356849
[Train] epoch:244	batch id:160	 lr:0.021050 loss:1.373276
[Train] epoch:244	batch id:170	 lr:0.021050 loss:1.459499
[Train] epoch:244	batch id:180	 lr:0.021050 loss:1.361582
[Train] epoch:244	batch id:190	 lr:0.021050 loss:1.498955
[Train] epoch:244	batch id:200	 lr:0.021050 loss:1.349535
[Train] epoch:244	batch id:210	 lr:0.021050 loss:1.340369
[Train] epoch:244	batch id:220	 lr:0.021050 loss:1.372929
[Train] epoch:244	batch id:230	 lr:0.021050 loss:1.331621
[Train] epoch:244	batch id:240	 lr:0.021050 loss:1.394675
[Train] epoch:244	batch id:250	 lr:0.021050 loss:1.400721
[Train] epoch:244	batch id:260	 lr:0.021050 loss:1.453123
[Train] epoch:244	batch id:270	 lr:0.021050 loss:1.413711
[Train] epoch:244	batch id:280	 lr:0.021050 loss:1.376784
[Train] epoch:244	batch id:290	 lr:0.021050 loss:1.370153
[Train] epoch:244	batch id:300	 lr:0.021050 loss:1.344731
[Train] 244, loss: 1.402964, train acc: 0.972109, 
[Test] epoch:244	batch id:0	 loss:1.278540
[Test] epoch:244	batch id:10	 loss:1.424523
[Test] epoch:244	batch id:20	 loss:1.443009
[Test] epoch:244	batch id:30	 loss:1.462260
[Test] epoch:244	batch id:40	 loss:1.436011
[Test] epoch:244	batch id:50	 loss:1.538109
[Test] epoch:244	batch id:60	 loss:1.272683
[Test] epoch:244	batch id:70	 loss:1.466170
[Test] epoch:244	batch id:80	 loss:1.701548
[Test] epoch:244	batch id:90	 loss:1.676237
[Test] epoch:244	batch id:100	 loss:1.980818
[Test] epoch:244	batch id:110	 loss:1.488975
[Test] epoch:244	batch id:120	 loss:1.339066
[Test] epoch:244	batch id:130	 loss:1.384323
[Test] epoch:244	batch id:140	 loss:1.358660
[Test] epoch:244	batch id:150	 loss:1.833341
[Test] 244, loss: 1.502034, test acc: 0.905592,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:245	batch id:0	 lr:0.020694 loss:1.388732
[Train] epoch:245	batch id:10	 lr:0.020694 loss:1.401740
[Train] epoch:245	batch id:20	 lr:0.020694 loss:1.407382
[Train] epoch:245	batch id:30	 lr:0.020694 loss:1.428200
[Train] epoch:245	batch id:40	 lr:0.020694 loss:1.380477
[Train] epoch:245	batch id:50	 lr:0.020694 loss:1.410716
[Train] epoch:245	batch id:60	 lr:0.020694 loss:1.371894
[Train] epoch:245	batch id:70	 lr:0.020694 loss:1.397125
[Train] epoch:245	batch id:80	 lr:0.020694 loss:1.358099
[Train] epoch:245	batch id:90	 lr:0.020694 loss:1.437429
[Train] epoch:245	batch id:100	 lr:0.020694 loss:1.438751
[Train] epoch:245	batch id:110	 lr:0.020694 loss:1.355340
[Train] epoch:245	batch id:120	 lr:0.020694 loss:1.341406
[Train] epoch:245	batch id:130	 lr:0.020694 loss:1.411331
[Train] epoch:245	batch id:140	 lr:0.020694 loss:1.345538
[Train] epoch:245	batch id:150	 lr:0.020694 loss:1.395372
[Train] epoch:245	batch id:160	 lr:0.020694 loss:1.331349
[Train] epoch:245	batch id:170	 lr:0.020694 loss:1.368770
[Train] epoch:245	batch id:180	 lr:0.020694 loss:1.408564
[Train] epoch:245	batch id:190	 lr:0.020694 loss:1.355341
[Train] epoch:245	batch id:200	 lr:0.020694 loss:1.389431
[Train] epoch:245	batch id:210	 lr:0.020694 loss:1.457720
[Train] epoch:245	batch id:220	 lr:0.020694 loss:1.405083
[Train] epoch:245	batch id:230	 lr:0.020694 loss:1.480723
[Train] epoch:245	batch id:240	 lr:0.020694 loss:1.404631
[Train] epoch:245	batch id:250	 lr:0.020694 loss:1.526947
[Train] epoch:245	batch id:260	 lr:0.020694 loss:1.357327
[Train] epoch:245	batch id:270	 lr:0.020694 loss:1.370037
[Train] epoch:245	batch id:280	 lr:0.020694 loss:1.329738
[Train] epoch:245	batch id:290	 lr:0.020694 loss:1.402015
[Train] epoch:245	batch id:300	 lr:0.020694 loss:1.419085
[Train] 245, loss: 1.399783, train acc: 0.974654, 
[Test] epoch:245	batch id:0	 loss:1.264535
[Test] epoch:245	batch id:10	 loss:1.417981
[Test] epoch:245	batch id:20	 loss:1.465246
[Test] epoch:245	batch id:30	 loss:1.407244
[Test] epoch:245	batch id:40	 loss:1.480193
[Test] epoch:245	batch id:50	 loss:1.470523
[Test] epoch:245	batch id:60	 loss:1.256447
[Test] epoch:245	batch id:70	 loss:1.407597
[Test] epoch:245	batch id:80	 loss:1.678738
[Test] epoch:245	batch id:90	 loss:1.498640
[Test] epoch:245	batch id:100	 loss:1.884086
[Test] epoch:245	batch id:110	 loss:1.329099
[Test] epoch:245	batch id:120	 loss:1.320696
[Test] epoch:245	batch id:130	 loss:1.369239
[Test] epoch:245	batch id:140	 loss:1.354987
[Test] epoch:245	batch id:150	 loss:1.700202
[Test] 245, loss: 1.470329, test acc: 0.915721,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:246	batch id:0	 lr:0.020341 loss:1.335651
[Train] epoch:246	batch id:10	 lr:0.020341 loss:1.377217
[Train] epoch:246	batch id:20	 lr:0.020341 loss:1.362813
[Train] epoch:246	batch id:30	 lr:0.020341 loss:1.378421
[Train] epoch:246	batch id:40	 lr:0.020341 loss:1.392221
[Train] epoch:246	batch id:50	 lr:0.020341 loss:1.480391
[Train] epoch:246	batch id:60	 lr:0.020341 loss:1.394929
[Train] epoch:246	batch id:70	 lr:0.020341 loss:1.428984
[Train] epoch:246	batch id:80	 lr:0.020341 loss:1.365001
[Train] epoch:246	batch id:90	 lr:0.020341 loss:1.347322
[Train] epoch:246	batch id:100	 lr:0.020341 loss:1.357127
[Train] epoch:246	batch id:110	 lr:0.020341 loss:1.392452
[Train] epoch:246	batch id:120	 lr:0.020341 loss:1.397820
[Train] epoch:246	batch id:130	 lr:0.020341 loss:1.385950
[Train] epoch:246	batch id:140	 lr:0.020341 loss:1.391845
[Train] epoch:246	batch id:150	 lr:0.020341 loss:1.437249
[Train] epoch:246	batch id:160	 lr:0.020341 loss:1.354813
[Train] epoch:246	batch id:170	 lr:0.020341 loss:1.407469
[Train] epoch:246	batch id:180	 lr:0.020341 loss:1.368650
[Train] epoch:246	batch id:190	 lr:0.020341 loss:1.368694
[Train] epoch:246	batch id:200	 lr:0.020341 loss:1.367865
[Train] epoch:246	batch id:210	 lr:0.020341 loss:1.450951
[Train] epoch:246	batch id:220	 lr:0.020341 loss:1.383481
[Train] epoch:246	batch id:230	 lr:0.020341 loss:1.408217
[Train] epoch:246	batch id:240	 lr:0.020341 loss:1.410753
[Train] epoch:246	batch id:250	 lr:0.020341 loss:1.446521
[Train] epoch:246	batch id:260	 lr:0.020341 loss:1.416118
[Train] epoch:246	batch id:270	 lr:0.020341 loss:1.414283
[Train] epoch:246	batch id:280	 lr:0.020341 loss:1.373027
[Train] epoch:246	batch id:290	 lr:0.020341 loss:1.482237
[Train] epoch:246	batch id:300	 lr:0.020341 loss:1.489969
[Train] 246, loss: 1.398485, train acc: 0.976486, 
[Test] epoch:246	batch id:0	 loss:1.336012
[Test] epoch:246	batch id:10	 loss:1.445238
[Test] epoch:246	batch id:20	 loss:1.439059
[Test] epoch:246	batch id:30	 loss:1.424968
[Test] epoch:246	batch id:40	 loss:1.401605
[Test] epoch:246	batch id:50	 loss:1.547978
[Test] epoch:246	batch id:60	 loss:1.285341
[Test] epoch:246	batch id:70	 loss:1.365137
[Test] epoch:246	batch id:80	 loss:1.667734
[Test] epoch:246	batch id:90	 loss:1.681267
[Test] epoch:246	batch id:100	 loss:1.769764
[Test] epoch:246	batch id:110	 loss:1.424793
[Test] epoch:246	batch id:120	 loss:1.344072
[Test] epoch:246	batch id:130	 loss:1.437563
[Test] epoch:246	batch id:140	 loss:1.331132
[Test] epoch:246	batch id:150	 loss:1.803169
[Test] 246, loss: 1.500680, test acc: 0.903566,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:247	batch id:0	 lr:0.019990 loss:1.345478
[Train] epoch:247	batch id:10	 lr:0.019990 loss:1.402358
[Train] epoch:247	batch id:20	 lr:0.019990 loss:1.347813
[Train] epoch:247	batch id:30	 lr:0.019990 loss:1.401099
[Train] epoch:247	batch id:40	 lr:0.019990 loss:1.462425
[Train] epoch:247	batch id:50	 lr:0.019990 loss:1.372238
[Train] epoch:247	batch id:60	 lr:0.019990 loss:1.361102
[Train] epoch:247	batch id:70	 lr:0.019990 loss:1.467616
[Train] epoch:247	batch id:80	 lr:0.019990 loss:1.377795
[Train] epoch:247	batch id:90	 lr:0.019990 loss:1.366905
[Train] epoch:247	batch id:100	 lr:0.019990 loss:1.354526
[Train] epoch:247	batch id:110	 lr:0.019990 loss:1.435265
[Train] epoch:247	batch id:120	 lr:0.019990 loss:1.412928
[Train] epoch:247	batch id:130	 lr:0.019990 loss:1.402427
[Train] epoch:247	batch id:140	 lr:0.019990 loss:1.452334
[Train] epoch:247	batch id:150	 lr:0.019990 loss:1.356736
[Train] epoch:247	batch id:160	 lr:0.019990 loss:1.386733
[Train] epoch:247	batch id:170	 lr:0.019990 loss:1.361819
[Train] epoch:247	batch id:180	 lr:0.019990 loss:1.405558
[Train] epoch:247	batch id:190	 lr:0.019990 loss:1.339384
[Train] epoch:247	batch id:200	 lr:0.019990 loss:1.431279
[Train] epoch:247	batch id:210	 lr:0.019990 loss:1.458926
[Train] epoch:247	batch id:220	 lr:0.019990 loss:1.391437
[Train] epoch:247	batch id:230	 lr:0.019990 loss:1.408542
[Train] epoch:247	batch id:240	 lr:0.019990 loss:1.365235
[Train] epoch:247	batch id:250	 lr:0.019990 loss:1.364388
[Train] epoch:247	batch id:260	 lr:0.019990 loss:1.367050
[Train] epoch:247	batch id:270	 lr:0.019990 loss:1.388545
[Train] epoch:247	batch id:280	 lr:0.019990 loss:1.351057
[Train] epoch:247	batch id:290	 lr:0.019990 loss:1.395962
[Train] epoch:247	batch id:300	 lr:0.019990 loss:1.446165
[Train] 247, loss: 1.396507, train acc: 0.974145, 
[Test] epoch:247	batch id:0	 loss:1.282535
[Test] epoch:247	batch id:10	 loss:1.432129
[Test] epoch:247	batch id:20	 loss:1.486200
[Test] epoch:247	batch id:30	 loss:1.462301
[Test] epoch:247	batch id:40	 loss:1.469415
[Test] epoch:247	batch id:50	 loss:1.485059
[Test] epoch:247	batch id:60	 loss:1.254140
[Test] epoch:247	batch id:70	 loss:1.434057
[Test] epoch:247	batch id:80	 loss:1.572705
[Test] epoch:247	batch id:90	 loss:1.494646
[Test] epoch:247	batch id:100	 loss:2.078415
[Test] epoch:247	batch id:110	 loss:1.327145
[Test] epoch:247	batch id:120	 loss:1.297333
[Test] epoch:247	batch id:130	 loss:1.302542
[Test] epoch:247	batch id:140	 loss:1.309251
[Test] epoch:247	batch id:150	 loss:1.730909
[Test] 247, loss: 1.463095, test acc: 0.914100,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:248	batch id:0	 lr:0.019641 loss:1.345773
[Train] epoch:248	batch id:10	 lr:0.019641 loss:1.366562
[Train] epoch:248	batch id:20	 lr:0.019641 loss:1.429985
[Train] epoch:248	batch id:30	 lr:0.019641 loss:1.369891
[Train] epoch:248	batch id:40	 lr:0.019641 loss:1.327551
[Train] epoch:248	batch id:50	 lr:0.019641 loss:1.375706
[Train] epoch:248	batch id:60	 lr:0.019641 loss:1.409409
[Train] epoch:248	batch id:70	 lr:0.019641 loss:1.362268
[Train] epoch:248	batch id:80	 lr:0.019641 loss:1.443191
[Train] epoch:248	batch id:90	 lr:0.019641 loss:1.335636
[Train] epoch:248	batch id:100	 lr:0.019641 loss:1.356501
[Train] epoch:248	batch id:110	 lr:0.019641 loss:1.320213
[Train] epoch:248	batch id:120	 lr:0.019641 loss:1.334219
[Train] epoch:248	batch id:130	 lr:0.019641 loss:1.410110
[Train] epoch:248	batch id:140	 lr:0.019641 loss:1.368203
[Train] epoch:248	batch id:150	 lr:0.019641 loss:1.544820
[Train] epoch:248	batch id:160	 lr:0.019641 loss:1.360123
[Train] epoch:248	batch id:170	 lr:0.019641 loss:1.401439
[Train] epoch:248	batch id:180	 lr:0.019641 loss:1.379242
[Train] epoch:248	batch id:190	 lr:0.019641 loss:1.359316
[Train] epoch:248	batch id:200	 lr:0.019641 loss:1.429303
[Train] epoch:248	batch id:210	 lr:0.019641 loss:1.346158
[Train] epoch:248	batch id:220	 lr:0.019641 loss:1.421833
[Train] epoch:248	batch id:230	 lr:0.019641 loss:1.443571
[Train] epoch:248	batch id:240	 lr:0.019641 loss:1.349117
[Train] epoch:248	batch id:250	 lr:0.019641 loss:1.360771
[Train] epoch:248	batch id:260	 lr:0.019641 loss:1.401244
[Train] epoch:248	batch id:270	 lr:0.019641 loss:1.363471
[Train] epoch:248	batch id:280	 lr:0.019641 loss:1.421844
[Train] epoch:248	batch id:290	 lr:0.019641 loss:1.361731
[Train] epoch:248	batch id:300	 lr:0.019641 loss:1.340116
[Train] 248, loss: 1.399234, train acc: 0.976181, 
[Test] epoch:248	batch id:0	 loss:1.295024
[Test] epoch:248	batch id:10	 loss:1.458888
[Test] epoch:248	batch id:20	 loss:1.437787
[Test] epoch:248	batch id:30	 loss:1.476022
[Test] epoch:248	batch id:40	 loss:1.409053
[Test] epoch:248	batch id:50	 loss:1.471137
[Test] epoch:248	batch id:60	 loss:1.263562
[Test] epoch:248	batch id:70	 loss:1.439705
[Test] epoch:248	batch id:80	 loss:1.701866
[Test] epoch:248	batch id:90	 loss:1.559134
[Test] epoch:248	batch id:100	 loss:2.021900
[Test] epoch:248	batch id:110	 loss:1.384652
[Test] epoch:248	batch id:120	 loss:1.322187
[Test] epoch:248	batch id:130	 loss:1.373578
[Test] epoch:248	batch id:140	 loss:1.364190
[Test] epoch:248	batch id:150	 loss:1.950160
[Test] 248, loss: 1.482777, test acc: 0.912480,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:249	batch id:0	 lr:0.019295 loss:1.343228
[Train] epoch:249	batch id:10	 lr:0.019295 loss:1.387786
[Train] epoch:249	batch id:20	 lr:0.019295 loss:1.400967
[Train] epoch:249	batch id:30	 lr:0.019295 loss:1.340897
[Train] epoch:249	batch id:40	 lr:0.019295 loss:1.343514
[Train] epoch:249	batch id:50	 lr:0.019295 loss:1.368615
[Train] epoch:249	batch id:60	 lr:0.019295 loss:1.332289
[Train] epoch:249	batch id:70	 lr:0.019295 loss:1.372889
[Train] epoch:249	batch id:80	 lr:0.019295 loss:1.430927
[Train] epoch:249	batch id:90	 lr:0.019295 loss:1.344087
[Train] epoch:249	batch id:100	 lr:0.019295 loss:1.368443
[Train] epoch:249	batch id:110	 lr:0.019295 loss:1.468651
[Train] epoch:249	batch id:120	 lr:0.019295 loss:1.433801
[Train] epoch:249	batch id:130	 lr:0.019295 loss:1.402506
[Train] epoch:249	batch id:140	 lr:0.019295 loss:1.369479
[Train] epoch:249	batch id:150	 lr:0.019295 loss:1.397107
[Train] epoch:249	batch id:160	 lr:0.019295 loss:1.390616
[Train] epoch:249	batch id:170	 lr:0.019295 loss:1.413804
[Train] epoch:249	batch id:180	 lr:0.019295 loss:1.441391
[Train] epoch:249	batch id:190	 lr:0.019295 loss:1.530278
[Train] epoch:249	batch id:200	 lr:0.019295 loss:1.502731
[Train] epoch:249	batch id:210	 lr:0.019295 loss:1.512377
[Train] epoch:249	batch id:220	 lr:0.019295 loss:1.456178
[Train] epoch:249	batch id:230	 lr:0.019295 loss:1.379793
[Train] epoch:249	batch id:240	 lr:0.019295 loss:1.400308
[Train] epoch:249	batch id:250	 lr:0.019295 loss:1.438151
[Train] epoch:249	batch id:260	 lr:0.019295 loss:1.483948
[Train] epoch:249	batch id:270	 lr:0.019295 loss:1.443012
[Train] epoch:249	batch id:280	 lr:0.019295 loss:1.520374
[Train] epoch:249	batch id:290	 lr:0.019295 loss:1.449010
[Train] epoch:249	batch id:300	 lr:0.019295 loss:1.403564
[Train] 249, loss: 1.400682, train acc: 0.976588, 
[Test] epoch:249	batch id:0	 loss:1.270656
[Test] epoch:249	batch id:10	 loss:1.460987
[Test] epoch:249	batch id:20	 loss:1.406117
[Test] epoch:249	batch id:30	 loss:1.464513
[Test] epoch:249	batch id:40	 loss:1.456492
[Test] epoch:249	batch id:50	 loss:1.519949
[Test] epoch:249	batch id:60	 loss:1.272036
[Test] epoch:249	batch id:70	 loss:1.540878
[Test] epoch:249	batch id:80	 loss:1.597720
[Test] epoch:249	batch id:90	 loss:1.571831
[Test] epoch:249	batch id:100	 loss:1.855174
[Test] epoch:249	batch id:110	 loss:1.365726
[Test] epoch:249	batch id:120	 loss:1.344991
[Test] epoch:249	batch id:130	 loss:1.318397
[Test] epoch:249	batch id:140	 loss:1.362321
[Test] epoch:249	batch id:150	 loss:1.668367
[Test] 249, loss: 1.469220, test acc: 0.913290,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:250	batch id:0	 lr:0.018951 loss:1.368870
[Train] epoch:250	batch id:10	 lr:0.018951 loss:1.405527
[Train] epoch:250	batch id:20	 lr:0.018951 loss:1.384810
[Train] epoch:250	batch id:30	 lr:0.018951 loss:1.392822
[Train] epoch:250	batch id:40	 lr:0.018951 loss:1.396671
[Train] epoch:250	batch id:50	 lr:0.018951 loss:1.367288
[Train] epoch:250	batch id:60	 lr:0.018951 loss:1.467978
[Train] epoch:250	batch id:70	 lr:0.018951 loss:1.487822
[Train] epoch:250	batch id:80	 lr:0.018951 loss:1.366556
[Train] epoch:250	batch id:90	 lr:0.018951 loss:1.405467
[Train] epoch:250	batch id:100	 lr:0.018951 loss:1.441843
[Train] epoch:250	batch id:110	 lr:0.018951 loss:1.455789
[Train] epoch:250	batch id:120	 lr:0.018951 loss:1.423391
[Train] epoch:250	batch id:130	 lr:0.018951 loss:1.357579
[Train] epoch:250	batch id:140	 lr:0.018951 loss:1.461769
[Train] epoch:250	batch id:150	 lr:0.018951 loss:1.326516
[Train] epoch:250	batch id:160	 lr:0.018951 loss:1.349710
[Train] epoch:250	batch id:170	 lr:0.018951 loss:1.415266
[Train] epoch:250	batch id:180	 lr:0.018951 loss:1.335038
[Train] epoch:250	batch id:190	 lr:0.018951 loss:1.430957
[Train] epoch:250	batch id:200	 lr:0.018951 loss:1.440064
[Train] epoch:250	batch id:210	 lr:0.018951 loss:1.388875
[Train] epoch:250	batch id:220	 lr:0.018951 loss:1.364119
[Train] epoch:250	batch id:230	 lr:0.018951 loss:1.526878
[Train] epoch:250	batch id:240	 lr:0.018951 loss:1.365858
[Train] epoch:250	batch id:250	 lr:0.018951 loss:1.392114
[Train] epoch:250	batch id:260	 lr:0.018951 loss:1.435814
[Train] epoch:250	batch id:270	 lr:0.018951 loss:1.390218
[Train] epoch:250	batch id:280	 lr:0.018951 loss:1.469419
[Train] epoch:250	batch id:290	 lr:0.018951 loss:1.501550
[Train] epoch:250	batch id:300	 lr:0.018951 loss:1.320284
[Train] 250, loss: 1.399414, train acc: 0.974043, 
[Test] epoch:250	batch id:0	 loss:1.278865
[Test] epoch:250	batch id:10	 loss:1.516760
[Test] epoch:250	batch id:20	 loss:1.463815
[Test] epoch:250	batch id:30	 loss:1.482100
[Test] epoch:250	batch id:40	 loss:1.454220
[Test] epoch:250	batch id:50	 loss:1.541791
[Test] epoch:250	batch id:60	 loss:1.271751
[Test] epoch:250	batch id:70	 loss:1.339493
[Test] epoch:250	batch id:80	 loss:1.644024
[Test] epoch:250	batch id:90	 loss:1.587536
[Test] epoch:250	batch id:100	 loss:1.983392
[Test] epoch:250	batch id:110	 loss:1.366635
[Test] epoch:250	batch id:120	 loss:1.350030
[Test] epoch:250	batch id:130	 loss:1.358259
[Test] epoch:250	batch id:140	 loss:1.360495
[Test] epoch:250	batch id:150	 loss:1.789645
[Test] 250, loss: 1.487916, test acc: 0.910454,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:251	batch id:0	 lr:0.018610 loss:1.401785
[Train] epoch:251	batch id:10	 lr:0.018610 loss:1.362642
[Train] epoch:251	batch id:20	 lr:0.018610 loss:1.343487
[Train] epoch:251	batch id:30	 lr:0.018610 loss:1.377956
[Train] epoch:251	batch id:40	 lr:0.018610 loss:1.411679
[Train] epoch:251	batch id:50	 lr:0.018610 loss:1.420719
[Train] epoch:251	batch id:60	 lr:0.018610 loss:1.362434
[Train] epoch:251	batch id:70	 lr:0.018610 loss:1.359496
[Train] epoch:251	batch id:80	 lr:0.018610 loss:1.390262
[Train] epoch:251	batch id:90	 lr:0.018610 loss:1.501097
[Train] epoch:251	batch id:100	 lr:0.018610 loss:1.400907
[Train] epoch:251	batch id:110	 lr:0.018610 loss:1.449975
[Train] epoch:251	batch id:120	 lr:0.018610 loss:1.357633
[Train] epoch:251	batch id:130	 lr:0.018610 loss:1.488005
[Train] epoch:251	batch id:140	 lr:0.018610 loss:1.370516
[Train] epoch:251	batch id:150	 lr:0.018610 loss:1.397934
[Train] epoch:251	batch id:160	 lr:0.018610 loss:1.402521
[Train] epoch:251	batch id:170	 lr:0.018610 loss:1.397636
[Train] epoch:251	batch id:180	 lr:0.018610 loss:1.340309
[Train] epoch:251	batch id:190	 lr:0.018610 loss:1.343811
[Train] epoch:251	batch id:200	 lr:0.018610 loss:1.360652
[Train] epoch:251	batch id:210	 lr:0.018610 loss:1.357733
[Train] epoch:251	batch id:220	 lr:0.018610 loss:1.463568
[Train] epoch:251	batch id:230	 lr:0.018610 loss:1.423335
[Train] epoch:251	batch id:240	 lr:0.018610 loss:1.338354
[Train] epoch:251	batch id:250	 lr:0.018610 loss:1.396527
[Train] epoch:251	batch id:260	 lr:0.018610 loss:1.399371
[Train] epoch:251	batch id:270	 lr:0.018610 loss:1.335968
[Train] epoch:251	batch id:280	 lr:0.018610 loss:1.391256
[Train] epoch:251	batch id:290	 lr:0.018610 loss:1.444146
[Train] epoch:251	batch id:300	 lr:0.018610 loss:1.439884
[Train] 251, loss: 1.396448, train acc: 0.976690, 
[Test] epoch:251	batch id:0	 loss:1.285578
[Test] epoch:251	batch id:10	 loss:1.469757
[Test] epoch:251	batch id:20	 loss:1.477741
[Test] epoch:251	batch id:30	 loss:1.500876
[Test] epoch:251	batch id:40	 loss:1.583210
[Test] epoch:251	batch id:50	 loss:1.454632
[Test] epoch:251	batch id:60	 loss:1.283553
[Test] epoch:251	batch id:70	 loss:1.356135
[Test] epoch:251	batch id:80	 loss:1.675458
[Test] epoch:251	batch id:90	 loss:1.584701
[Test] epoch:251	batch id:100	 loss:2.105382
[Test] epoch:251	batch id:110	 loss:1.295611
[Test] epoch:251	batch id:120	 loss:1.370556
[Test] epoch:251	batch id:130	 loss:1.306593
[Test] epoch:251	batch id:140	 loss:1.342335
[Test] epoch:251	batch id:150	 loss:1.810914
[Test] 251, loss: 1.481638, test acc: 0.913695,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:252	batch id:0	 lr:0.018272 loss:1.343114
[Train] epoch:252	batch id:10	 lr:0.018272 loss:1.357271
[Train] epoch:252	batch id:20	 lr:0.018272 loss:1.326645
[Train] epoch:252	batch id:30	 lr:0.018272 loss:1.433760
[Train] epoch:252	batch id:40	 lr:0.018272 loss:1.354777
[Train] epoch:252	batch id:50	 lr:0.018272 loss:1.396551
[Train] epoch:252	batch id:60	 lr:0.018272 loss:1.426785
[Train] epoch:252	batch id:70	 lr:0.018272 loss:1.344708
[Train] epoch:252	batch id:80	 lr:0.018272 loss:1.364440
[Train] epoch:252	batch id:90	 lr:0.018272 loss:1.458326
[Train] epoch:252	batch id:100	 lr:0.018272 loss:1.340333
[Train] epoch:252	batch id:110	 lr:0.018272 loss:1.367821
[Train] epoch:252	batch id:120	 lr:0.018272 loss:1.381049
[Train] epoch:252	batch id:130	 lr:0.018272 loss:1.460478
[Train] epoch:252	batch id:140	 lr:0.018272 loss:1.433134
[Train] epoch:252	batch id:150	 lr:0.018272 loss:1.344486
[Train] epoch:252	batch id:160	 lr:0.018272 loss:1.345760
[Train] epoch:252	batch id:170	 lr:0.018272 loss:1.457967
[Train] epoch:252	batch id:180	 lr:0.018272 loss:1.479015
[Train] epoch:252	batch id:190	 lr:0.018272 loss:1.367915
[Train] epoch:252	batch id:200	 lr:0.018272 loss:1.383849
[Train] epoch:252	batch id:210	 lr:0.018272 loss:1.485097
[Train] epoch:252	batch id:220	 lr:0.018272 loss:1.377777
[Train] epoch:252	batch id:230	 lr:0.018272 loss:1.449958
[Train] epoch:252	batch id:240	 lr:0.018272 loss:1.367010
[Train] epoch:252	batch id:250	 lr:0.018272 loss:1.365615
[Train] epoch:252	batch id:260	 lr:0.018272 loss:1.309797
[Train] epoch:252	batch id:270	 lr:0.018272 loss:1.399661
[Train] epoch:252	batch id:280	 lr:0.018272 loss:1.590019
[Train] epoch:252	batch id:290	 lr:0.018272 loss:1.402851
[Train] epoch:252	batch id:300	 lr:0.018272 loss:1.362407
[Train] 252, loss: 1.389574, train acc: 0.978420, 
[Test] epoch:252	batch id:0	 loss:1.268583
[Test] epoch:252	batch id:10	 loss:1.353318
[Test] epoch:252	batch id:20	 loss:1.447123
[Test] epoch:252	batch id:30	 loss:1.486125
[Test] epoch:252	batch id:40	 loss:1.475701
[Test] epoch:252	batch id:50	 loss:1.500613
[Test] epoch:252	batch id:60	 loss:1.273033
[Test] epoch:252	batch id:70	 loss:1.540829
[Test] epoch:252	batch id:80	 loss:1.701341
[Test] epoch:252	batch id:90	 loss:1.518250
[Test] epoch:252	batch id:100	 loss:1.824263
[Test] epoch:252	batch id:110	 loss:1.396624
[Test] epoch:252	batch id:120	 loss:1.339739
[Test] epoch:252	batch id:130	 loss:1.335917
[Test] epoch:252	batch id:140	 loss:1.362519
[Test] epoch:252	batch id:150	 loss:1.899663
[Test] 252, loss: 1.475959, test acc: 0.915316,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:253	batch id:0	 lr:0.017936 loss:1.351545
[Train] epoch:253	batch id:10	 lr:0.017936 loss:1.351384
[Train] epoch:253	batch id:20	 lr:0.017936 loss:1.377629
[Train] epoch:253	batch id:30	 lr:0.017936 loss:1.432461
[Train] epoch:253	batch id:40	 lr:0.017936 loss:1.480629
[Train] epoch:253	batch id:50	 lr:0.017936 loss:1.394466
[Train] epoch:253	batch id:60	 lr:0.017936 loss:1.377124
[Train] epoch:253	batch id:70	 lr:0.017936 loss:1.366348
[Train] epoch:253	batch id:80	 lr:0.017936 loss:1.429579
[Train] epoch:253	batch id:90	 lr:0.017936 loss:1.395384
[Train] epoch:253	batch id:100	 lr:0.017936 loss:1.382993
[Train] epoch:253	batch id:110	 lr:0.017936 loss:1.354215
[Train] epoch:253	batch id:120	 lr:0.017936 loss:1.414943
[Train] epoch:253	batch id:130	 lr:0.017936 loss:1.368816
[Train] epoch:253	batch id:140	 lr:0.017936 loss:1.425665
[Train] epoch:253	batch id:150	 lr:0.017936 loss:1.495177
[Train] epoch:253	batch id:160	 lr:0.017936 loss:1.342644
[Train] epoch:253	batch id:170	 lr:0.017936 loss:1.445114
[Train] epoch:253	batch id:180	 lr:0.017936 loss:1.369528
[Train] epoch:253	batch id:190	 lr:0.017936 loss:1.401198
[Train] epoch:253	batch id:200	 lr:0.017936 loss:1.400920
[Train] epoch:253	batch id:210	 lr:0.017936 loss:1.415161
[Train] epoch:253	batch id:220	 lr:0.017936 loss:1.358286
[Train] epoch:253	batch id:230	 lr:0.017936 loss:1.364681
[Train] epoch:253	batch id:240	 lr:0.017936 loss:1.347160
[Train] epoch:253	batch id:250	 lr:0.017936 loss:1.347265
[Train] epoch:253	batch id:260	 lr:0.017936 loss:1.387356
[Train] epoch:253	batch id:270	 lr:0.017936 loss:1.323980
[Train] epoch:253	batch id:280	 lr:0.017936 loss:1.527921
[Train] epoch:253	batch id:290	 lr:0.017936 loss:1.329060
[Train] epoch:253	batch id:300	 lr:0.017936 loss:1.375499
[Train] 253, loss: 1.387520, train acc: 0.979336, 
[Test] epoch:253	batch id:0	 loss:1.265200
[Test] epoch:253	batch id:10	 loss:1.483060
[Test] epoch:253	batch id:20	 loss:1.453482
[Test] epoch:253	batch id:30	 loss:1.432124
[Test] epoch:253	batch id:40	 loss:1.337917
[Test] epoch:253	batch id:50	 loss:1.478171
[Test] epoch:253	batch id:60	 loss:1.250522
[Test] epoch:253	batch id:70	 loss:1.561621
[Test] epoch:253	batch id:80	 loss:1.611112
[Test] epoch:253	batch id:90	 loss:1.585631
[Test] epoch:253	batch id:100	 loss:1.873735
[Test] epoch:253	batch id:110	 loss:1.351862
[Test] epoch:253	batch id:120	 loss:1.310926
[Test] epoch:253	batch id:130	 loss:1.283488
[Test] epoch:253	batch id:140	 loss:1.354652
[Test] epoch:253	batch id:150	 loss:1.740220
[Test] 253, loss: 1.462100, test acc: 0.914911,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:254	batch id:0	 lr:0.017603 loss:1.374264
[Train] epoch:254	batch id:10	 lr:0.017603 loss:1.340811
[Train] epoch:254	batch id:20	 lr:0.017603 loss:1.370239
[Train] epoch:254	batch id:30	 lr:0.017603 loss:1.462061
[Train] epoch:254	batch id:40	 lr:0.017603 loss:1.359174
[Train] epoch:254	batch id:50	 lr:0.017603 loss:1.371060
[Train] epoch:254	batch id:60	 lr:0.017603 loss:1.363646
[Train] epoch:254	batch id:70	 lr:0.017603 loss:1.382111
[Train] epoch:254	batch id:80	 lr:0.017603 loss:1.394582
[Train] epoch:254	batch id:90	 lr:0.017603 loss:1.318940
[Train] epoch:254	batch id:100	 lr:0.017603 loss:1.348560
[Train] epoch:254	batch id:110	 lr:0.017603 loss:1.345853
[Train] epoch:254	batch id:120	 lr:0.017603 loss:1.405630
[Train] epoch:254	batch id:130	 lr:0.017603 loss:1.347854
[Train] epoch:254	batch id:140	 lr:0.017603 loss:1.369269
[Train] epoch:254	batch id:150	 lr:0.017603 loss:1.483370
[Train] epoch:254	batch id:160	 lr:0.017603 loss:1.402756
[Train] epoch:254	batch id:170	 lr:0.017603 loss:1.367350
[Train] epoch:254	batch id:180	 lr:0.017603 loss:1.454349
[Train] epoch:254	batch id:190	 lr:0.017603 loss:1.424142
[Train] epoch:254	batch id:200	 lr:0.017603 loss:1.376603
[Train] epoch:254	batch id:210	 lr:0.017603 loss:1.346382
[Train] epoch:254	batch id:220	 lr:0.017603 loss:1.338400
[Train] epoch:254	batch id:230	 lr:0.017603 loss:1.429811
[Train] epoch:254	batch id:240	 lr:0.017603 loss:1.486368
[Train] epoch:254	batch id:250	 lr:0.017603 loss:1.323652
[Train] epoch:254	batch id:260	 lr:0.017603 loss:1.336338
[Train] epoch:254	batch id:270	 lr:0.017603 loss:1.365323
[Train] epoch:254	batch id:280	 lr:0.017603 loss:1.355222
[Train] epoch:254	batch id:290	 lr:0.017603 loss:1.385434
[Train] epoch:254	batch id:300	 lr:0.017603 loss:1.345719
[Train] 254, loss: 1.386327, train acc: 0.978726, 
[Test] epoch:254	batch id:0	 loss:1.289612
[Test] epoch:254	batch id:10	 loss:1.467051
[Test] epoch:254	batch id:20	 loss:1.565693
[Test] epoch:254	batch id:30	 loss:1.384509
[Test] epoch:254	batch id:40	 loss:1.509889
[Test] epoch:254	batch id:50	 loss:1.477953
[Test] epoch:254	batch id:60	 loss:1.271748
[Test] epoch:254	batch id:70	 loss:1.521910
[Test] epoch:254	batch id:80	 loss:1.627615
[Test] epoch:254	batch id:90	 loss:1.609385
[Test] epoch:254	batch id:100	 loss:1.747789
[Test] epoch:254	batch id:110	 loss:1.436916
[Test] epoch:254	batch id:120	 loss:1.339242
[Test] epoch:254	batch id:130	 loss:1.334097
[Test] epoch:254	batch id:140	 loss:1.313725
[Test] epoch:254	batch id:150	 loss:1.765486
[Test] 254, loss: 1.489224, test acc: 0.908023,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:255	batch id:0	 lr:0.017272 loss:1.366426
[Train] epoch:255	batch id:10	 lr:0.017272 loss:1.337422
[Train] epoch:255	batch id:20	 lr:0.017272 loss:1.381505
[Train] epoch:255	batch id:30	 lr:0.017272 loss:1.397461
[Train] epoch:255	batch id:40	 lr:0.017272 loss:1.444042
[Train] epoch:255	batch id:50	 lr:0.017272 loss:1.366908
[Train] epoch:255	batch id:60	 lr:0.017272 loss:1.374435
[Train] epoch:255	batch id:70	 lr:0.017272 loss:1.354870
[Train] epoch:255	batch id:80	 lr:0.017272 loss:1.302607
[Train] epoch:255	batch id:90	 lr:0.017272 loss:1.341174
[Train] epoch:255	batch id:100	 lr:0.017272 loss:1.590853
[Train] epoch:255	batch id:110	 lr:0.017272 loss:1.487418
[Train] epoch:255	batch id:120	 lr:0.017272 loss:1.486848
[Train] epoch:255	batch id:130	 lr:0.017272 loss:1.450256
[Train] epoch:255	batch id:140	 lr:0.017272 loss:1.374340
[Train] epoch:255	batch id:150	 lr:0.017272 loss:1.403675
[Train] epoch:255	batch id:160	 lr:0.017272 loss:1.350812
[Train] epoch:255	batch id:170	 lr:0.017272 loss:1.359706
[Train] epoch:255	batch id:180	 lr:0.017272 loss:1.371109
[Train] epoch:255	batch id:190	 lr:0.017272 loss:1.418437
[Train] epoch:255	batch id:200	 lr:0.017272 loss:1.338620
[Train] epoch:255	batch id:210	 lr:0.017272 loss:1.336784
[Train] epoch:255	batch id:220	 lr:0.017272 loss:1.414523
[Train] epoch:255	batch id:230	 lr:0.017272 loss:1.355796
[Train] epoch:255	batch id:240	 lr:0.017272 loss:1.429273
[Train] epoch:255	batch id:250	 lr:0.017272 loss:1.383208
[Train] epoch:255	batch id:260	 lr:0.017272 loss:1.380704
[Train] epoch:255	batch id:270	 lr:0.017272 loss:1.396657
[Train] epoch:255	batch id:280	 lr:0.017272 loss:1.399715
[Train] epoch:255	batch id:290	 lr:0.017272 loss:1.448791
[Train] epoch:255	batch id:300	 lr:0.017272 loss:1.364110
[Train] 255, loss: 1.389232, train acc: 0.980965, 
[Test] epoch:255	batch id:0	 loss:1.299637
[Test] epoch:255	batch id:10	 loss:1.505969
[Test] epoch:255	batch id:20	 loss:1.513788
[Test] epoch:255	batch id:30	 loss:1.372533
[Test] epoch:255	batch id:40	 loss:1.465182
[Test] epoch:255	batch id:50	 loss:1.502550
[Test] epoch:255	batch id:60	 loss:1.279220
[Test] epoch:255	batch id:70	 loss:1.344372
[Test] epoch:255	batch id:80	 loss:1.718200
[Test] epoch:255	batch id:90	 loss:1.690826
[Test] epoch:255	batch id:100	 loss:2.143239
[Test] epoch:255	batch id:110	 loss:1.362835
[Test] epoch:255	batch id:120	 loss:1.346645
[Test] epoch:255	batch id:130	 loss:1.312426
[Test] epoch:255	batch id:140	 loss:1.407045
[Test] epoch:255	batch id:150	 loss:1.841941
[Test] 255, loss: 1.489765, test acc: 0.914911,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:256	batch id:0	 lr:0.016944 loss:1.349486
[Train] epoch:256	batch id:10	 lr:0.016944 loss:1.372722
[Train] epoch:256	batch id:20	 lr:0.016944 loss:1.364086
[Train] epoch:256	batch id:30	 lr:0.016944 loss:1.405463
[Train] epoch:256	batch id:40	 lr:0.016944 loss:1.428645
[Train] epoch:256	batch id:50	 lr:0.016944 loss:1.366417
[Train] epoch:256	batch id:60	 lr:0.016944 loss:1.362074
[Train] epoch:256	batch id:70	 lr:0.016944 loss:1.363666
[Train] epoch:256	batch id:80	 lr:0.016944 loss:1.426873
[Train] epoch:256	batch id:90	 lr:0.016944 loss:1.372039
[Train] epoch:256	batch id:100	 lr:0.016944 loss:1.426296
[Train] epoch:256	batch id:110	 lr:0.016944 loss:1.395848
[Train] epoch:256	batch id:120	 lr:0.016944 loss:1.412732
[Train] epoch:256	batch id:130	 lr:0.016944 loss:1.384694
[Train] epoch:256	batch id:140	 lr:0.016944 loss:1.344130
[Train] epoch:256	batch id:150	 lr:0.016944 loss:1.367912
[Train] epoch:256	batch id:160	 lr:0.016944 loss:1.401127
[Train] epoch:256	batch id:170	 lr:0.016944 loss:1.341507
[Train] epoch:256	batch id:180	 lr:0.016944 loss:1.366088
[Train] epoch:256	batch id:190	 lr:0.016944 loss:1.353144
[Train] epoch:256	batch id:200	 lr:0.016944 loss:1.386024
[Train] epoch:256	batch id:210	 lr:0.016944 loss:1.374458
[Train] epoch:256	batch id:220	 lr:0.016944 loss:1.362724
[Train] epoch:256	batch id:230	 lr:0.016944 loss:1.400748
[Train] epoch:256	batch id:240	 lr:0.016944 loss:1.531675
[Train] epoch:256	batch id:250	 lr:0.016944 loss:1.403108
[Train] epoch:256	batch id:260	 lr:0.016944 loss:1.381838
[Train] epoch:256	batch id:270	 lr:0.016944 loss:1.399435
[Train] epoch:256	batch id:280	 lr:0.016944 loss:1.333782
[Train] epoch:256	batch id:290	 lr:0.016944 loss:1.423142
[Train] epoch:256	batch id:300	 lr:0.016944 loss:1.389178
[Train] 256, loss: 1.385336, train acc: 0.980049, 
[Test] epoch:256	batch id:0	 loss:1.327455
[Test] epoch:256	batch id:10	 loss:1.419760
[Test] epoch:256	batch id:20	 loss:1.481546
[Test] epoch:256	batch id:30	 loss:1.404805
[Test] epoch:256	batch id:40	 loss:1.468514
[Test] epoch:256	batch id:50	 loss:1.502683
[Test] epoch:256	batch id:60	 loss:1.270383
[Test] epoch:256	batch id:70	 loss:1.532300
[Test] epoch:256	batch id:80	 loss:1.583846
[Test] epoch:256	batch id:90	 loss:1.470413
[Test] epoch:256	batch id:100	 loss:2.100817
[Test] epoch:256	batch id:110	 loss:1.328588
[Test] epoch:256	batch id:120	 loss:1.410191
[Test] epoch:256	batch id:130	 loss:1.318933
[Test] epoch:256	batch id:140	 loss:1.317825
[Test] epoch:256	batch id:150	 loss:1.841806
[Test] 256, loss: 1.481514, test acc: 0.909238,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:257	batch id:0	 lr:0.016619 loss:1.381613
[Train] epoch:257	batch id:10	 lr:0.016619 loss:1.397057
[Train] epoch:257	batch id:20	 lr:0.016619 loss:1.424090
[Train] epoch:257	batch id:30	 lr:0.016619 loss:1.304313
[Train] epoch:257	batch id:40	 lr:0.016619 loss:1.345791
[Train] epoch:257	batch id:50	 lr:0.016619 loss:1.328480
[Train] epoch:257	batch id:60	 lr:0.016619 loss:1.386770
[Train] epoch:257	batch id:70	 lr:0.016619 loss:1.431497
[Train] epoch:257	batch id:80	 lr:0.016619 loss:1.398439
[Train] epoch:257	batch id:90	 lr:0.016619 loss:1.360981
[Train] epoch:257	batch id:100	 lr:0.016619 loss:1.406469
[Train] epoch:257	batch id:110	 lr:0.016619 loss:1.394045
[Train] epoch:257	batch id:120	 lr:0.016619 loss:1.385204
[Train] epoch:257	batch id:130	 lr:0.016619 loss:1.331045
[Train] epoch:257	batch id:140	 lr:0.016619 loss:1.497821
[Train] epoch:257	batch id:150	 lr:0.016619 loss:1.599077
[Train] epoch:257	batch id:160	 lr:0.016619 loss:1.362959
[Train] epoch:257	batch id:170	 lr:0.016619 loss:1.407840
[Train] epoch:257	batch id:180	 lr:0.016619 loss:1.430318
[Train] epoch:257	batch id:190	 lr:0.016619 loss:1.389751
[Train] epoch:257	batch id:200	 lr:0.016619 loss:1.322790
[Train] epoch:257	batch id:210	 lr:0.016619 loss:1.357496
[Train] epoch:257	batch id:220	 lr:0.016619 loss:1.327399
[Train] epoch:257	batch id:230	 lr:0.016619 loss:1.381407
[Train] epoch:257	batch id:240	 lr:0.016619 loss:1.403347
[Train] epoch:257	batch id:250	 lr:0.016619 loss:1.473199
[Train] epoch:257	batch id:260	 lr:0.016619 loss:1.373123
[Train] epoch:257	batch id:270	 lr:0.016619 loss:1.401499
[Train] epoch:257	batch id:280	 lr:0.016619 loss:1.376104
[Train] epoch:257	batch id:290	 lr:0.016619 loss:1.394372
[Train] epoch:257	batch id:300	 lr:0.016619 loss:1.347646
[Train] 257, loss: 1.386314, train acc: 0.980558, 
[Test] epoch:257	batch id:0	 loss:1.322433
[Test] epoch:257	batch id:10	 loss:1.536377
[Test] epoch:257	batch id:20	 loss:1.393693
[Test] epoch:257	batch id:30	 loss:1.459543
[Test] epoch:257	batch id:40	 loss:1.460888
[Test] epoch:257	batch id:50	 loss:1.533253
[Test] epoch:257	batch id:60	 loss:1.265392
[Test] epoch:257	batch id:70	 loss:1.297927
[Test] epoch:257	batch id:80	 loss:1.691927
[Test] epoch:257	batch id:90	 loss:1.620055
[Test] epoch:257	batch id:100	 loss:1.978958
[Test] epoch:257	batch id:110	 loss:1.340764
[Test] epoch:257	batch id:120	 loss:1.356347
[Test] epoch:257	batch id:130	 loss:1.371378
[Test] epoch:257	batch id:140	 loss:1.341471
[Test] epoch:257	batch id:150	 loss:1.672866
[Test] 257, loss: 1.501124, test acc: 0.903566,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:258	batch id:0	 lr:0.016296 loss:1.418060
[Train] epoch:258	batch id:10	 lr:0.016296 loss:1.422332
[Train] epoch:258	batch id:20	 lr:0.016296 loss:1.367260
[Train] epoch:258	batch id:30	 lr:0.016296 loss:1.382488
[Train] epoch:258	batch id:40	 lr:0.016296 loss:1.398536
[Train] epoch:258	batch id:50	 lr:0.016296 loss:1.392163
[Train] epoch:258	batch id:60	 lr:0.016296 loss:1.334753
[Train] epoch:258	batch id:70	 lr:0.016296 loss:1.428015
[Train] epoch:258	batch id:80	 lr:0.016296 loss:1.368675
[Train] epoch:258	batch id:90	 lr:0.016296 loss:1.376012
[Train] epoch:258	batch id:100	 lr:0.016296 loss:1.377852
[Train] epoch:258	batch id:110	 lr:0.016296 loss:1.358091
[Train] epoch:258	batch id:120	 lr:0.016296 loss:1.415493
[Train] epoch:258	batch id:130	 lr:0.016296 loss:1.342823
[Train] epoch:258	batch id:140	 lr:0.016296 loss:1.441237
[Train] epoch:258	batch id:150	 lr:0.016296 loss:1.392747
[Train] epoch:258	batch id:160	 lr:0.016296 loss:1.373401
[Train] epoch:258	batch id:170	 lr:0.016296 loss:1.349615
[Train] epoch:258	batch id:180	 lr:0.016296 loss:1.445506
[Train] epoch:258	batch id:190	 lr:0.016296 loss:1.347509
[Train] epoch:258	batch id:200	 lr:0.016296 loss:1.364640
[Train] epoch:258	batch id:210	 lr:0.016296 loss:1.565009
[Train] epoch:258	batch id:220	 lr:0.016296 loss:1.353521
[Train] epoch:258	batch id:230	 lr:0.016296 loss:1.348497
[Train] epoch:258	batch id:240	 lr:0.016296 loss:1.421450
[Train] epoch:258	batch id:250	 lr:0.016296 loss:1.341247
[Train] epoch:258	batch id:260	 lr:0.016296 loss:1.471714
[Train] epoch:258	batch id:270	 lr:0.016296 loss:1.366477
[Train] epoch:258	batch id:280	 lr:0.016296 loss:1.367958
[Train] epoch:258	batch id:290	 lr:0.016296 loss:1.464017
[Train] epoch:258	batch id:300	 lr:0.016296 loss:1.383286
[Train] 258, loss: 1.387628, train acc: 0.979336, 
[Test] epoch:258	batch id:0	 loss:1.267128
[Test] epoch:258	batch id:10	 loss:1.533163
[Test] epoch:258	batch id:20	 loss:1.465731
[Test] epoch:258	batch id:30	 loss:1.438176
[Test] epoch:258	batch id:40	 loss:1.379662
[Test] epoch:258	batch id:50	 loss:1.545114
[Test] epoch:258	batch id:60	 loss:1.254236
[Test] epoch:258	batch id:70	 loss:1.399921
[Test] epoch:258	batch id:80	 loss:1.608241
[Test] epoch:258	batch id:90	 loss:1.514056
[Test] epoch:258	batch id:100	 loss:1.998137
[Test] epoch:258	batch id:110	 loss:1.366751
[Test] epoch:258	batch id:120	 loss:1.331294
[Test] epoch:258	batch id:130	 loss:1.336535
[Test] epoch:258	batch id:140	 loss:1.339811
[Test] epoch:258	batch id:150	 loss:1.699411
[Test] 258, loss: 1.460742, test acc: 0.916532,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:259	batch id:0	 lr:0.015977 loss:1.364569
[Train] epoch:259	batch id:10	 lr:0.015977 loss:1.386863
[Train] epoch:259	batch id:20	 lr:0.015977 loss:1.485939
[Train] epoch:259	batch id:30	 lr:0.015977 loss:1.359597
[Train] epoch:259	batch id:40	 lr:0.015977 loss:1.336092
[Train] epoch:259	batch id:50	 lr:0.015977 loss:1.400521
[Train] epoch:259	batch id:60	 lr:0.015977 loss:1.356361
[Train] epoch:259	batch id:70	 lr:0.015977 loss:1.353410
[Train] epoch:259	batch id:80	 lr:0.015977 loss:1.347191
[Train] epoch:259	batch id:90	 lr:0.015977 loss:1.347589
[Train] epoch:259	batch id:100	 lr:0.015977 loss:1.317357
[Train] epoch:259	batch id:110	 lr:0.015977 loss:1.379186
[Train] epoch:259	batch id:120	 lr:0.015977 loss:1.397241
[Train] epoch:259	batch id:130	 lr:0.015977 loss:1.414831
[Train] epoch:259	batch id:140	 lr:0.015977 loss:1.425507
[Train] epoch:259	batch id:150	 lr:0.015977 loss:1.359921
[Train] epoch:259	batch id:160	 lr:0.015977 loss:1.336670
[Train] epoch:259	batch id:170	 lr:0.015977 loss:1.331058
[Train] epoch:259	batch id:180	 lr:0.015977 loss:1.570152
[Train] epoch:259	batch id:190	 lr:0.015977 loss:1.330784
[Train] epoch:259	batch id:200	 lr:0.015977 loss:1.444162
[Train] epoch:259	batch id:210	 lr:0.015977 loss:1.398971
[Train] epoch:259	batch id:220	 lr:0.015977 loss:1.359393
[Train] epoch:259	batch id:230	 lr:0.015977 loss:1.350680
[Train] epoch:259	batch id:240	 lr:0.015977 loss:1.490787
[Train] epoch:259	batch id:250	 lr:0.015977 loss:1.415341
[Train] epoch:259	batch id:260	 lr:0.015977 loss:1.424129
[Train] epoch:259	batch id:270	 lr:0.015977 loss:1.430368
[Train] epoch:259	batch id:280	 lr:0.015977 loss:1.322059
[Train] epoch:259	batch id:290	 lr:0.015977 loss:1.415766
[Train] epoch:259	batch id:300	 lr:0.015977 loss:1.422242
[Train] 259, loss: 1.384421, train acc: 0.979540, 
[Test] epoch:259	batch id:0	 loss:1.332960
[Test] epoch:259	batch id:10	 loss:1.488858
[Test] epoch:259	batch id:20	 loss:1.410377
[Test] epoch:259	batch id:30	 loss:1.376093
[Test] epoch:259	batch id:40	 loss:1.623165
[Test] epoch:259	batch id:50	 loss:1.584979
[Test] epoch:259	batch id:60	 loss:1.289166
[Test] epoch:259	batch id:70	 loss:1.338028
[Test] epoch:259	batch id:80	 loss:1.651876
[Test] epoch:259	batch id:90	 loss:1.574337
[Test] epoch:259	batch id:100	 loss:2.106685
[Test] epoch:259	batch id:110	 loss:1.381187
[Test] epoch:259	batch id:120	 loss:1.391228
[Test] epoch:259	batch id:130	 loss:1.356436
[Test] epoch:259	batch id:140	 loss:1.341300
[Test] epoch:259	batch id:150	 loss:1.997793
[Test] 259, loss: 1.507636, test acc: 0.908023,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:260	batch id:0	 lr:0.015660 loss:1.386101
[Train] epoch:260	batch id:10	 lr:0.015660 loss:1.333408
[Train] epoch:260	batch id:20	 lr:0.015660 loss:1.417549
[Train] epoch:260	batch id:30	 lr:0.015660 loss:1.326585
[Train] epoch:260	batch id:40	 lr:0.015660 loss:1.333576
[Train] epoch:260	batch id:50	 lr:0.015660 loss:1.348391
[Train] epoch:260	batch id:60	 lr:0.015660 loss:1.381858
[Train] epoch:260	batch id:70	 lr:0.015660 loss:1.507041
[Train] epoch:260	batch id:80	 lr:0.015660 loss:1.321088
[Train] epoch:260	batch id:90	 lr:0.015660 loss:1.312333
[Train] epoch:260	batch id:100	 lr:0.015660 loss:1.447182
[Train] epoch:260	batch id:110	 lr:0.015660 loss:1.353979
[Train] epoch:260	batch id:120	 lr:0.015660 loss:1.332277
[Train] epoch:260	batch id:130	 lr:0.015660 loss:1.320861
[Train] epoch:260	batch id:140	 lr:0.015660 loss:1.338440
[Train] epoch:260	batch id:150	 lr:0.015660 loss:1.354210
[Train] epoch:260	batch id:160	 lr:0.015660 loss:1.435527
[Train] epoch:260	batch id:170	 lr:0.015660 loss:1.336235
[Train] epoch:260	batch id:180	 lr:0.015660 loss:1.493875
[Train] epoch:260	batch id:190	 lr:0.015660 loss:1.401022
[Train] epoch:260	batch id:200	 lr:0.015660 loss:1.346437
[Train] epoch:260	batch id:210	 lr:0.015660 loss:1.440611
[Train] epoch:260	batch id:220	 lr:0.015660 loss:1.355391
[Train] epoch:260	batch id:230	 lr:0.015660 loss:1.403531
[Train] epoch:260	batch id:240	 lr:0.015660 loss:1.376729
[Train] epoch:260	batch id:250	 lr:0.015660 loss:1.315243
[Train] epoch:260	batch id:260	 lr:0.015660 loss:1.396065
[Train] epoch:260	batch id:270	 lr:0.015660 loss:1.412621
[Train] epoch:260	batch id:280	 lr:0.015660 loss:1.384083
[Train] epoch:260	batch id:290	 lr:0.015660 loss:1.402463
[Train] epoch:260	batch id:300	 lr:0.015660 loss:1.410694
[Train] 260, loss: 1.383794, train acc: 0.980049, 
[Test] epoch:260	batch id:0	 loss:1.298705
[Test] epoch:260	batch id:10	 loss:1.546686
[Test] epoch:260	batch id:20	 loss:1.449895
[Test] epoch:260	batch id:30	 loss:1.445187
[Test] epoch:260	batch id:40	 loss:1.564182
[Test] epoch:260	batch id:50	 loss:1.584411
[Test] epoch:260	batch id:60	 loss:1.285229
[Test] epoch:260	batch id:70	 loss:1.321048
[Test] epoch:260	batch id:80	 loss:1.715758
[Test] epoch:260	batch id:90	 loss:1.704716
[Test] epoch:260	batch id:100	 loss:2.077320
[Test] epoch:260	batch id:110	 loss:1.503313
[Test] epoch:260	batch id:120	 loss:1.306155
[Test] epoch:260	batch id:130	 loss:1.439530
[Test] epoch:260	batch id:140	 loss:1.353177
[Test] epoch:260	batch id:150	 loss:1.791521
[Test] 260, loss: 1.506733, test acc: 0.910859,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:261	batch id:0	 lr:0.015345 loss:1.389401
[Train] epoch:261	batch id:10	 lr:0.015345 loss:1.441628
[Train] epoch:261	batch id:20	 lr:0.015345 loss:1.399706
[Train] epoch:261	batch id:30	 lr:0.015345 loss:1.340905
[Train] epoch:261	batch id:40	 lr:0.015345 loss:1.352474
[Train] epoch:261	batch id:50	 lr:0.015345 loss:1.429474
[Train] epoch:261	batch id:60	 lr:0.015345 loss:1.454954
[Train] epoch:261	batch id:70	 lr:0.015345 loss:1.481037
[Train] epoch:261	batch id:80	 lr:0.015345 loss:1.326702
[Train] epoch:261	batch id:90	 lr:0.015345 loss:1.345086
[Train] epoch:261	batch id:100	 lr:0.015345 loss:1.431309
[Train] epoch:261	batch id:110	 lr:0.015345 loss:1.438248
[Train] epoch:261	batch id:120	 lr:0.015345 loss:1.332332
[Train] epoch:261	batch id:130	 lr:0.015345 loss:1.387677
[Train] epoch:261	batch id:140	 lr:0.015345 loss:1.325357
[Train] epoch:261	batch id:150	 lr:0.015345 loss:1.357523
[Train] epoch:261	batch id:160	 lr:0.015345 loss:1.336577
[Train] epoch:261	batch id:170	 lr:0.015345 loss:1.422759
[Train] epoch:261	batch id:180	 lr:0.015345 loss:1.344717
[Train] epoch:261	batch id:190	 lr:0.015345 loss:1.334692
[Train] epoch:261	batch id:200	 lr:0.015345 loss:1.424744
[Train] epoch:261	batch id:210	 lr:0.015345 loss:1.375359
[Train] epoch:261	batch id:220	 lr:0.015345 loss:1.469939
[Train] epoch:261	batch id:230	 lr:0.015345 loss:1.444835
[Train] epoch:261	batch id:240	 lr:0.015345 loss:1.413653
[Train] epoch:261	batch id:250	 lr:0.015345 loss:1.402574
[Train] epoch:261	batch id:260	 lr:0.015345 loss:1.362381
[Train] epoch:261	batch id:270	 lr:0.015345 loss:1.353088
[Train] epoch:261	batch id:280	 lr:0.015345 loss:1.424747
[Train] epoch:261	batch id:290	 lr:0.015345 loss:1.371490
[Train] epoch:261	batch id:300	 lr:0.015345 loss:1.339689
[Train] 261, loss: 1.380164, train acc: 0.981474, 
[Test] epoch:261	batch id:0	 loss:1.264030
[Test] epoch:261	batch id:10	 loss:1.457824
[Test] epoch:261	batch id:20	 loss:1.461465
[Test] epoch:261	batch id:30	 loss:1.372566
[Test] epoch:261	batch id:40	 loss:1.491101
[Test] epoch:261	batch id:50	 loss:1.547948
[Test] epoch:261	batch id:60	 loss:1.255509
[Test] epoch:261	batch id:70	 loss:1.329949
[Test] epoch:261	batch id:80	 loss:1.632225
[Test] epoch:261	batch id:90	 loss:1.495278
[Test] epoch:261	batch id:100	 loss:1.979076
[Test] epoch:261	batch id:110	 loss:1.321353
[Test] epoch:261	batch id:120	 loss:1.316935
[Test] epoch:261	batch id:130	 loss:1.297295
[Test] epoch:261	batch id:140	 loss:1.324262
[Test] epoch:261	batch id:150	 loss:1.794897
[Test] 261, loss: 1.455007, test acc: 0.912885,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:262	batch id:0	 lr:0.015034 loss:1.395623
[Train] epoch:262	batch id:10	 lr:0.015034 loss:1.424110
[Train] epoch:262	batch id:20	 lr:0.015034 loss:1.399961
[Train] epoch:262	batch id:30	 lr:0.015034 loss:1.301862
[Train] epoch:262	batch id:40	 lr:0.015034 loss:1.359797
[Train] epoch:262	batch id:50	 lr:0.015034 loss:1.358613
[Train] epoch:262	batch id:60	 lr:0.015034 loss:1.510362
[Train] epoch:262	batch id:70	 lr:0.015034 loss:1.365051
[Train] epoch:262	batch id:80	 lr:0.015034 loss:1.416849
[Train] epoch:262	batch id:90	 lr:0.015034 loss:1.343850
[Train] epoch:262	batch id:100	 lr:0.015034 loss:1.351498
[Train] epoch:262	batch id:110	 lr:0.015034 loss:1.384943
[Train] epoch:262	batch id:120	 lr:0.015034 loss:1.371158
[Train] epoch:262	batch id:130	 lr:0.015034 loss:1.396931
[Train] epoch:262	batch id:140	 lr:0.015034 loss:1.396654
[Train] epoch:262	batch id:150	 lr:0.015034 loss:1.405222
[Train] epoch:262	batch id:160	 lr:0.015034 loss:1.365617
[Train] epoch:262	batch id:170	 lr:0.015034 loss:1.364727
[Train] epoch:262	batch id:180	 lr:0.015034 loss:1.362104
[Train] epoch:262	batch id:190	 lr:0.015034 loss:1.362492
[Train] epoch:262	batch id:200	 lr:0.015034 loss:1.382264
[Train] epoch:262	batch id:210	 lr:0.015034 loss:1.415821
[Train] epoch:262	batch id:220	 lr:0.015034 loss:1.417683
[Train] epoch:262	batch id:230	 lr:0.015034 loss:1.394707
[Train] epoch:262	batch id:240	 lr:0.015034 loss:1.419710
[Train] epoch:262	batch id:250	 lr:0.015034 loss:1.390969
[Train] epoch:262	batch id:260	 lr:0.015034 loss:1.353804
[Train] epoch:262	batch id:270	 lr:0.015034 loss:1.328907
[Train] epoch:262	batch id:280	 lr:0.015034 loss:1.403615
[Train] epoch:262	batch id:290	 lr:0.015034 loss:1.421679
[Train] epoch:262	batch id:300	 lr:0.015034 loss:1.377809
[Train] 262, loss: 1.380405, train acc: 0.979845, 
[Test] epoch:262	batch id:0	 loss:1.285627
[Test] epoch:262	batch id:10	 loss:1.434718
[Test] epoch:262	batch id:20	 loss:1.513728
[Test] epoch:262	batch id:30	 loss:1.389260
[Test] epoch:262	batch id:40	 loss:1.485385
[Test] epoch:262	batch id:50	 loss:1.448569
[Test] epoch:262	batch id:60	 loss:1.277188
[Test] epoch:262	batch id:70	 loss:1.454559
[Test] epoch:262	batch id:80	 loss:1.612982
[Test] epoch:262	batch id:90	 loss:1.580426
[Test] epoch:262	batch id:100	 loss:2.046662
[Test] epoch:262	batch id:110	 loss:1.336428
[Test] epoch:262	batch id:120	 loss:1.394027
[Test] epoch:262	batch id:130	 loss:1.356970
[Test] epoch:262	batch id:140	 loss:1.360441
[Test] epoch:262	batch id:150	 loss:1.755296
[Test] 262, loss: 1.471992, test acc: 0.918152,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:263	batch id:0	 lr:0.014726 loss:1.361917
[Train] epoch:263	batch id:10	 lr:0.014726 loss:1.330104
[Train] epoch:263	batch id:20	 lr:0.014726 loss:1.404456
[Train] epoch:263	batch id:30	 lr:0.014726 loss:1.402513
[Train] epoch:263	batch id:40	 lr:0.014726 loss:1.430056
[Train] epoch:263	batch id:50	 lr:0.014726 loss:1.353759
[Train] epoch:263	batch id:60	 lr:0.014726 loss:1.364778
[Train] epoch:263	batch id:70	 lr:0.014726 loss:1.373499
[Train] epoch:263	batch id:80	 lr:0.014726 loss:1.403177
[Train] epoch:263	batch id:90	 lr:0.014726 loss:1.358230
[Train] epoch:263	batch id:100	 lr:0.014726 loss:1.412297
[Train] epoch:263	batch id:110	 lr:0.014726 loss:1.377216
[Train] epoch:263	batch id:120	 lr:0.014726 loss:1.347308
[Train] epoch:263	batch id:130	 lr:0.014726 loss:1.350187
[Train] epoch:263	batch id:140	 lr:0.014726 loss:1.345194
[Train] epoch:263	batch id:150	 lr:0.014726 loss:1.398519
[Train] epoch:263	batch id:160	 lr:0.014726 loss:1.346383
[Train] epoch:263	batch id:170	 lr:0.014726 loss:1.450761
[Train] epoch:263	batch id:180	 lr:0.014726 loss:1.348101
[Train] epoch:263	batch id:190	 lr:0.014726 loss:1.427390
[Train] epoch:263	batch id:200	 lr:0.014726 loss:1.354600
[Train] epoch:263	batch id:210	 lr:0.014726 loss:1.373746
[Train] epoch:263	batch id:220	 lr:0.014726 loss:1.405002
[Train] epoch:263	batch id:230	 lr:0.014726 loss:1.365877
[Train] epoch:263	batch id:240	 lr:0.014726 loss:1.330625
[Train] epoch:263	batch id:250	 lr:0.014726 loss:1.329063
[Train] epoch:263	batch id:260	 lr:0.014726 loss:1.411271
[Train] epoch:263	batch id:270	 lr:0.014726 loss:1.352362
[Train] epoch:263	batch id:280	 lr:0.014726 loss:1.363550
[Train] epoch:263	batch id:290	 lr:0.014726 loss:1.379640
[Train] epoch:263	batch id:300	 lr:0.014726 loss:1.371287
[Train] 263, loss: 1.378250, train acc: 0.981881, 
[Test] epoch:263	batch id:0	 loss:1.406862
[Test] epoch:263	batch id:10	 loss:1.522687
[Test] epoch:263	batch id:20	 loss:1.580451
[Test] epoch:263	batch id:30	 loss:1.366258
[Test] epoch:263	batch id:40	 loss:1.593425
[Test] epoch:263	batch id:50	 loss:1.581388
[Test] epoch:263	batch id:60	 loss:1.277878
[Test] epoch:263	batch id:70	 loss:1.384184
[Test] epoch:263	batch id:80	 loss:1.621889
[Test] epoch:263	batch id:90	 loss:1.612057
[Test] epoch:263	batch id:100	 loss:2.139055
[Test] epoch:263	batch id:110	 loss:1.315979
[Test] epoch:263	batch id:120	 loss:1.338677
[Test] epoch:263	batch id:130	 loss:1.329494
[Test] epoch:263	batch id:140	 loss:1.366419
[Test] epoch:263	batch id:150	 loss:1.827015
[Test] 263, loss: 1.497137, test acc: 0.910049,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:264	batch id:0	 lr:0.014420 loss:1.370764
[Train] epoch:264	batch id:10	 lr:0.014420 loss:1.364796
[Train] epoch:264	batch id:20	 lr:0.014420 loss:1.327139
[Train] epoch:264	batch id:30	 lr:0.014420 loss:1.361424
[Train] epoch:264	batch id:40	 lr:0.014420 loss:1.353098
[Train] epoch:264	batch id:50	 lr:0.014420 loss:1.386250
[Train] epoch:264	batch id:60	 lr:0.014420 loss:1.403460
[Train] epoch:264	batch id:70	 lr:0.014420 loss:1.357242
[Train] epoch:264	batch id:80	 lr:0.014420 loss:1.364113
[Train] epoch:264	batch id:90	 lr:0.014420 loss:1.388364
[Train] epoch:264	batch id:100	 lr:0.014420 loss:1.372808
[Train] epoch:264	batch id:110	 lr:0.014420 loss:1.366250
[Train] epoch:264	batch id:120	 lr:0.014420 loss:1.322795
[Train] epoch:264	batch id:130	 lr:0.014420 loss:1.372204
[Train] epoch:264	batch id:140	 lr:0.014420 loss:1.352121
[Train] epoch:264	batch id:150	 lr:0.014420 loss:1.480154
[Train] epoch:264	batch id:160	 lr:0.014420 loss:1.380410
[Train] epoch:264	batch id:170	 lr:0.014420 loss:1.314440
[Train] epoch:264	batch id:180	 lr:0.014420 loss:1.320139
[Train] epoch:264	batch id:190	 lr:0.014420 loss:1.341386
[Train] epoch:264	batch id:200	 lr:0.014420 loss:1.367325
[Train] epoch:264	batch id:210	 lr:0.014420 loss:1.363143
[Train] epoch:264	batch id:220	 lr:0.014420 loss:1.354599
[Train] epoch:264	batch id:230	 lr:0.014420 loss:1.369867
[Train] epoch:264	batch id:240	 lr:0.014420 loss:1.408904
[Train] epoch:264	batch id:250	 lr:0.014420 loss:1.350764
[Train] epoch:264	batch id:260	 lr:0.014420 loss:1.373643
[Train] epoch:264	batch id:270	 lr:0.014420 loss:1.506672
[Train] epoch:264	batch id:280	 lr:0.014420 loss:1.395859
[Train] epoch:264	batch id:290	 lr:0.014420 loss:1.340780
[Train] epoch:264	batch id:300	 lr:0.014420 loss:1.326248
[Train] 264, loss: 1.377718, train acc: 0.983612, 
[Test] epoch:264	batch id:0	 loss:1.298596
[Test] epoch:264	batch id:10	 loss:1.511990
[Test] epoch:264	batch id:20	 loss:1.513538
[Test] epoch:264	batch id:30	 loss:1.505767
[Test] epoch:264	batch id:40	 loss:1.598252
[Test] epoch:264	batch id:50	 loss:1.505569
[Test] epoch:264	batch id:60	 loss:1.271324
[Test] epoch:264	batch id:70	 loss:1.357328
[Test] epoch:264	batch id:80	 loss:1.562681
[Test] epoch:264	batch id:90	 loss:1.532129
[Test] epoch:264	batch id:100	 loss:1.967762
[Test] epoch:264	batch id:110	 loss:1.457438
[Test] epoch:264	batch id:120	 loss:1.332848
[Test] epoch:264	batch id:130	 loss:1.369735
[Test] epoch:264	batch id:140	 loss:1.377661
[Test] epoch:264	batch id:150	 loss:1.804408
[Test] 264, loss: 1.488810, test acc: 0.907212,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:265	batch id:0	 lr:0.014117 loss:1.333625
[Train] epoch:265	batch id:10	 lr:0.014117 loss:1.379803
[Train] epoch:265	batch id:20	 lr:0.014117 loss:1.341136
[Train] epoch:265	batch id:30	 lr:0.014117 loss:1.324593
[Train] epoch:265	batch id:40	 lr:0.014117 loss:1.511102
[Train] epoch:265	batch id:50	 lr:0.014117 loss:1.356175
[Train] epoch:265	batch id:60	 lr:0.014117 loss:1.342273
[Train] epoch:265	batch id:70	 lr:0.014117 loss:1.378264
[Train] epoch:265	batch id:80	 lr:0.014117 loss:1.382936
[Train] epoch:265	batch id:90	 lr:0.014117 loss:1.364927
[Train] epoch:265	batch id:100	 lr:0.014117 loss:1.324861
[Train] epoch:265	batch id:110	 lr:0.014117 loss:1.343108
[Train] epoch:265	batch id:120	 lr:0.014117 loss:1.352140
[Train] epoch:265	batch id:130	 lr:0.014117 loss:1.415562
[Train] epoch:265	batch id:140	 lr:0.014117 loss:1.394114
[Train] epoch:265	batch id:150	 lr:0.014117 loss:1.344968
[Train] epoch:265	batch id:160	 lr:0.014117 loss:1.327765
[Train] epoch:265	batch id:170	 lr:0.014117 loss:1.388005
[Train] epoch:265	batch id:180	 lr:0.014117 loss:1.347283
[Train] epoch:265	batch id:190	 lr:0.014117 loss:1.347898
[Train] epoch:265	batch id:200	 lr:0.014117 loss:1.377781
[Train] epoch:265	batch id:210	 lr:0.014117 loss:1.437129
[Train] epoch:265	batch id:220	 lr:0.014117 loss:1.438954
[Train] epoch:265	batch id:230	 lr:0.014117 loss:1.363300
[Train] epoch:265	batch id:240	 lr:0.014117 loss:1.379029
[Train] epoch:265	batch id:250	 lr:0.014117 loss:1.355237
[Train] epoch:265	batch id:260	 lr:0.014117 loss:1.367800
[Train] epoch:265	batch id:270	 lr:0.014117 loss:1.357693
[Train] epoch:265	batch id:280	 lr:0.014117 loss:1.365465
[Train] epoch:265	batch id:290	 lr:0.014117 loss:1.354479
[Train] epoch:265	batch id:300	 lr:0.014117 loss:1.354134
[Train] 265, loss: 1.374370, train acc: 0.983510, 
[Test] epoch:265	batch id:0	 loss:1.288730
[Test] epoch:265	batch id:10	 loss:1.593389
[Test] epoch:265	batch id:20	 loss:1.421018
[Test] epoch:265	batch id:30	 loss:1.452355
[Test] epoch:265	batch id:40	 loss:1.434964
[Test] epoch:265	batch id:50	 loss:1.494393
[Test] epoch:265	batch id:60	 loss:1.260256
[Test] epoch:265	batch id:70	 loss:1.434578
[Test] epoch:265	batch id:80	 loss:1.727182
[Test] epoch:265	batch id:90	 loss:1.506699
[Test] epoch:265	batch id:100	 loss:1.990875
[Test] epoch:265	batch id:110	 loss:1.356354
[Test] epoch:265	batch id:120	 loss:1.351772
[Test] epoch:265	batch id:130	 loss:1.324597
[Test] epoch:265	batch id:140	 loss:1.393878
[Test] epoch:265	batch id:150	 loss:1.893301
[Test] 265, loss: 1.486088, test acc: 0.913695,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:266	batch id:0	 lr:0.013817 loss:1.408466
[Train] epoch:266	batch id:10	 lr:0.013817 loss:1.470892
[Train] epoch:266	batch id:20	 lr:0.013817 loss:1.386514
[Train] epoch:266	batch id:30	 lr:0.013817 loss:1.331190
[Train] epoch:266	batch id:40	 lr:0.013817 loss:1.332079
[Train] epoch:266	batch id:50	 lr:0.013817 loss:1.357486
[Train] epoch:266	batch id:60	 lr:0.013817 loss:1.388797
[Train] epoch:266	batch id:70	 lr:0.013817 loss:1.349681
[Train] epoch:266	batch id:80	 lr:0.013817 loss:1.357904
[Train] epoch:266	batch id:90	 lr:0.013817 loss:1.347896
[Train] epoch:266	batch id:100	 lr:0.013817 loss:1.393682
[Train] epoch:266	batch id:110	 lr:0.013817 loss:1.354302
[Train] epoch:266	batch id:120	 lr:0.013817 loss:1.356452
[Train] epoch:266	batch id:130	 lr:0.013817 loss:1.406235
[Train] epoch:266	batch id:140	 lr:0.013817 loss:1.376806
[Train] epoch:266	batch id:150	 lr:0.013817 loss:1.330214
[Train] epoch:266	batch id:160	 lr:0.013817 loss:1.323777
[Train] epoch:266	batch id:170	 lr:0.013817 loss:1.343518
[Train] epoch:266	batch id:180	 lr:0.013817 loss:1.345681
[Train] epoch:266	batch id:190	 lr:0.013817 loss:1.348670
[Train] epoch:266	batch id:200	 lr:0.013817 loss:1.371618
[Train] epoch:266	batch id:210	 lr:0.013817 loss:1.327703
[Train] epoch:266	batch id:220	 lr:0.013817 loss:1.325768
[Train] epoch:266	batch id:230	 lr:0.013817 loss:1.331393
[Train] epoch:266	batch id:240	 lr:0.013817 loss:1.361373
[Train] epoch:266	batch id:250	 lr:0.013817 loss:1.398998
[Train] epoch:266	batch id:260	 lr:0.013817 loss:1.346501
[Train] epoch:266	batch id:270	 lr:0.013817 loss:1.406686
[Train] epoch:266	batch id:280	 lr:0.013817 loss:1.313029
[Train] epoch:266	batch id:290	 lr:0.013817 loss:1.422725
[Train] epoch:266	batch id:300	 lr:0.013817 loss:1.370261
[Train] 266, loss: 1.372764, train acc: 0.986564, 
[Test] epoch:266	batch id:0	 loss:1.261388
[Test] epoch:266	batch id:10	 loss:1.433030
[Test] epoch:266	batch id:20	 loss:1.432026
[Test] epoch:266	batch id:30	 loss:1.432812
[Test] epoch:266	batch id:40	 loss:1.463557
[Test] epoch:266	batch id:50	 loss:1.509694
[Test] epoch:266	batch id:60	 loss:1.251390
[Test] epoch:266	batch id:70	 loss:1.378504
[Test] epoch:266	batch id:80	 loss:1.638412
[Test] epoch:266	batch id:90	 loss:1.513744
[Test] epoch:266	batch id:100	 loss:1.887348
[Test] epoch:266	batch id:110	 loss:1.360879
[Test] epoch:266	batch id:120	 loss:1.350398
[Test] epoch:266	batch id:130	 loss:1.337260
[Test] epoch:266	batch id:140	 loss:1.333747
[Test] epoch:266	batch id:150	 loss:1.667280
[Test] 266, loss: 1.470100, test acc: 0.912480,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:267	batch id:0	 lr:0.013521 loss:1.340794
[Train] epoch:267	batch id:10	 lr:0.013521 loss:1.361765
[Train] epoch:267	batch id:20	 lr:0.013521 loss:1.390148
[Train] epoch:267	batch id:30	 lr:0.013521 loss:1.334471
[Train] epoch:267	batch id:40	 lr:0.013521 loss:1.347812
[Train] epoch:267	batch id:50	 lr:0.013521 loss:1.480733
[Train] epoch:267	batch id:60	 lr:0.013521 loss:1.364543
[Train] epoch:267	batch id:70	 lr:0.013521 loss:1.407122
[Train] epoch:267	batch id:80	 lr:0.013521 loss:1.399750
[Train] epoch:267	batch id:90	 lr:0.013521 loss:1.417440
[Train] epoch:267	batch id:100	 lr:0.013521 loss:1.347504
[Train] epoch:267	batch id:110	 lr:0.013521 loss:1.391153
[Train] epoch:267	batch id:120	 lr:0.013521 loss:1.381033
[Train] epoch:267	batch id:130	 lr:0.013521 loss:1.329305
[Train] epoch:267	batch id:140	 lr:0.013521 loss:1.393239
[Train] epoch:267	batch id:150	 lr:0.013521 loss:1.399987
[Train] epoch:267	batch id:160	 lr:0.013521 loss:1.362885
[Train] epoch:267	batch id:170	 lr:0.013521 loss:1.358408
[Train] epoch:267	batch id:180	 lr:0.013521 loss:1.366857
[Train] epoch:267	batch id:190	 lr:0.013521 loss:1.312522
[Train] epoch:267	batch id:200	 lr:0.013521 loss:1.320964
[Train] epoch:267	batch id:210	 lr:0.013521 loss:1.324502
[Train] epoch:267	batch id:220	 lr:0.013521 loss:1.375508
[Train] epoch:267	batch id:230	 lr:0.013521 loss:1.363611
[Train] epoch:267	batch id:240	 lr:0.013521 loss:1.348992
[Train] epoch:267	batch id:250	 lr:0.013521 loss:1.331522
[Train] epoch:267	batch id:260	 lr:0.013521 loss:1.451177
[Train] epoch:267	batch id:270	 lr:0.013521 loss:1.367795
[Train] epoch:267	batch id:280	 lr:0.013521 loss:1.328648
[Train] epoch:267	batch id:290	 lr:0.013521 loss:1.384099
[Train] epoch:267	batch id:300	 lr:0.013521 loss:1.403616
[Train] 267, loss: 1.374334, train acc: 0.983917, 
[Test] epoch:267	batch id:0	 loss:1.271700
[Test] epoch:267	batch id:10	 loss:1.416358
[Test] epoch:267	batch id:20	 loss:1.419783
[Test] epoch:267	batch id:30	 loss:1.329880
[Test] epoch:267	batch id:40	 loss:1.424774
[Test] epoch:267	batch id:50	 loss:1.466289
[Test] epoch:267	batch id:60	 loss:1.246979
[Test] epoch:267	batch id:70	 loss:1.538337
[Test] epoch:267	batch id:80	 loss:1.581803
[Test] epoch:267	batch id:90	 loss:1.467285
[Test] epoch:267	batch id:100	 loss:2.012022
[Test] epoch:267	batch id:110	 loss:1.337984
[Test] epoch:267	batch id:120	 loss:1.342777
[Test] epoch:267	batch id:130	 loss:1.314470
[Test] epoch:267	batch id:140	 loss:1.343121
[Test] epoch:267	batch id:150	 loss:1.907124
[Test] 267, loss: 1.456672, test acc: 0.918152,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:268	batch id:0	 lr:0.013227 loss:1.363283
[Train] epoch:268	batch id:10	 lr:0.013227 loss:1.410122
[Train] epoch:268	batch id:20	 lr:0.013227 loss:1.380260
[Train] epoch:268	batch id:30	 lr:0.013227 loss:1.333676
[Train] epoch:268	batch id:40	 lr:0.013227 loss:1.492522
[Train] epoch:268	batch id:50	 lr:0.013227 loss:1.349243
[Train] epoch:268	batch id:60	 lr:0.013227 loss:1.345606
[Train] epoch:268	batch id:70	 lr:0.013227 loss:1.371800
[Train] epoch:268	batch id:80	 lr:0.013227 loss:1.445982
[Train] epoch:268	batch id:90	 lr:0.013227 loss:1.397740
[Train] epoch:268	batch id:100	 lr:0.013227 loss:1.346598
[Train] epoch:268	batch id:110	 lr:0.013227 loss:1.410927
[Train] epoch:268	batch id:120	 lr:0.013227 loss:1.422218
[Train] epoch:268	batch id:130	 lr:0.013227 loss:1.457336
[Train] epoch:268	batch id:140	 lr:0.013227 loss:1.367698
[Train] epoch:268	batch id:150	 lr:0.013227 loss:1.341582
[Train] epoch:268	batch id:160	 lr:0.013227 loss:1.427051
[Train] epoch:268	batch id:170	 lr:0.013227 loss:1.371194
[Train] epoch:268	batch id:180	 lr:0.013227 loss:1.400073
[Train] epoch:268	batch id:190	 lr:0.013227 loss:1.386451
[Train] epoch:268	batch id:200	 lr:0.013227 loss:1.320992
[Train] epoch:268	batch id:210	 lr:0.013227 loss:1.306772
[Train] epoch:268	batch id:220	 lr:0.013227 loss:1.322839
[Train] epoch:268	batch id:230	 lr:0.013227 loss:1.388749
[Train] epoch:268	batch id:240	 lr:0.013227 loss:1.312195
[Train] epoch:268	batch id:250	 lr:0.013227 loss:1.409477
[Train] epoch:268	batch id:260	 lr:0.013227 loss:1.369387
[Train] epoch:268	batch id:270	 lr:0.013227 loss:1.342649
[Train] epoch:268	batch id:280	 lr:0.013227 loss:1.332659
[Train] epoch:268	batch id:290	 lr:0.013227 loss:1.401932
[Train] epoch:268	batch id:300	 lr:0.013227 loss:1.335017
[Train] 268, loss: 1.373781, train acc: 0.985342, 
[Test] epoch:268	batch id:0	 loss:1.262199
[Test] epoch:268	batch id:10	 loss:1.466286
[Test] epoch:268	batch id:20	 loss:1.431645
[Test] epoch:268	batch id:30	 loss:1.333439
[Test] epoch:268	batch id:40	 loss:1.454232
[Test] epoch:268	batch id:50	 loss:1.464511
[Test] epoch:268	batch id:60	 loss:1.249098
[Test] epoch:268	batch id:70	 loss:1.371473
[Test] epoch:268	batch id:80	 loss:1.585352
[Test] epoch:268	batch id:90	 loss:1.471735
[Test] epoch:268	batch id:100	 loss:1.877200
[Test] epoch:268	batch id:110	 loss:1.293396
[Test] epoch:268	batch id:120	 loss:1.274348
[Test] epoch:268	batch id:130	 loss:1.297529
[Test] epoch:268	batch id:140	 loss:1.356817
[Test] epoch:268	batch id:150	 loss:1.631002
[Test] 268, loss: 1.452682, test acc: 0.918963,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:269	batch id:0	 lr:0.012936 loss:1.376492
[Train] epoch:269	batch id:10	 lr:0.012936 loss:1.383493
[Train] epoch:269	batch id:20	 lr:0.012936 loss:1.372891
[Train] epoch:269	batch id:30	 lr:0.012936 loss:1.366858
[Train] epoch:269	batch id:40	 lr:0.012936 loss:1.438748
[Train] epoch:269	batch id:50	 lr:0.012936 loss:1.354794
[Train] epoch:269	batch id:60	 lr:0.012936 loss:1.341494
[Train] epoch:269	batch id:70	 lr:0.012936 loss:1.347069
[Train] epoch:269	batch id:80	 lr:0.012936 loss:1.347794
[Train] epoch:269	batch id:90	 lr:0.012936 loss:1.374626
[Train] epoch:269	batch id:100	 lr:0.012936 loss:1.378107
[Train] epoch:269	batch id:110	 lr:0.012936 loss:1.388283
[Train] epoch:269	batch id:120	 lr:0.012936 loss:1.382507
[Train] epoch:269	batch id:130	 lr:0.012936 loss:1.419632
[Train] epoch:269	batch id:140	 lr:0.012936 loss:1.359936
[Train] epoch:269	batch id:150	 lr:0.012936 loss:1.324665
[Train] epoch:269	batch id:160	 lr:0.012936 loss:1.329033
[Train] epoch:269	batch id:170	 lr:0.012936 loss:1.382197
[Train] epoch:269	batch id:180	 lr:0.012936 loss:1.354076
[Train] epoch:269	batch id:190	 lr:0.012936 loss:1.325038
[Train] epoch:269	batch id:200	 lr:0.012936 loss:1.333801
[Train] epoch:269	batch id:210	 lr:0.012936 loss:1.393669
[Train] epoch:269	batch id:220	 lr:0.012936 loss:1.449033
[Train] epoch:269	batch id:230	 lr:0.012936 loss:1.340649
[Train] epoch:269	batch id:240	 lr:0.012936 loss:1.477785
[Train] epoch:269	batch id:250	 lr:0.012936 loss:1.355138
[Train] epoch:269	batch id:260	 lr:0.012936 loss:1.389627
[Train] epoch:269	batch id:270	 lr:0.012936 loss:1.330237
[Train] epoch:269	batch id:280	 lr:0.012936 loss:1.385098
[Train] epoch:269	batch id:290	 lr:0.012936 loss:1.358988
[Train] epoch:269	batch id:300	 lr:0.012936 loss:1.438277
[Train] 269, loss: 1.372132, train acc: 0.984935, 
[Test] epoch:269	batch id:0	 loss:1.299650
[Test] epoch:269	batch id:10	 loss:1.506973
[Test] epoch:269	batch id:20	 loss:1.474362
[Test] epoch:269	batch id:30	 loss:1.344398
[Test] epoch:269	batch id:40	 loss:1.555771
[Test] epoch:269	batch id:50	 loss:1.548928
[Test] epoch:269	batch id:60	 loss:1.274842
[Test] epoch:269	batch id:70	 loss:1.429451
[Test] epoch:269	batch id:80	 loss:1.632499
[Test] epoch:269	batch id:90	 loss:1.642869
[Test] epoch:269	batch id:100	 loss:2.003847
[Test] epoch:269	batch id:110	 loss:1.384138
[Test] epoch:269	batch id:120	 loss:1.344495
[Test] epoch:269	batch id:130	 loss:1.333515
[Test] epoch:269	batch id:140	 loss:1.415174
[Test] epoch:269	batch id:150	 loss:1.726842
[Test] 269, loss: 1.504352, test acc: 0.905997,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:270	batch id:0	 lr:0.012648 loss:1.356189
[Train] epoch:270	batch id:10	 lr:0.012648 loss:1.345828
[Train] epoch:270	batch id:20	 lr:0.012648 loss:1.340737
[Train] epoch:270	batch id:30	 lr:0.012648 loss:1.319748
[Train] epoch:270	batch id:40	 lr:0.012648 loss:1.328353
[Train] epoch:270	batch id:50	 lr:0.012648 loss:1.363935
[Train] epoch:270	batch id:60	 lr:0.012648 loss:1.354459
[Train] epoch:270	batch id:70	 lr:0.012648 loss:1.344459
[Train] epoch:270	batch id:80	 lr:0.012648 loss:1.374144
[Train] epoch:270	batch id:90	 lr:0.012648 loss:1.338597
[Train] epoch:270	batch id:100	 lr:0.012648 loss:1.328667
[Train] epoch:270	batch id:110	 lr:0.012648 loss:1.405204
[Train] epoch:270	batch id:120	 lr:0.012648 loss:1.439515
[Train] epoch:270	batch id:130	 lr:0.012648 loss:1.374625
[Train] epoch:270	batch id:140	 lr:0.012648 loss:1.403779
[Train] epoch:270	batch id:150	 lr:0.012648 loss:1.318714
[Train] epoch:270	batch id:160	 lr:0.012648 loss:1.323078
[Train] epoch:270	batch id:170	 lr:0.012648 loss:1.362692
[Train] epoch:270	batch id:180	 lr:0.012648 loss:1.344934
[Train] epoch:270	batch id:190	 lr:0.012648 loss:1.368412
[Train] epoch:270	batch id:200	 lr:0.012648 loss:1.353367
[Train] epoch:270	batch id:210	 lr:0.012648 loss:1.342857
[Train] epoch:270	batch id:220	 lr:0.012648 loss:1.325870
[Train] epoch:270	batch id:230	 lr:0.012648 loss:1.385498
[Train] epoch:270	batch id:240	 lr:0.012648 loss:1.377434
[Train] epoch:270	batch id:250	 lr:0.012648 loss:1.332555
[Train] epoch:270	batch id:260	 lr:0.012648 loss:1.355148
[Train] epoch:270	batch id:270	 lr:0.012648 loss:1.372156
[Train] epoch:270	batch id:280	 lr:0.012648 loss:1.381910
[Train] epoch:270	batch id:290	 lr:0.012648 loss:1.406775
[Train] epoch:270	batch id:300	 lr:0.012648 loss:1.360078
[Train] 270, loss: 1.371085, train acc: 0.986462, 
[Test] epoch:270	batch id:0	 loss:1.266402
[Test] epoch:270	batch id:10	 loss:1.431558
[Test] epoch:270	batch id:20	 loss:1.375966
[Test] epoch:270	batch id:30	 loss:1.356182
[Test] epoch:270	batch id:40	 loss:1.508088
[Test] epoch:270	batch id:50	 loss:1.539477
[Test] epoch:270	batch id:60	 loss:1.260495
[Test] epoch:270	batch id:70	 loss:1.424843
[Test] epoch:270	batch id:80	 loss:1.644381
[Test] epoch:270	batch id:90	 loss:1.488587
[Test] epoch:270	batch id:100	 loss:2.141263
[Test] epoch:270	batch id:110	 loss:1.290262
[Test] epoch:270	batch id:120	 loss:1.404181
[Test] epoch:270	batch id:130	 loss:1.309693
[Test] epoch:270	batch id:140	 loss:1.442804
[Test] epoch:270	batch id:150	 loss:1.838838
[Test] 270, loss: 1.480928, test acc: 0.914100,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:271	batch id:0	 lr:0.012363 loss:1.372712
[Train] epoch:271	batch id:10	 lr:0.012363 loss:1.398170
[Train] epoch:271	batch id:20	 lr:0.012363 loss:1.329895
[Train] epoch:271	batch id:30	 lr:0.012363 loss:1.373957
[Train] epoch:271	batch id:40	 lr:0.012363 loss:1.332261
[Train] epoch:271	batch id:50	 lr:0.012363 loss:1.344313
[Train] epoch:271	batch id:60	 lr:0.012363 loss:1.435184
[Train] epoch:271	batch id:70	 lr:0.012363 loss:1.328745
[Train] epoch:271	batch id:80	 lr:0.012363 loss:1.331300
[Train] epoch:271	batch id:90	 lr:0.012363 loss:1.346648
[Train] epoch:271	batch id:100	 lr:0.012363 loss:1.333421
[Train] epoch:271	batch id:110	 lr:0.012363 loss:1.386034
[Train] epoch:271	batch id:120	 lr:0.012363 loss:1.404423
[Train] epoch:271	batch id:130	 lr:0.012363 loss:1.422825
[Train] epoch:271	batch id:140	 lr:0.012363 loss:1.355033
[Train] epoch:271	batch id:150	 lr:0.012363 loss:1.389873
[Train] epoch:271	batch id:160	 lr:0.012363 loss:1.437168
[Train] epoch:271	batch id:170	 lr:0.012363 loss:1.350147
[Train] epoch:271	batch id:180	 lr:0.012363 loss:1.352836
[Train] epoch:271	batch id:190	 lr:0.012363 loss:1.374615
[Train] epoch:271	batch id:200	 lr:0.012363 loss:1.341225
[Train] epoch:271	batch id:210	 lr:0.012363 loss:1.331299
[Train] epoch:271	batch id:220	 lr:0.012363 loss:1.372492
[Train] epoch:271	batch id:230	 lr:0.012363 loss:1.344343
[Train] epoch:271	batch id:240	 lr:0.012363 loss:1.365643
[Train] epoch:271	batch id:250	 lr:0.012363 loss:1.338067
[Train] epoch:271	batch id:260	 lr:0.012363 loss:1.337333
[Train] epoch:271	batch id:270	 lr:0.012363 loss:1.354824
[Train] epoch:271	batch id:280	 lr:0.012363 loss:1.463377
[Train] epoch:271	batch id:290	 lr:0.012363 loss:1.358180
[Train] epoch:271	batch id:300	 lr:0.012363 loss:1.419820
[Train] 271, loss: 1.367098, train acc: 0.986869, 
[Test] epoch:271	batch id:0	 loss:1.270313
[Test] epoch:271	batch id:10	 loss:1.575106
[Test] epoch:271	batch id:20	 loss:1.413042
[Test] epoch:271	batch id:30	 loss:1.359798
[Test] epoch:271	batch id:40	 loss:1.587908
[Test] epoch:271	batch id:50	 loss:1.456034
[Test] epoch:271	batch id:60	 loss:1.253570
[Test] epoch:271	batch id:70	 loss:1.447528
[Test] epoch:271	batch id:80	 loss:1.607995
[Test] epoch:271	batch id:90	 loss:1.465985
[Test] epoch:271	batch id:100	 loss:2.007972
[Test] epoch:271	batch id:110	 loss:1.302008
[Test] epoch:271	batch id:120	 loss:1.323068
[Test] epoch:271	batch id:130	 loss:1.305313
[Test] epoch:271	batch id:140	 loss:1.312378
[Test] epoch:271	batch id:150	 loss:1.821848
[Test] 271, loss: 1.470263, test acc: 0.916532,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:272	batch id:0	 lr:0.012082 loss:1.313894
[Train] epoch:272	batch id:10	 lr:0.012082 loss:1.386299
[Train] epoch:272	batch id:20	 lr:0.012082 loss:1.326738
[Train] epoch:272	batch id:30	 lr:0.012082 loss:1.352802
[Train] epoch:272	batch id:40	 lr:0.012082 loss:1.394254
[Train] epoch:272	batch id:50	 lr:0.012082 loss:1.388902
[Train] epoch:272	batch id:60	 lr:0.012082 loss:1.353148
[Train] epoch:272	batch id:70	 lr:0.012082 loss:1.376239
[Train] epoch:272	batch id:80	 lr:0.012082 loss:1.380603
[Train] epoch:272	batch id:90	 lr:0.012082 loss:1.341034
[Train] epoch:272	batch id:100	 lr:0.012082 loss:1.387899
[Train] epoch:272	batch id:110	 lr:0.012082 loss:1.344188
[Train] epoch:272	batch id:120	 lr:0.012082 loss:1.364290
[Train] epoch:272	batch id:130	 lr:0.012082 loss:1.430315
[Train] epoch:272	batch id:140	 lr:0.012082 loss:1.381616
[Train] epoch:272	batch id:150	 lr:0.012082 loss:1.497874
[Train] epoch:272	batch id:160	 lr:0.012082 loss:1.350946
[Train] epoch:272	batch id:170	 lr:0.012082 loss:1.375567
[Train] epoch:272	batch id:180	 lr:0.012082 loss:1.354809
[Train] epoch:272	batch id:190	 lr:0.012082 loss:1.374595
[Train] epoch:272	batch id:200	 lr:0.012082 loss:1.386362
[Train] epoch:272	batch id:210	 lr:0.012082 loss:1.346230
[Train] epoch:272	batch id:220	 lr:0.012082 loss:1.348630
[Train] epoch:272	batch id:230	 lr:0.012082 loss:1.339099
[Train] epoch:272	batch id:240	 lr:0.012082 loss:1.359367
[Train] epoch:272	batch id:250	 lr:0.012082 loss:1.326178
[Train] epoch:272	batch id:260	 lr:0.012082 loss:1.378808
[Train] epoch:272	batch id:270	 lr:0.012082 loss:1.327304
[Train] epoch:272	batch id:280	 lr:0.012082 loss:1.348383
[Train] epoch:272	batch id:290	 lr:0.012082 loss:1.331241
[Train] epoch:272	batch id:300	 lr:0.012082 loss:1.391707
[Train] 272, loss: 1.363883, train acc: 0.988294, 
[Test] epoch:272	batch id:0	 loss:1.276181
[Test] epoch:272	batch id:10	 loss:1.488690
[Test] epoch:272	batch id:20	 loss:1.460175
[Test] epoch:272	batch id:30	 loss:1.373261
[Test] epoch:272	batch id:40	 loss:1.421232
[Test] epoch:272	batch id:50	 loss:1.486502
[Test] epoch:272	batch id:60	 loss:1.286563
[Test] epoch:272	batch id:70	 loss:1.379589
[Test] epoch:272	batch id:80	 loss:1.567190
[Test] epoch:272	batch id:90	 loss:1.613700
[Test] epoch:272	batch id:100	 loss:2.045007
[Test] epoch:272	batch id:110	 loss:1.376934
[Test] epoch:272	batch id:120	 loss:1.312323
[Test] epoch:272	batch id:130	 loss:1.308808
[Test] epoch:272	batch id:140	 loss:1.324518
[Test] epoch:272	batch id:150	 loss:1.776445
[Test] 272, loss: 1.486202, test acc: 0.910049,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:273	batch id:0	 lr:0.011803 loss:1.357391
[Train] epoch:273	batch id:10	 lr:0.011803 loss:1.324182
[Train] epoch:273	batch id:20	 lr:0.011803 loss:1.338400
[Train] epoch:273	batch id:30	 lr:0.011803 loss:1.367488
[Train] epoch:273	batch id:40	 lr:0.011803 loss:1.345411
[Train] epoch:273	batch id:50	 lr:0.011803 loss:1.336954
[Train] epoch:273	batch id:60	 lr:0.011803 loss:1.378623
[Train] epoch:273	batch id:70	 lr:0.011803 loss:1.354804
[Train] epoch:273	batch id:80	 lr:0.011803 loss:1.415359
[Train] epoch:273	batch id:90	 lr:0.011803 loss:1.331486
[Train] epoch:273	batch id:100	 lr:0.011803 loss:1.354342
[Train] epoch:273	batch id:110	 lr:0.011803 loss:1.350096
[Train] epoch:273	batch id:120	 lr:0.011803 loss:1.337123
[Train] epoch:273	batch id:130	 lr:0.011803 loss:1.352122
[Train] epoch:273	batch id:140	 lr:0.011803 loss:1.344404
[Train] epoch:273	batch id:150	 lr:0.011803 loss:1.395261
[Train] epoch:273	batch id:160	 lr:0.011803 loss:1.352598
[Train] epoch:273	batch id:170	 lr:0.011803 loss:1.425079
[Train] epoch:273	batch id:180	 lr:0.011803 loss:1.346763
[Train] epoch:273	batch id:190	 lr:0.011803 loss:1.342979
[Train] epoch:273	batch id:200	 lr:0.011803 loss:1.320251
[Train] epoch:273	batch id:210	 lr:0.011803 loss:1.384260
[Train] epoch:273	batch id:220	 lr:0.011803 loss:1.371490
[Train] epoch:273	batch id:230	 lr:0.011803 loss:1.399792
[Train] epoch:273	batch id:240	 lr:0.011803 loss:1.407838
[Train] epoch:273	batch id:250	 lr:0.011803 loss:1.389827
[Train] epoch:273	batch id:260	 lr:0.011803 loss:1.343211
[Train] epoch:273	batch id:270	 lr:0.011803 loss:1.365587
[Train] epoch:273	batch id:280	 lr:0.011803 loss:1.388087
[Train] epoch:273	batch id:290	 lr:0.011803 loss:1.357917
[Train] epoch:273	batch id:300	 lr:0.011803 loss:1.391274
[Train] 273, loss: 1.367082, train acc: 0.985546, 
[Test] epoch:273	batch id:0	 loss:1.267753
[Test] epoch:273	batch id:10	 loss:1.387411
[Test] epoch:273	batch id:20	 loss:1.462243
[Test] epoch:273	batch id:30	 loss:1.297788
[Test] epoch:273	batch id:40	 loss:1.388081
[Test] epoch:273	batch id:50	 loss:1.455162
[Test] epoch:273	batch id:60	 loss:1.278121
[Test] epoch:273	batch id:70	 loss:1.418288
[Test] epoch:273	batch id:80	 loss:1.689486
[Test] epoch:273	batch id:90	 loss:1.574995
[Test] epoch:273	batch id:100	 loss:1.958452
[Test] epoch:273	batch id:110	 loss:1.305605
[Test] epoch:273	batch id:120	 loss:1.332351
[Test] epoch:273	batch id:130	 loss:1.343370
[Test] epoch:273	batch id:140	 loss:1.400276
[Test] epoch:273	batch id:150	 loss:1.898672
[Test] 273, loss: 1.467563, test acc: 0.918558,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:274	batch id:0	 lr:0.011528 loss:1.340441
[Train] epoch:274	batch id:10	 lr:0.011528 loss:1.320905
[Train] epoch:274	batch id:20	 lr:0.011528 loss:1.366217
[Train] epoch:274	batch id:30	 lr:0.011528 loss:1.372231
[Train] epoch:274	batch id:40	 lr:0.011528 loss:1.412821
[Train] epoch:274	batch id:50	 lr:0.011528 loss:1.335348
[Train] epoch:274	batch id:60	 lr:0.011528 loss:1.419112
[Train] epoch:274	batch id:70	 lr:0.011528 loss:1.392724
[Train] epoch:274	batch id:80	 lr:0.011528 loss:1.495348
[Train] epoch:274	batch id:90	 lr:0.011528 loss:1.390101
[Train] epoch:274	batch id:100	 lr:0.011528 loss:1.333394
[Train] epoch:274	batch id:110	 lr:0.011528 loss:1.325672
[Train] epoch:274	batch id:120	 lr:0.011528 loss:1.340635
[Train] epoch:274	batch id:130	 lr:0.011528 loss:1.325000
[Train] epoch:274	batch id:140	 lr:0.011528 loss:1.366613
[Train] epoch:274	batch id:150	 lr:0.011528 loss:1.345205
[Train] epoch:274	batch id:160	 lr:0.011528 loss:1.404085
[Train] epoch:274	batch id:170	 lr:0.011528 loss:1.379336
[Train] epoch:274	batch id:180	 lr:0.011528 loss:1.364619
[Train] epoch:274	batch id:190	 lr:0.011528 loss:1.373037
[Train] epoch:274	batch id:200	 lr:0.011528 loss:1.336811
[Train] epoch:274	batch id:210	 lr:0.011528 loss:1.322829
[Train] epoch:274	batch id:220	 lr:0.011528 loss:1.358418
[Train] epoch:274	batch id:230	 lr:0.011528 loss:1.347986
[Train] epoch:274	batch id:240	 lr:0.011528 loss:1.325449
[Train] epoch:274	batch id:250	 lr:0.011528 loss:1.356531
[Train] epoch:274	batch id:260	 lr:0.011528 loss:1.380636
[Train] epoch:274	batch id:270	 lr:0.011528 loss:1.360010
[Train] epoch:274	batch id:280	 lr:0.011528 loss:1.330770
[Train] epoch:274	batch id:290	 lr:0.011528 loss:1.353910
[Train] epoch:274	batch id:300	 lr:0.011528 loss:1.323350
[Train] 274, loss: 1.364275, train acc: 0.986869, 
[Test] epoch:274	batch id:0	 loss:1.267408
[Test] epoch:274	batch id:10	 loss:1.421833
[Test] epoch:274	batch id:20	 loss:1.473424
[Test] epoch:274	batch id:30	 loss:1.330084
[Test] epoch:274	batch id:40	 loss:1.461627
[Test] epoch:274	batch id:50	 loss:1.580157
[Test] epoch:274	batch id:60	 loss:1.265058
[Test] epoch:274	batch id:70	 loss:1.450842
[Test] epoch:274	batch id:80	 loss:1.564211
[Test] epoch:274	batch id:90	 loss:1.477332
[Test] epoch:274	batch id:100	 loss:1.769069
[Test] epoch:274	batch id:110	 loss:1.419643
[Test] epoch:274	batch id:120	 loss:1.335644
[Test] epoch:274	batch id:130	 loss:1.377586
[Test] epoch:274	batch id:140	 loss:1.340670
[Test] epoch:274	batch id:150	 loss:1.718890
[Test] 274, loss: 1.468582, test acc: 0.914911,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:275	batch id:0	 lr:0.011255 loss:1.376742
[Train] epoch:275	batch id:10	 lr:0.011255 loss:1.353581
[Train] epoch:275	batch id:20	 lr:0.011255 loss:1.351831
[Train] epoch:275	batch id:30	 lr:0.011255 loss:1.311897
[Train] epoch:275	batch id:40	 lr:0.011255 loss:1.336533
[Train] epoch:275	batch id:50	 lr:0.011255 loss:1.372083
[Train] epoch:275	batch id:60	 lr:0.011255 loss:1.352329
[Train] epoch:275	batch id:70	 lr:0.011255 loss:1.389319
[Train] epoch:275	batch id:80	 lr:0.011255 loss:1.338214
[Train] epoch:275	batch id:90	 lr:0.011255 loss:1.387680
[Train] epoch:275	batch id:100	 lr:0.011255 loss:1.368701
[Train] epoch:275	batch id:110	 lr:0.011255 loss:1.319459
[Train] epoch:275	batch id:120	 lr:0.011255 loss:1.423510
[Train] epoch:275	batch id:130	 lr:0.011255 loss:1.358242
[Train] epoch:275	batch id:140	 lr:0.011255 loss:1.355150
[Train] epoch:275	batch id:150	 lr:0.011255 loss:1.540762
[Train] epoch:275	batch id:160	 lr:0.011255 loss:1.363980
[Train] epoch:275	batch id:170	 lr:0.011255 loss:1.317763
[Train] epoch:275	batch id:180	 lr:0.011255 loss:1.373525
[Train] epoch:275	batch id:190	 lr:0.011255 loss:1.342775
[Train] epoch:275	batch id:200	 lr:0.011255 loss:1.339860
[Train] epoch:275	batch id:210	 lr:0.011255 loss:1.347664
[Train] epoch:275	batch id:220	 lr:0.011255 loss:1.374739
[Train] epoch:275	batch id:230	 lr:0.011255 loss:1.363088
[Train] epoch:275	batch id:240	 lr:0.011255 loss:1.351828
[Train] epoch:275	batch id:250	 lr:0.011255 loss:1.388002
[Train] epoch:275	batch id:260	 lr:0.011255 loss:1.329865
[Train] epoch:275	batch id:270	 lr:0.011255 loss:1.380383
[Train] epoch:275	batch id:280	 lr:0.011255 loss:1.321718
[Train] epoch:275	batch id:290	 lr:0.011255 loss:1.329596
[Train] epoch:275	batch id:300	 lr:0.011255 loss:1.353410
[Train] 275, loss: 1.361695, train acc: 0.988396, 
[Test] epoch:275	batch id:0	 loss:1.295146
[Test] epoch:275	batch id:10	 loss:1.497926
[Test] epoch:275	batch id:20	 loss:1.403810
[Test] epoch:275	batch id:30	 loss:1.355294
[Test] epoch:275	batch id:40	 loss:1.501522
[Test] epoch:275	batch id:50	 loss:1.409360
[Test] epoch:275	batch id:60	 loss:1.270051
[Test] epoch:275	batch id:70	 loss:1.360287
[Test] epoch:275	batch id:80	 loss:1.726285
[Test] epoch:275	batch id:90	 loss:1.652961
[Test] epoch:275	batch id:100	 loss:1.983310
[Test] epoch:275	batch id:110	 loss:1.311497
[Test] epoch:275	batch id:120	 loss:1.293987
[Test] epoch:275	batch id:130	 loss:1.318174
[Test] epoch:275	batch id:140	 loss:1.285602
[Test] epoch:275	batch id:150	 loss:1.739213
[Test] 275, loss: 1.476061, test acc: 0.916126,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:276	batch id:0	 lr:0.010986 loss:1.356324
[Train] epoch:276	batch id:10	 lr:0.010986 loss:1.357610
[Train] epoch:276	batch id:20	 lr:0.010986 loss:1.383282
[Train] epoch:276	batch id:30	 lr:0.010986 loss:1.376254
[Train] epoch:276	batch id:40	 lr:0.010986 loss:1.316468
[Train] epoch:276	batch id:50	 lr:0.010986 loss:1.325408
[Train] epoch:276	batch id:60	 lr:0.010986 loss:1.336006
[Train] epoch:276	batch id:70	 lr:0.010986 loss:1.323180
[Train] epoch:276	batch id:80	 lr:0.010986 loss:1.380253
[Train] epoch:276	batch id:90	 lr:0.010986 loss:1.317758
[Train] epoch:276	batch id:100	 lr:0.010986 loss:1.346414
[Train] epoch:276	batch id:110	 lr:0.010986 loss:1.428154
[Train] epoch:276	batch id:120	 lr:0.010986 loss:1.319514
[Train] epoch:276	batch id:130	 lr:0.010986 loss:1.341328
[Train] epoch:276	batch id:140	 lr:0.010986 loss:1.436463
[Train] epoch:276	batch id:150	 lr:0.010986 loss:1.314207
[Train] epoch:276	batch id:160	 lr:0.010986 loss:1.345762
[Train] epoch:276	batch id:170	 lr:0.010986 loss:1.335048
[Train] epoch:276	batch id:180	 lr:0.010986 loss:1.348797
[Train] epoch:276	batch id:190	 lr:0.010986 loss:1.410047
[Train] epoch:276	batch id:200	 lr:0.010986 loss:1.327720
[Train] epoch:276	batch id:210	 lr:0.010986 loss:1.342541
[Train] epoch:276	batch id:220	 lr:0.010986 loss:1.349391
[Train] epoch:276	batch id:230	 lr:0.010986 loss:1.331586
[Train] epoch:276	batch id:240	 lr:0.010986 loss:1.343241
[Train] epoch:276	batch id:250	 lr:0.010986 loss:1.358727
[Train] epoch:276	batch id:260	 lr:0.010986 loss:1.412939
[Train] epoch:276	batch id:270	 lr:0.010986 loss:1.355305
[Train] epoch:276	batch id:280	 lr:0.010986 loss:1.358563
[Train] epoch:276	batch id:290	 lr:0.010986 loss:1.327056
[Train] epoch:276	batch id:300	 lr:0.010986 loss:1.380949
[Train] 276, loss: 1.364994, train acc: 0.988599, 
[Test] epoch:276	batch id:0	 loss:1.277881
[Test] epoch:276	batch id:10	 loss:1.514577
[Test] epoch:276	batch id:20	 loss:1.436004
[Test] epoch:276	batch id:30	 loss:1.328099
[Test] epoch:276	batch id:40	 loss:1.436749
[Test] epoch:276	batch id:50	 loss:1.507346
[Test] epoch:276	batch id:60	 loss:1.302943
[Test] epoch:276	batch id:70	 loss:1.320367
[Test] epoch:276	batch id:80	 loss:1.616292
[Test] epoch:276	batch id:90	 loss:1.590972
[Test] epoch:276	batch id:100	 loss:2.020847
[Test] epoch:276	batch id:110	 loss:1.320764
[Test] epoch:276	batch id:120	 loss:1.308111
[Test] epoch:276	batch id:130	 loss:1.335455
[Test] epoch:276	batch id:140	 loss:1.315779
[Test] epoch:276	batch id:150	 loss:1.634280
[Test] 276, loss: 1.480367, test acc: 0.918152,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:277	batch id:0	 lr:0.010720 loss:1.398399
[Train] epoch:277	batch id:10	 lr:0.010720 loss:1.334882
[Train] epoch:277	batch id:20	 lr:0.010720 loss:1.357062
[Train] epoch:277	batch id:30	 lr:0.010720 loss:1.338771
[Train] epoch:277	batch id:40	 lr:0.010720 loss:1.333274
[Train] epoch:277	batch id:50	 lr:0.010720 loss:1.407911
[Train] epoch:277	batch id:60	 lr:0.010720 loss:1.348407
[Train] epoch:277	batch id:70	 lr:0.010720 loss:1.395809
[Train] epoch:277	batch id:80	 lr:0.010720 loss:1.377382
[Train] epoch:277	batch id:90	 lr:0.010720 loss:1.320334
[Train] epoch:277	batch id:100	 lr:0.010720 loss:1.370812
[Train] epoch:277	batch id:110	 lr:0.010720 loss:1.372948
[Train] epoch:277	batch id:120	 lr:0.010720 loss:1.380838
[Train] epoch:277	batch id:130	 lr:0.010720 loss:1.351230
[Train] epoch:277	batch id:140	 lr:0.010720 loss:1.369374
[Train] epoch:277	batch id:150	 lr:0.010720 loss:1.300969
[Train] epoch:277	batch id:160	 lr:0.010720 loss:1.412542
[Train] epoch:277	batch id:170	 lr:0.010720 loss:1.311277
[Train] epoch:277	batch id:180	 lr:0.010720 loss:1.350723
[Train] epoch:277	batch id:190	 lr:0.010720 loss:1.369238
[Train] epoch:277	batch id:200	 lr:0.010720 loss:1.331686
[Train] epoch:277	batch id:210	 lr:0.010720 loss:1.316166
[Train] epoch:277	batch id:220	 lr:0.010720 loss:1.359872
[Train] epoch:277	batch id:230	 lr:0.010720 loss:1.398955
[Train] epoch:277	batch id:240	 lr:0.010720 loss:1.427642
[Train] epoch:277	batch id:250	 lr:0.010720 loss:1.341406
[Train] epoch:277	batch id:260	 lr:0.010720 loss:1.392036
[Train] epoch:277	batch id:270	 lr:0.010720 loss:1.350235
[Train] epoch:277	batch id:280	 lr:0.010720 loss:1.331596
[Train] epoch:277	batch id:290	 lr:0.010720 loss:1.322512
[Train] epoch:277	batch id:300	 lr:0.010720 loss:1.336650
[Train] 277, loss: 1.360474, train acc: 0.988396, 
[Test] epoch:277	batch id:0	 loss:1.302493
[Test] epoch:277	batch id:10	 loss:1.413125
[Test] epoch:277	batch id:20	 loss:1.416339
[Test] epoch:277	batch id:30	 loss:1.422860
[Test] epoch:277	batch id:40	 loss:1.431472
[Test] epoch:277	batch id:50	 loss:1.459072
[Test] epoch:277	batch id:60	 loss:1.261674
[Test] epoch:277	batch id:70	 loss:1.412983
[Test] epoch:277	batch id:80	 loss:1.565413
[Test] epoch:277	batch id:90	 loss:1.524324
[Test] epoch:277	batch id:100	 loss:2.003647
[Test] epoch:277	batch id:110	 loss:1.333776
[Test] epoch:277	batch id:120	 loss:1.289618
[Test] epoch:277	batch id:130	 loss:1.352744
[Test] epoch:277	batch id:140	 loss:1.293493
[Test] epoch:277	batch id:150	 loss:1.667165
[Test] 277, loss: 1.462161, test acc: 0.918963,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:278	batch id:0	 lr:0.010458 loss:1.324756
[Train] epoch:278	batch id:10	 lr:0.010458 loss:1.363646
[Train] epoch:278	batch id:20	 lr:0.010458 loss:1.369967
[Train] epoch:278	batch id:30	 lr:0.010458 loss:1.446540
[Train] epoch:278	batch id:40	 lr:0.010458 loss:1.395106
[Train] epoch:278	batch id:50	 lr:0.010458 loss:1.339962
[Train] epoch:278	batch id:60	 lr:0.010458 loss:1.391554
[Train] epoch:278	batch id:70	 lr:0.010458 loss:1.435554
[Train] epoch:278	batch id:80	 lr:0.010458 loss:1.356751
[Train] epoch:278	batch id:90	 lr:0.010458 loss:1.406842
[Train] epoch:278	batch id:100	 lr:0.010458 loss:1.436378
[Train] epoch:278	batch id:110	 lr:0.010458 loss:1.366084
[Train] epoch:278	batch id:120	 lr:0.010458 loss:1.327975
[Train] epoch:278	batch id:130	 lr:0.010458 loss:1.392956
[Train] epoch:278	batch id:140	 lr:0.010458 loss:1.322535
[Train] epoch:278	batch id:150	 lr:0.010458 loss:1.385173
[Train] epoch:278	batch id:160	 lr:0.010458 loss:1.397056
[Train] epoch:278	batch id:170	 lr:0.010458 loss:1.373310
[Train] epoch:278	batch id:180	 lr:0.010458 loss:1.341600
[Train] epoch:278	batch id:190	 lr:0.010458 loss:1.316521
[Train] epoch:278	batch id:200	 lr:0.010458 loss:1.367409
[Train] epoch:278	batch id:210	 lr:0.010458 loss:1.409357
[Train] epoch:278	batch id:220	 lr:0.010458 loss:1.312216
[Train] epoch:278	batch id:230	 lr:0.010458 loss:1.384336
[Train] epoch:278	batch id:240	 lr:0.010458 loss:1.360035
[Train] epoch:278	batch id:250	 lr:0.010458 loss:1.354746
[Train] epoch:278	batch id:260	 lr:0.010458 loss:1.399365
[Train] epoch:278	batch id:270	 lr:0.010458 loss:1.353543
[Train] epoch:278	batch id:280	 lr:0.010458 loss:1.355439
[Train] epoch:278	batch id:290	 lr:0.010458 loss:1.373760
[Train] epoch:278	batch id:300	 lr:0.010458 loss:1.320833
[Train] 278, loss: 1.363437, train acc: 0.986360, 
[Test] epoch:278	batch id:0	 loss:1.257688
[Test] epoch:278	batch id:10	 loss:1.567791
[Test] epoch:278	batch id:20	 loss:1.503493
[Test] epoch:278	batch id:30	 loss:1.404151
[Test] epoch:278	batch id:40	 loss:1.414428
[Test] epoch:278	batch id:50	 loss:1.481703
[Test] epoch:278	batch id:60	 loss:1.253105
[Test] epoch:278	batch id:70	 loss:1.332408
[Test] epoch:278	batch id:80	 loss:1.596672
[Test] epoch:278	batch id:90	 loss:1.447423
[Test] epoch:278	batch id:100	 loss:1.970686
[Test] epoch:278	batch id:110	 loss:1.323659
[Test] epoch:278	batch id:120	 loss:1.319539
[Test] epoch:278	batch id:130	 loss:1.328775
[Test] epoch:278	batch id:140	 loss:1.361727
[Test] epoch:278	batch id:150	 loss:1.645106
[Test] 278, loss: 1.464709, test acc: 0.920583,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:279	batch id:0	 lr:0.010198 loss:1.376800
[Train] epoch:279	batch id:10	 lr:0.010198 loss:1.338653
[Train] epoch:279	batch id:20	 lr:0.010198 loss:1.367923
[Train] epoch:279	batch id:30	 lr:0.010198 loss:1.321640
[Train] epoch:279	batch id:40	 lr:0.010198 loss:1.356668
[Train] epoch:279	batch id:50	 lr:0.010198 loss:1.316444
[Train] epoch:279	batch id:60	 lr:0.010198 loss:1.426262
[Train] epoch:279	batch id:70	 lr:0.010198 loss:1.314106
[Train] epoch:279	batch id:80	 lr:0.010198 loss:1.365227
[Train] epoch:279	batch id:90	 lr:0.010198 loss:1.364313
[Train] epoch:279	batch id:100	 lr:0.010198 loss:1.472908
[Train] epoch:279	batch id:110	 lr:0.010198 loss:1.371232
[Train] epoch:279	batch id:120	 lr:0.010198 loss:1.366628
[Train] epoch:279	batch id:130	 lr:0.010198 loss:1.355144
[Train] epoch:279	batch id:140	 lr:0.010198 loss:1.328139
[Train] epoch:279	batch id:150	 lr:0.010198 loss:1.347283
[Train] epoch:279	batch id:160	 lr:0.010198 loss:1.461211
[Train] epoch:279	batch id:170	 lr:0.010198 loss:1.359818
[Train] epoch:279	batch id:180	 lr:0.010198 loss:1.334241
[Train] epoch:279	batch id:190	 lr:0.010198 loss:1.338838
[Train] epoch:279	batch id:200	 lr:0.010198 loss:1.381719
[Train] epoch:279	batch id:210	 lr:0.010198 loss:1.405157
[Train] epoch:279	batch id:220	 lr:0.010198 loss:1.339707
[Train] epoch:279	batch id:230	 lr:0.010198 loss:1.323789
[Train] epoch:279	batch id:240	 lr:0.010198 loss:1.418383
[Train] epoch:279	batch id:250	 lr:0.010198 loss:1.425763
[Train] epoch:279	batch id:260	 lr:0.010198 loss:1.341740
[Train] epoch:279	batch id:270	 lr:0.010198 loss:1.386461
[Train] epoch:279	batch id:280	 lr:0.010198 loss:1.335384
[Train] epoch:279	batch id:290	 lr:0.010198 loss:1.317452
[Train] epoch:279	batch id:300	 lr:0.010198 loss:1.390057
[Train] 279, loss: 1.362499, train acc: 0.987480, 
[Test] epoch:279	batch id:0	 loss:1.268510
[Test] epoch:279	batch id:10	 loss:1.494820
[Test] epoch:279	batch id:20	 loss:1.496359
[Test] epoch:279	batch id:30	 loss:1.327806
[Test] epoch:279	batch id:40	 loss:1.353595
[Test] epoch:279	batch id:50	 loss:1.437245
[Test] epoch:279	batch id:60	 loss:1.271348
[Test] epoch:279	batch id:70	 loss:1.433399
[Test] epoch:279	batch id:80	 loss:1.597661
[Test] epoch:279	batch id:90	 loss:1.451572
[Test] epoch:279	batch id:100	 loss:1.846421
[Test] epoch:279	batch id:110	 loss:1.338848
[Test] epoch:279	batch id:120	 loss:1.318263
[Test] epoch:279	batch id:130	 loss:1.350783
[Test] epoch:279	batch id:140	 loss:1.437030
[Test] epoch:279	batch id:150	 loss:1.704903
[Test] 279, loss: 1.472406, test acc: 0.917342,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:280	batch id:0	 lr:0.009942 loss:1.354970
[Train] epoch:280	batch id:10	 lr:0.009942 loss:1.333164
[Train] epoch:280	batch id:20	 lr:0.009942 loss:1.335844
[Train] epoch:280	batch id:30	 lr:0.009942 loss:1.346892
[Train] epoch:280	batch id:40	 lr:0.009942 loss:1.331869
[Train] epoch:280	batch id:50	 lr:0.009942 loss:1.346606
[Train] epoch:280	batch id:60	 lr:0.009942 loss:1.440364
[Train] epoch:280	batch id:70	 lr:0.009942 loss:1.361205
[Train] epoch:280	batch id:80	 lr:0.009942 loss:1.335307
[Train] epoch:280	batch id:90	 lr:0.009942 loss:1.373255
[Train] epoch:280	batch id:100	 lr:0.009942 loss:1.314137
[Train] epoch:280	batch id:110	 lr:0.009942 loss:1.393888
[Train] epoch:280	batch id:120	 lr:0.009942 loss:1.359237
[Train] epoch:280	batch id:130	 lr:0.009942 loss:1.347203
[Train] epoch:280	batch id:140	 lr:0.009942 loss:1.332648
[Train] epoch:280	batch id:150	 lr:0.009942 loss:1.372082
[Train] epoch:280	batch id:160	 lr:0.009942 loss:1.405595
[Train] epoch:280	batch id:170	 lr:0.009942 loss:1.370173
[Train] epoch:280	batch id:180	 lr:0.009942 loss:1.311250
[Train] epoch:280	batch id:190	 lr:0.009942 loss:1.394643
[Train] epoch:280	batch id:200	 lr:0.009942 loss:1.316772
[Train] epoch:280	batch id:210	 lr:0.009942 loss:1.388270
[Train] epoch:280	batch id:220	 lr:0.009942 loss:1.406706
[Train] epoch:280	batch id:230	 lr:0.009942 loss:1.334486
[Train] epoch:280	batch id:240	 lr:0.009942 loss:1.320979
[Train] epoch:280	batch id:250	 lr:0.009942 loss:1.370607
[Train] epoch:280	batch id:260	 lr:0.009942 loss:1.306570
[Train] epoch:280	batch id:270	 lr:0.009942 loss:1.340086
[Train] epoch:280	batch id:280	 lr:0.009942 loss:1.334739
[Train] epoch:280	batch id:290	 lr:0.009942 loss:1.339077
[Train] epoch:280	batch id:300	 lr:0.009942 loss:1.405006
[Train] 280, loss: 1.358172, train acc: 0.988803, 
[Test] epoch:280	batch id:0	 loss:1.260578
[Test] epoch:280	batch id:10	 loss:1.504329
[Test] epoch:280	batch id:20	 loss:1.463611
[Test] epoch:280	batch id:30	 loss:1.410167
[Test] epoch:280	batch id:40	 loss:1.542747
[Test] epoch:280	batch id:50	 loss:1.618116
[Test] epoch:280	batch id:60	 loss:1.271333
[Test] epoch:280	batch id:70	 loss:1.339400
[Test] epoch:280	batch id:80	 loss:1.697923
[Test] epoch:280	batch id:90	 loss:1.644796
[Test] epoch:280	batch id:100	 loss:1.970537
[Test] epoch:280	batch id:110	 loss:1.331219
[Test] epoch:280	batch id:120	 loss:1.407844
[Test] epoch:280	batch id:130	 loss:1.314436
[Test] epoch:280	batch id:140	 loss:1.442544
[Test] epoch:280	batch id:150	 loss:1.783239
[Test] 280, loss: 1.504675, test acc: 0.907212,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:281	batch id:0	 lr:0.009689 loss:1.313972
[Train] epoch:281	batch id:10	 lr:0.009689 loss:1.318012
[Train] epoch:281	batch id:20	 lr:0.009689 loss:1.324517
[Train] epoch:281	batch id:30	 lr:0.009689 loss:1.416077
[Train] epoch:281	batch id:40	 lr:0.009689 loss:1.346621
[Train] epoch:281	batch id:50	 lr:0.009689 loss:1.358393
[Train] epoch:281	batch id:60	 lr:0.009689 loss:1.355314
[Train] epoch:281	batch id:70	 lr:0.009689 loss:1.370876
[Train] epoch:281	batch id:80	 lr:0.009689 loss:1.369633
[Train] epoch:281	batch id:90	 lr:0.009689 loss:1.374643
[Train] epoch:281	batch id:100	 lr:0.009689 loss:1.327625
[Train] epoch:281	batch id:110	 lr:0.009689 loss:1.335961
[Train] epoch:281	batch id:120	 lr:0.009689 loss:1.364220
[Train] epoch:281	batch id:130	 lr:0.009689 loss:1.395317
[Train] epoch:281	batch id:140	 lr:0.009689 loss:1.313723
[Train] epoch:281	batch id:150	 lr:0.009689 loss:1.388342
[Train] epoch:281	batch id:160	 lr:0.009689 loss:1.401197
[Train] epoch:281	batch id:170	 lr:0.009689 loss:1.323737
[Train] epoch:281	batch id:180	 lr:0.009689 loss:1.335247
[Train] epoch:281	batch id:190	 lr:0.009689 loss:1.370998
[Train] epoch:281	batch id:200	 lr:0.009689 loss:1.349230
[Train] epoch:281	batch id:210	 lr:0.009689 loss:1.330957
[Train] epoch:281	batch id:220	 lr:0.009689 loss:1.370222
[Train] epoch:281	batch id:230	 lr:0.009689 loss:1.323305
[Train] epoch:281	batch id:240	 lr:0.009689 loss:1.363720
[Train] epoch:281	batch id:250	 lr:0.009689 loss:1.354553
[Train] epoch:281	batch id:260	 lr:0.009689 loss:1.337700
[Train] epoch:281	batch id:270	 lr:0.009689 loss:1.341314
[Train] epoch:281	batch id:280	 lr:0.009689 loss:1.348291
[Train] epoch:281	batch id:290	 lr:0.009689 loss:1.338499
[Train] epoch:281	batch id:300	 lr:0.009689 loss:1.300224
[Train] 281, loss: 1.355965, train acc: 0.990330, 
[Test] epoch:281	batch id:0	 loss:1.298604
[Test] epoch:281	batch id:10	 loss:1.487229
[Test] epoch:281	batch id:20	 loss:1.446109
[Test] epoch:281	batch id:30	 loss:1.424917
[Test] epoch:281	batch id:40	 loss:1.466167
[Test] epoch:281	batch id:50	 loss:1.480192
[Test] epoch:281	batch id:60	 loss:1.269370
[Test] epoch:281	batch id:70	 loss:1.455512
[Test] epoch:281	batch id:80	 loss:1.678751
[Test] epoch:281	batch id:90	 loss:1.507354
[Test] epoch:281	batch id:100	 loss:1.882469
[Test] epoch:281	batch id:110	 loss:1.331262
[Test] epoch:281	batch id:120	 loss:1.311940
[Test] epoch:281	batch id:130	 loss:1.353525
[Test] epoch:281	batch id:140	 loss:1.328917
[Test] epoch:281	batch id:150	 loss:1.775090
[Test] 281, loss: 1.478621, test acc: 0.918152,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:282	batch id:0	 lr:0.009439 loss:1.346007
[Train] epoch:282	batch id:10	 lr:0.009439 loss:1.394764
[Train] epoch:282	batch id:20	 lr:0.009439 loss:1.316943
[Train] epoch:282	batch id:30	 lr:0.009439 loss:1.424576
[Train] epoch:282	batch id:40	 lr:0.009439 loss:1.396705
[Train] epoch:282	batch id:50	 lr:0.009439 loss:1.350610
[Train] epoch:282	batch id:60	 lr:0.009439 loss:1.346685
[Train] epoch:282	batch id:70	 lr:0.009439 loss:1.350750
[Train] epoch:282	batch id:80	 lr:0.009439 loss:1.343671
[Train] epoch:282	batch id:90	 lr:0.009439 loss:1.332981
[Train] epoch:282	batch id:100	 lr:0.009439 loss:1.365000
[Train] epoch:282	batch id:110	 lr:0.009439 loss:1.348009
[Train] epoch:282	batch id:120	 lr:0.009439 loss:1.345540
[Train] epoch:282	batch id:130	 lr:0.009439 loss:1.343603
[Train] epoch:282	batch id:140	 lr:0.009439 loss:1.456540
[Train] epoch:282	batch id:150	 lr:0.009439 loss:1.378538
[Train] epoch:282	batch id:160	 lr:0.009439 loss:1.428052
[Train] epoch:282	batch id:170	 lr:0.009439 loss:1.323446
[Train] epoch:282	batch id:180	 lr:0.009439 loss:1.360021
[Train] epoch:282	batch id:190	 lr:0.009439 loss:1.309439
[Train] epoch:282	batch id:200	 lr:0.009439 loss:1.313918
[Train] epoch:282	batch id:210	 lr:0.009439 loss:1.338278
[Train] epoch:282	batch id:220	 lr:0.009439 loss:1.338520
[Train] epoch:282	batch id:230	 lr:0.009439 loss:1.358192
[Train] epoch:282	batch id:240	 lr:0.009439 loss:1.386554
[Train] epoch:282	batch id:250	 lr:0.009439 loss:1.347052
[Train] epoch:282	batch id:260	 lr:0.009439 loss:1.378374
[Train] epoch:282	batch id:270	 lr:0.009439 loss:1.384568
[Train] epoch:282	batch id:280	 lr:0.009439 loss:1.430357
[Train] epoch:282	batch id:290	 lr:0.009439 loss:1.366107
[Train] epoch:282	batch id:300	 lr:0.009439 loss:1.328044
[Train] 282, loss: 1.355242, train acc: 0.990330, 
[Test] epoch:282	batch id:0	 loss:1.277477
[Test] epoch:282	batch id:10	 loss:1.470193
[Test] epoch:282	batch id:20	 loss:1.391724
[Test] epoch:282	batch id:30	 loss:1.417778
[Test] epoch:282	batch id:40	 loss:1.491500
[Test] epoch:282	batch id:50	 loss:1.574718
[Test] epoch:282	batch id:60	 loss:1.270358
[Test] epoch:282	batch id:70	 loss:1.426927
[Test] epoch:282	batch id:80	 loss:1.593323
[Test] epoch:282	batch id:90	 loss:1.659360
[Test] epoch:282	batch id:100	 loss:1.912976
[Test] epoch:282	batch id:110	 loss:1.390041
[Test] epoch:282	batch id:120	 loss:1.348008
[Test] epoch:282	batch id:130	 loss:1.475727
[Test] epoch:282	batch id:140	 loss:1.334695
[Test] epoch:282	batch id:150	 loss:1.777631
[Test] 282, loss: 1.489472, test acc: 0.914100,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:283	batch id:0	 lr:0.009192 loss:1.352219
[Train] epoch:283	batch id:10	 lr:0.009192 loss:1.337906
[Train] epoch:283	batch id:20	 lr:0.009192 loss:1.327408
[Train] epoch:283	batch id:30	 lr:0.009192 loss:1.366198
[Train] epoch:283	batch id:40	 lr:0.009192 loss:1.398834
[Train] epoch:283	batch id:50	 lr:0.009192 loss:1.327200
[Train] epoch:283	batch id:60	 lr:0.009192 loss:1.346088
[Train] epoch:283	batch id:70	 lr:0.009192 loss:1.332497
[Train] epoch:283	batch id:80	 lr:0.009192 loss:1.363911
[Train] epoch:283	batch id:90	 lr:0.009192 loss:1.422028
[Train] epoch:283	batch id:100	 lr:0.009192 loss:1.361440
[Train] epoch:283	batch id:110	 lr:0.009192 loss:1.417400
[Train] epoch:283	batch id:120	 lr:0.009192 loss:1.359183
[Train] epoch:283	batch id:130	 lr:0.009192 loss:1.437409
[Train] epoch:283	batch id:140	 lr:0.009192 loss:1.318039
[Train] epoch:283	batch id:150	 lr:0.009192 loss:1.385874
[Train] epoch:283	batch id:160	 lr:0.009192 loss:1.465180
[Train] epoch:283	batch id:170	 lr:0.009192 loss:1.410480
[Train] epoch:283	batch id:180	 lr:0.009192 loss:1.354015
[Train] epoch:283	batch id:190	 lr:0.009192 loss:1.437262
[Train] epoch:283	batch id:200	 lr:0.009192 loss:1.308976
[Train] epoch:283	batch id:210	 lr:0.009192 loss:1.369123
[Train] epoch:283	batch id:220	 lr:0.009192 loss:1.327867
[Train] epoch:283	batch id:230	 lr:0.009192 loss:1.332399
[Train] epoch:283	batch id:240	 lr:0.009192 loss:1.390913
[Train] epoch:283	batch id:250	 lr:0.009192 loss:1.393012
[Train] epoch:283	batch id:260	 lr:0.009192 loss:1.402291
[Train] epoch:283	batch id:270	 lr:0.009192 loss:1.399232
[Train] epoch:283	batch id:280	 lr:0.009192 loss:1.371536
[Train] epoch:283	batch id:290	 lr:0.009192 loss:1.364862
[Train] epoch:283	batch id:300	 lr:0.009192 loss:1.377207
[Train] 283, loss: 1.357997, train acc: 0.989312, 
[Test] epoch:283	batch id:0	 loss:1.283715
[Test] epoch:283	batch id:10	 loss:1.436831
[Test] epoch:283	batch id:20	 loss:1.377504
[Test] epoch:283	batch id:30	 loss:1.308104
[Test] epoch:283	batch id:40	 loss:1.498791
[Test] epoch:283	batch id:50	 loss:1.450801
[Test] epoch:283	batch id:60	 loss:1.272629
[Test] epoch:283	batch id:70	 loss:1.397142
[Test] epoch:283	batch id:80	 loss:1.636420
[Test] epoch:283	batch id:90	 loss:1.642667
[Test] epoch:283	batch id:100	 loss:1.987817
[Test] epoch:283	batch id:110	 loss:1.319707
[Test] epoch:283	batch id:120	 loss:1.297771
[Test] epoch:283	batch id:130	 loss:1.292956
[Test] epoch:283	batch id:140	 loss:1.372560
[Test] epoch:283	batch id:150	 loss:1.761214
[Test] 283, loss: 1.477973, test acc: 0.918152,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:284	batch id:0	 lr:0.008949 loss:1.364924
[Train] epoch:284	batch id:10	 lr:0.008949 loss:1.428207
[Train] epoch:284	batch id:20	 lr:0.008949 loss:1.331709
[Train] epoch:284	batch id:30	 lr:0.008949 loss:1.343593
[Train] epoch:284	batch id:40	 lr:0.008949 loss:1.349435
[Train] epoch:284	batch id:50	 lr:0.008949 loss:1.397139
[Train] epoch:284	batch id:60	 lr:0.008949 loss:1.328483
[Train] epoch:284	batch id:70	 lr:0.008949 loss:1.325572
[Train] epoch:284	batch id:80	 lr:0.008949 loss:1.350959
[Train] epoch:284	batch id:90	 lr:0.008949 loss:1.319246
[Train] epoch:284	batch id:100	 lr:0.008949 loss:1.386388
[Train] epoch:284	batch id:110	 lr:0.008949 loss:1.396555
[Train] epoch:284	batch id:120	 lr:0.008949 loss:1.323104
[Train] epoch:284	batch id:130	 lr:0.008949 loss:1.397692
[Train] epoch:284	batch id:140	 lr:0.008949 loss:1.372861
[Train] epoch:284	batch id:150	 lr:0.008949 loss:1.384771
[Train] epoch:284	batch id:160	 lr:0.008949 loss:1.381441
[Train] epoch:284	batch id:170	 lr:0.008949 loss:1.318012
[Train] epoch:284	batch id:180	 lr:0.008949 loss:1.313773
[Train] epoch:284	batch id:190	 lr:0.008949 loss:1.363029
[Train] epoch:284	batch id:200	 lr:0.008949 loss:1.368308
[Train] epoch:284	batch id:210	 lr:0.008949 loss:1.373035
[Train] epoch:284	batch id:220	 lr:0.008949 loss:1.381258
[Train] epoch:284	batch id:230	 lr:0.008949 loss:1.334571
[Train] epoch:284	batch id:240	 lr:0.008949 loss:1.338237
[Train] epoch:284	batch id:250	 lr:0.008949 loss:1.388733
[Train] epoch:284	batch id:260	 lr:0.008949 loss:1.397370
[Train] epoch:284	batch id:270	 lr:0.008949 loss:1.453371
[Train] epoch:284	batch id:280	 lr:0.008949 loss:1.373228
[Train] epoch:284	batch id:290	 lr:0.008949 loss:1.333967
[Train] epoch:284	batch id:300	 lr:0.008949 loss:1.301538
[Train] 284, loss: 1.357470, train acc: 0.989719, 
[Test] epoch:284	batch id:0	 loss:1.280633
[Test] epoch:284	batch id:10	 loss:1.464137
[Test] epoch:284	batch id:20	 loss:1.401966
[Test] epoch:284	batch id:30	 loss:1.318244
[Test] epoch:284	batch id:40	 loss:1.456767
[Test] epoch:284	batch id:50	 loss:1.413789
[Test] epoch:284	batch id:60	 loss:1.262058
[Test] epoch:284	batch id:70	 loss:1.339867
[Test] epoch:284	batch id:80	 loss:1.599533
[Test] epoch:284	batch id:90	 loss:1.687490
[Test] epoch:284	batch id:100	 loss:1.985128
[Test] epoch:284	batch id:110	 loss:1.296162
[Test] epoch:284	batch id:120	 loss:1.342368
[Test] epoch:284	batch id:130	 loss:1.316512
[Test] epoch:284	batch id:140	 loss:1.373199
[Test] epoch:284	batch id:150	 loss:1.864209
[Test] 284, loss: 1.468928, test acc: 0.918152,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:285	batch id:0	 lr:0.008710 loss:1.370777
[Train] epoch:285	batch id:10	 lr:0.008710 loss:1.315087
[Train] epoch:285	batch id:20	 lr:0.008710 loss:1.353728
[Train] epoch:285	batch id:30	 lr:0.008710 loss:1.351016
[Train] epoch:285	batch id:40	 lr:0.008710 loss:1.360243
[Train] epoch:285	batch id:50	 lr:0.008710 loss:1.368848
[Train] epoch:285	batch id:60	 lr:0.008710 loss:1.331421
[Train] epoch:285	batch id:70	 lr:0.008710 loss:1.354144
[Train] epoch:285	batch id:80	 lr:0.008710 loss:1.364330
[Train] epoch:285	batch id:90	 lr:0.008710 loss:1.444109
[Train] epoch:285	batch id:100	 lr:0.008710 loss:1.376922
[Train] epoch:285	batch id:110	 lr:0.008710 loss:1.346033
[Train] epoch:285	batch id:120	 lr:0.008710 loss:1.366181
[Train] epoch:285	batch id:130	 lr:0.008710 loss:1.412936
[Train] epoch:285	batch id:140	 lr:0.008710 loss:1.365495
[Train] epoch:285	batch id:150	 lr:0.008710 loss:1.333512
[Train] epoch:285	batch id:160	 lr:0.008710 loss:1.321329
[Train] epoch:285	batch id:170	 lr:0.008710 loss:1.364688
[Train] epoch:285	batch id:180	 lr:0.008710 loss:1.379823
[Train] epoch:285	batch id:190	 lr:0.008710 loss:1.332063
[Train] epoch:285	batch id:200	 lr:0.008710 loss:1.318322
[Train] epoch:285	batch id:210	 lr:0.008710 loss:1.367754
[Train] epoch:285	batch id:220	 lr:0.008710 loss:1.323883
[Train] epoch:285	batch id:230	 lr:0.008710 loss:1.318909
[Train] epoch:285	batch id:240	 lr:0.008710 loss:1.345179
[Train] epoch:285	batch id:250	 lr:0.008710 loss:1.351403
[Train] epoch:285	batch id:260	 lr:0.008710 loss:1.402413
[Train] epoch:285	batch id:270	 lr:0.008710 loss:1.379125
[Train] epoch:285	batch id:280	 lr:0.008710 loss:1.364651
[Train] epoch:285	batch id:290	 lr:0.008710 loss:1.320324
[Train] epoch:285	batch id:300	 lr:0.008710 loss:1.374838
[Train] 285, loss: 1.355767, train acc: 0.989617, 
[Test] epoch:285	batch id:0	 loss:1.287435
[Test] epoch:285	batch id:10	 loss:1.503651
[Test] epoch:285	batch id:20	 loss:1.523953
[Test] epoch:285	batch id:30	 loss:1.358897
[Test] epoch:285	batch id:40	 loss:1.546835
[Test] epoch:285	batch id:50	 loss:1.519807
[Test] epoch:285	batch id:60	 loss:1.264475
[Test] epoch:285	batch id:70	 loss:1.469346
[Test] epoch:285	batch id:80	 loss:1.623016
[Test] epoch:285	batch id:90	 loss:1.655675
[Test] epoch:285	batch id:100	 loss:1.835438
[Test] epoch:285	batch id:110	 loss:1.387201
[Test] epoch:285	batch id:120	 loss:1.360795
[Test] epoch:285	batch id:130	 loss:1.344874
[Test] epoch:285	batch id:140	 loss:1.394881
[Test] epoch:285	batch id:150	 loss:1.851985
[Test] 285, loss: 1.484386, test acc: 0.909238,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:286	batch id:0	 lr:0.008473 loss:1.368150
[Train] epoch:286	batch id:10	 lr:0.008473 loss:1.428346
[Train] epoch:286	batch id:20	 lr:0.008473 loss:1.447286
[Train] epoch:286	batch id:30	 lr:0.008473 loss:1.332019
[Train] epoch:286	batch id:40	 lr:0.008473 loss:1.360234
[Train] epoch:286	batch id:50	 lr:0.008473 loss:1.342391
[Train] epoch:286	batch id:60	 lr:0.008473 loss:1.414738
[Train] epoch:286	batch id:70	 lr:0.008473 loss:1.340832
[Train] epoch:286	batch id:80	 lr:0.008473 loss:1.337323
[Train] epoch:286	batch id:90	 lr:0.008473 loss:1.375385
[Train] epoch:286	batch id:100	 lr:0.008473 loss:1.318081
[Train] epoch:286	batch id:110	 lr:0.008473 loss:1.367202
[Train] epoch:286	batch id:120	 lr:0.008473 loss:1.317731
[Train] epoch:286	batch id:130	 lr:0.008473 loss:1.345211
[Train] epoch:286	batch id:140	 lr:0.008473 loss:1.366513
[Train] epoch:286	batch id:150	 lr:0.008473 loss:1.353650
[Train] epoch:286	batch id:160	 lr:0.008473 loss:1.350220
[Train] epoch:286	batch id:170	 lr:0.008473 loss:1.328129
[Train] epoch:286	batch id:180	 lr:0.008473 loss:1.313405
[Train] epoch:286	batch id:190	 lr:0.008473 loss:1.324766
[Train] epoch:286	batch id:200	 lr:0.008473 loss:1.368989
[Train] epoch:286	batch id:210	 lr:0.008473 loss:1.355953
[Train] epoch:286	batch id:220	 lr:0.008473 loss:1.340216
[Train] epoch:286	batch id:230	 lr:0.008473 loss:1.339440
[Train] epoch:286	batch id:240	 lr:0.008473 loss:1.394154
[Train] epoch:286	batch id:250	 lr:0.008473 loss:1.328429
[Train] epoch:286	batch id:260	 lr:0.008473 loss:1.374033
[Train] epoch:286	batch id:270	 lr:0.008473 loss:1.398848
[Train] epoch:286	batch id:280	 lr:0.008473 loss:1.354631
[Train] epoch:286	batch id:290	 lr:0.008473 loss:1.323358
[Train] epoch:286	batch id:300	 lr:0.008473 loss:1.368563
[Train] 286, loss: 1.353842, train acc: 0.990839, 
[Test] epoch:286	batch id:0	 loss:1.285407
[Test] epoch:286	batch id:10	 loss:1.477692
[Test] epoch:286	batch id:20	 loss:1.452294
[Test] epoch:286	batch id:30	 loss:1.382588
[Test] epoch:286	batch id:40	 loss:1.544770
[Test] epoch:286	batch id:50	 loss:1.525280
[Test] epoch:286	batch id:60	 loss:1.265658
[Test] epoch:286	batch id:70	 loss:1.443919
[Test] epoch:286	batch id:80	 loss:1.585889
[Test] epoch:286	batch id:90	 loss:1.595632
[Test] epoch:286	batch id:100	 loss:1.920470
[Test] epoch:286	batch id:110	 loss:1.407443
[Test] epoch:286	batch id:120	 loss:1.296959
[Test] epoch:286	batch id:130	 loss:1.346066
[Test] epoch:286	batch id:140	 loss:1.307182
[Test] epoch:286	batch id:150	 loss:1.840578
[Test] 286, loss: 1.478151, test acc: 0.914506,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:287	batch id:0	 lr:0.008240 loss:1.328139
[Train] epoch:287	batch id:10	 lr:0.008240 loss:1.353765
[Train] epoch:287	batch id:20	 lr:0.008240 loss:1.339737
[Train] epoch:287	batch id:30	 lr:0.008240 loss:1.339388
[Train] epoch:287	batch id:40	 lr:0.008240 loss:1.325406
[Train] epoch:287	batch id:50	 lr:0.008240 loss:1.407956
[Train] epoch:287	batch id:60	 lr:0.008240 loss:1.343600
[Train] epoch:287	batch id:70	 lr:0.008240 loss:1.345072
[Train] epoch:287	batch id:80	 lr:0.008240 loss:1.354181
[Train] epoch:287	batch id:90	 lr:0.008240 loss:1.352275
[Train] epoch:287	batch id:100	 lr:0.008240 loss:1.310099
[Train] epoch:287	batch id:110	 lr:0.008240 loss:1.308566
[Train] epoch:287	batch id:120	 lr:0.008240 loss:1.374165
[Train] epoch:287	batch id:130	 lr:0.008240 loss:1.316676
[Train] epoch:287	batch id:140	 lr:0.008240 loss:1.335154
[Train] epoch:287	batch id:150	 lr:0.008240 loss:1.330801
[Train] epoch:287	batch id:160	 lr:0.008240 loss:1.374583
[Train] epoch:287	batch id:170	 lr:0.008240 loss:1.336155
[Train] epoch:287	batch id:180	 lr:0.008240 loss:1.344781
[Train] epoch:287	batch id:190	 lr:0.008240 loss:1.322810
[Train] epoch:287	batch id:200	 lr:0.008240 loss:1.323856
[Train] epoch:287	batch id:210	 lr:0.008240 loss:1.337957
[Train] epoch:287	batch id:220	 lr:0.008240 loss:1.474733
[Train] epoch:287	batch id:230	 lr:0.008240 loss:1.470582
[Train] epoch:287	batch id:240	 lr:0.008240 loss:1.386808
[Train] epoch:287	batch id:250	 lr:0.008240 loss:1.372906
[Train] epoch:287	batch id:260	 lr:0.008240 loss:1.312129
[Train] epoch:287	batch id:270	 lr:0.008240 loss:1.346684
[Train] epoch:287	batch id:280	 lr:0.008240 loss:1.380322
[Train] epoch:287	batch id:290	 lr:0.008240 loss:1.321931
[Train] epoch:287	batch id:300	 lr:0.008240 loss:1.365692
[Train] 287, loss: 1.350945, train acc: 0.992060, 
[Test] epoch:287	batch id:0	 loss:1.268388
[Test] epoch:287	batch id:10	 loss:1.423878
[Test] epoch:287	batch id:20	 loss:1.452758
[Test] epoch:287	batch id:30	 loss:1.403810
[Test] epoch:287	batch id:40	 loss:1.478470
[Test] epoch:287	batch id:50	 loss:1.601254
[Test] epoch:287	batch id:60	 loss:1.268674
[Test] epoch:287	batch id:70	 loss:1.515135
[Test] epoch:287	batch id:80	 loss:1.740182
[Test] epoch:287	batch id:90	 loss:1.681988
[Test] epoch:287	batch id:100	 loss:1.927255
[Test] epoch:287	batch id:110	 loss:1.319872
[Test] epoch:287	batch id:120	 loss:1.341182
[Test] epoch:287	batch id:130	 loss:1.320646
[Test] epoch:287	batch id:140	 loss:1.387763
[Test] epoch:287	batch id:150	 loss:1.783459
[Test] 287, loss: 1.492021, test acc: 0.914506,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:288	batch id:0	 lr:0.008011 loss:1.356845
[Train] epoch:288	batch id:10	 lr:0.008011 loss:1.329731
[Train] epoch:288	batch id:20	 lr:0.008011 loss:1.379894
[Train] epoch:288	batch id:30	 lr:0.008011 loss:1.350393
[Train] epoch:288	batch id:40	 lr:0.008011 loss:1.301159
[Train] epoch:288	batch id:50	 lr:0.008011 loss:1.324669
[Train] epoch:288	batch id:60	 lr:0.008011 loss:1.365042
[Train] epoch:288	batch id:70	 lr:0.008011 loss:1.356779
[Train] epoch:288	batch id:80	 lr:0.008011 loss:1.338951
[Train] epoch:288	batch id:90	 lr:0.008011 loss:1.332068
[Train] epoch:288	batch id:100	 lr:0.008011 loss:1.388555
[Train] epoch:288	batch id:110	 lr:0.008011 loss:1.313805
[Train] epoch:288	batch id:120	 lr:0.008011 loss:1.334329
[Train] epoch:288	batch id:130	 lr:0.008011 loss:1.355272
[Train] epoch:288	batch id:140	 lr:0.008011 loss:1.334388
[Train] epoch:288	batch id:150	 lr:0.008011 loss:1.339733
[Train] epoch:288	batch id:160	 lr:0.008011 loss:1.354859
[Train] epoch:288	batch id:170	 lr:0.008011 loss:1.331144
[Train] epoch:288	batch id:180	 lr:0.008011 loss:1.371660
[Train] epoch:288	batch id:190	 lr:0.008011 loss:1.350098
[Train] epoch:288	batch id:200	 lr:0.008011 loss:1.425115
[Train] epoch:288	batch id:210	 lr:0.008011 loss:1.501949
[Train] epoch:288	batch id:220	 lr:0.008011 loss:1.317954
[Train] epoch:288	batch id:230	 lr:0.008011 loss:1.362549
[Train] epoch:288	batch id:240	 lr:0.008011 loss:1.343912
[Train] epoch:288	batch id:250	 lr:0.008011 loss:1.374176
[Train] epoch:288	batch id:260	 lr:0.008011 loss:1.329846
[Train] epoch:288	batch id:270	 lr:0.008011 loss:1.396313
[Train] epoch:288	batch id:280	 lr:0.008011 loss:1.323242
[Train] epoch:288	batch id:290	 lr:0.008011 loss:1.396988
[Train] epoch:288	batch id:300	 lr:0.008011 loss:1.377222
[Train] 288, loss: 1.355384, train acc: 0.990839, 
[Test] epoch:288	batch id:0	 loss:1.273497
[Test] epoch:288	batch id:10	 loss:1.460900
[Test] epoch:288	batch id:20	 loss:1.407525
[Test] epoch:288	batch id:30	 loss:1.314706
[Test] epoch:288	batch id:40	 loss:1.502979
[Test] epoch:288	batch id:50	 loss:1.480040
[Test] epoch:288	batch id:60	 loss:1.251449
[Test] epoch:288	batch id:70	 loss:1.445700
[Test] epoch:288	batch id:80	 loss:1.564664
[Test] epoch:288	batch id:90	 loss:1.625794
[Test] epoch:288	batch id:100	 loss:1.923127
[Test] epoch:288	batch id:110	 loss:1.371442
[Test] epoch:288	batch id:120	 loss:1.300268
[Test] epoch:288	batch id:130	 loss:1.305626
[Test] epoch:288	batch id:140	 loss:1.313511
[Test] epoch:288	batch id:150	 loss:1.759626
[Test] 288, loss: 1.471945, test acc: 0.915316,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:289	batch id:0	 lr:0.007784 loss:1.359291
[Train] epoch:289	batch id:10	 lr:0.007784 loss:1.320268
[Train] epoch:289	batch id:20	 lr:0.007784 loss:1.338989
[Train] epoch:289	batch id:30	 lr:0.007784 loss:1.321222
[Train] epoch:289	batch id:40	 lr:0.007784 loss:1.336287
[Train] epoch:289	batch id:50	 lr:0.007784 loss:1.346385
[Train] epoch:289	batch id:60	 lr:0.007784 loss:1.333499
[Train] epoch:289	batch id:70	 lr:0.007784 loss:1.352315
[Train] epoch:289	batch id:80	 lr:0.007784 loss:1.361806
[Train] epoch:289	batch id:90	 lr:0.007784 loss:1.348736
[Train] epoch:289	batch id:100	 lr:0.007784 loss:1.375405
[Train] epoch:289	batch id:110	 lr:0.007784 loss:1.395118
[Train] epoch:289	batch id:120	 lr:0.007784 loss:1.315470
[Train] epoch:289	batch id:130	 lr:0.007784 loss:1.333696
[Train] epoch:289	batch id:140	 lr:0.007784 loss:1.323623
[Train] epoch:289	batch id:150	 lr:0.007784 loss:1.378621
[Train] epoch:289	batch id:160	 lr:0.007784 loss:1.415553
[Train] epoch:289	batch id:170	 lr:0.007784 loss:1.320513
[Train] epoch:289	batch id:180	 lr:0.007784 loss:1.359875
[Train] epoch:289	batch id:190	 lr:0.007784 loss:1.368846
[Train] epoch:289	batch id:200	 lr:0.007784 loss:1.309340
[Train] epoch:289	batch id:210	 lr:0.007784 loss:1.316731
[Train] epoch:289	batch id:220	 lr:0.007784 loss:1.397381
[Train] epoch:289	batch id:230	 lr:0.007784 loss:1.480374
[Train] epoch:289	batch id:240	 lr:0.007784 loss:1.343243
[Train] epoch:289	batch id:250	 lr:0.007784 loss:1.335832
[Train] epoch:289	batch id:260	 lr:0.007784 loss:1.326574
[Train] epoch:289	batch id:270	 lr:0.007784 loss:1.321463
[Train] epoch:289	batch id:280	 lr:0.007784 loss:1.383944
[Train] epoch:289	batch id:290	 lr:0.007784 loss:1.317749
[Train] epoch:289	batch id:300	 lr:0.007784 loss:1.323718
[Train] 289, loss: 1.351888, train acc: 0.990635, 
[Test] epoch:289	batch id:0	 loss:1.255747
[Test] epoch:289	batch id:10	 loss:1.418891
[Test] epoch:289	batch id:20	 loss:1.499161
[Test] epoch:289	batch id:30	 loss:1.334647
[Test] epoch:289	batch id:40	 loss:1.501374
[Test] epoch:289	batch id:50	 loss:1.506986
[Test] epoch:289	batch id:60	 loss:1.250179
[Test] epoch:289	batch id:70	 loss:1.347305
[Test] epoch:289	batch id:80	 loss:1.632102
[Test] epoch:289	batch id:90	 loss:1.604961
[Test] epoch:289	batch id:100	 loss:2.069602
[Test] epoch:289	batch id:110	 loss:1.320675
[Test] epoch:289	batch id:120	 loss:1.299290
[Test] epoch:289	batch id:130	 loss:1.302709
[Test] epoch:289	batch id:140	 loss:1.362960
[Test] epoch:289	batch id:150	 loss:1.703516
[Test] 289, loss: 1.470969, test acc: 0.916126,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:290	batch id:0	 lr:0.007562 loss:1.327414
[Train] epoch:290	batch id:10	 lr:0.007562 loss:1.338826
[Train] epoch:290	batch id:20	 lr:0.007562 loss:1.346608
[Train] epoch:290	batch id:30	 lr:0.007562 loss:1.415070
[Train] epoch:290	batch id:40	 lr:0.007562 loss:1.372017
[Train] epoch:290	batch id:50	 lr:0.007562 loss:1.375310
[Train] epoch:290	batch id:60	 lr:0.007562 loss:1.329843
[Train] epoch:290	batch id:70	 lr:0.007562 loss:1.340894
[Train] epoch:290	batch id:80	 lr:0.007562 loss:1.349052
[Train] epoch:290	batch id:90	 lr:0.007562 loss:1.329436
[Train] epoch:290	batch id:100	 lr:0.007562 loss:1.350139
[Train] epoch:290	batch id:110	 lr:0.007562 loss:1.462790
[Train] epoch:290	batch id:120	 lr:0.007562 loss:1.322009
[Train] epoch:290	batch id:130	 lr:0.007562 loss:1.385908
[Train] epoch:290	batch id:140	 lr:0.007562 loss:1.373077
[Train] epoch:290	batch id:150	 lr:0.007562 loss:1.383659
[Train] epoch:290	batch id:160	 lr:0.007562 loss:1.398022
[Train] epoch:290	batch id:170	 lr:0.007562 loss:1.338353
[Train] epoch:290	batch id:180	 lr:0.007562 loss:1.357416
[Train] epoch:290	batch id:190	 lr:0.007562 loss:1.367657
[Train] epoch:290	batch id:200	 lr:0.007562 loss:1.327036
[Train] epoch:290	batch id:210	 lr:0.007562 loss:1.350231
[Train] epoch:290	batch id:220	 lr:0.007562 loss:1.364877
[Train] epoch:290	batch id:230	 lr:0.007562 loss:1.410149
[Train] epoch:290	batch id:240	 lr:0.007562 loss:1.327761
[Train] epoch:290	batch id:250	 lr:0.007562 loss:1.348519
[Train] epoch:290	batch id:260	 lr:0.007562 loss:1.318593
[Train] epoch:290	batch id:270	 lr:0.007562 loss:1.362068
[Train] epoch:290	batch id:280	 lr:0.007562 loss:1.336876
[Train] epoch:290	batch id:290	 lr:0.007562 loss:1.350918
[Train] epoch:290	batch id:300	 lr:0.007562 loss:1.339941
[Train] 290, loss: 1.349377, train acc: 0.992060, 
[Test] epoch:290	batch id:0	 loss:1.321705
[Test] epoch:290	batch id:10	 loss:1.445066
[Test] epoch:290	batch id:20	 loss:1.548178
[Test] epoch:290	batch id:30	 loss:1.368588
[Test] epoch:290	batch id:40	 loss:1.553737
[Test] epoch:290	batch id:50	 loss:1.595168
[Test] epoch:290	batch id:60	 loss:1.272255
[Test] epoch:290	batch id:70	 loss:1.428477
[Test] epoch:290	batch id:80	 loss:1.688509
[Test] epoch:290	batch id:90	 loss:1.649254
[Test] epoch:290	batch id:100	 loss:1.944090
[Test] epoch:290	batch id:110	 loss:1.359749
[Test] epoch:290	batch id:120	 loss:1.321311
[Test] epoch:290	batch id:130	 loss:1.347312
[Test] epoch:290	batch id:140	 loss:1.313958
[Test] epoch:290	batch id:150	 loss:1.735912
[Test] 290, loss: 1.481958, test acc: 0.913695,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:291	batch id:0	 lr:0.007342 loss:1.344939
[Train] epoch:291	batch id:10	 lr:0.007342 loss:1.337007
[Train] epoch:291	batch id:20	 lr:0.007342 loss:1.327962
[Train] epoch:291	batch id:30	 lr:0.007342 loss:1.387167
[Train] epoch:291	batch id:40	 lr:0.007342 loss:1.361344
[Train] epoch:291	batch id:50	 lr:0.007342 loss:1.325749
[Train] epoch:291	batch id:60	 lr:0.007342 loss:1.408680
[Train] epoch:291	batch id:70	 lr:0.007342 loss:1.326340
[Train] epoch:291	batch id:80	 lr:0.007342 loss:1.328556
[Train] epoch:291	batch id:90	 lr:0.007342 loss:1.338835
[Train] epoch:291	batch id:100	 lr:0.007342 loss:1.349574
[Train] epoch:291	batch id:110	 lr:0.007342 loss:1.352686
[Train] epoch:291	batch id:120	 lr:0.007342 loss:1.378580
[Train] epoch:291	batch id:130	 lr:0.007342 loss:1.338947
[Train] epoch:291	batch id:140	 lr:0.007342 loss:1.365263
[Train] epoch:291	batch id:150	 lr:0.007342 loss:1.335985
[Train] epoch:291	batch id:160	 lr:0.007342 loss:1.349849
[Train] epoch:291	batch id:170	 lr:0.007342 loss:1.352498
[Train] epoch:291	batch id:180	 lr:0.007342 loss:1.361378
[Train] epoch:291	batch id:190	 lr:0.007342 loss:1.387952
[Train] epoch:291	batch id:200	 lr:0.007342 loss:1.323875
[Train] epoch:291	batch id:210	 lr:0.007342 loss:1.353300
[Train] epoch:291	batch id:220	 lr:0.007342 loss:1.320992
[Train] epoch:291	batch id:230	 lr:0.007342 loss:1.377763
[Train] epoch:291	batch id:240	 lr:0.007342 loss:1.346564
[Train] epoch:291	batch id:250	 lr:0.007342 loss:1.329880
[Train] epoch:291	batch id:260	 lr:0.007342 loss:1.372519
[Train] epoch:291	batch id:270	 lr:0.007342 loss:1.332579
[Train] epoch:291	batch id:280	 lr:0.007342 loss:1.343731
[Train] epoch:291	batch id:290	 lr:0.007342 loss:1.344046
[Train] epoch:291	batch id:300	 lr:0.007342 loss:1.358002
[Train] 291, loss: 1.349969, train acc: 0.990330, 
[Test] epoch:291	batch id:0	 loss:1.275790
[Test] epoch:291	batch id:10	 loss:1.420777
[Test] epoch:291	batch id:20	 loss:1.428815
[Test] epoch:291	batch id:30	 loss:1.406390
[Test] epoch:291	batch id:40	 loss:1.585292
[Test] epoch:291	batch id:50	 loss:1.502980
[Test] epoch:291	batch id:60	 loss:1.256651
[Test] epoch:291	batch id:70	 loss:1.452662
[Test] epoch:291	batch id:80	 loss:1.759214
[Test] epoch:291	batch id:90	 loss:1.754226
[Test] epoch:291	batch id:100	 loss:1.970847
[Test] epoch:291	batch id:110	 loss:1.332719
[Test] epoch:291	batch id:120	 loss:1.348498
[Test] epoch:291	batch id:130	 loss:1.291389
[Test] epoch:291	batch id:140	 loss:1.404004
[Test] epoch:291	batch id:150	 loss:1.797197
[Test] 291, loss: 1.487730, test acc: 0.908428,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:292	batch id:0	 lr:0.007127 loss:1.343672
[Train] epoch:292	batch id:10	 lr:0.007127 loss:1.307321
[Train] epoch:292	batch id:20	 lr:0.007127 loss:1.366236
[Train] epoch:292	batch id:30	 lr:0.007127 loss:1.315055
[Train] epoch:292	batch id:40	 lr:0.007127 loss:1.321440
[Train] epoch:292	batch id:50	 lr:0.007127 loss:1.460282
[Train] epoch:292	batch id:60	 lr:0.007127 loss:1.318693
[Train] epoch:292	batch id:70	 lr:0.007127 loss:1.344032
[Train] epoch:292	batch id:80	 lr:0.007127 loss:1.314532
[Train] epoch:292	batch id:90	 lr:0.007127 loss:1.352585
[Train] epoch:292	batch id:100	 lr:0.007127 loss:1.346299
[Train] epoch:292	batch id:110	 lr:0.007127 loss:1.407719
[Train] epoch:292	batch id:120	 lr:0.007127 loss:1.385024
[Train] epoch:292	batch id:130	 lr:0.007127 loss:1.317165
[Train] epoch:292	batch id:140	 lr:0.007127 loss:1.396860
[Train] epoch:292	batch id:150	 lr:0.007127 loss:1.346286
[Train] epoch:292	batch id:160	 lr:0.007127 loss:1.361276
[Train] epoch:292	batch id:170	 lr:0.007127 loss:1.331156
[Train] epoch:292	batch id:180	 lr:0.007127 loss:1.387598
[Train] epoch:292	batch id:190	 lr:0.007127 loss:1.400845
[Train] epoch:292	batch id:200	 lr:0.007127 loss:1.331117
[Train] epoch:292	batch id:210	 lr:0.007127 loss:1.315316
[Train] epoch:292	batch id:220	 lr:0.007127 loss:1.336348
[Train] epoch:292	batch id:230	 lr:0.007127 loss:1.403144
[Train] epoch:292	batch id:240	 lr:0.007127 loss:1.351291
[Train] epoch:292	batch id:250	 lr:0.007127 loss:1.310655
[Train] epoch:292	batch id:260	 lr:0.007127 loss:1.328632
[Train] epoch:292	batch id:270	 lr:0.007127 loss:1.358996
[Train] epoch:292	batch id:280	 lr:0.007127 loss:1.333427
[Train] epoch:292	batch id:290	 lr:0.007127 loss:1.364375
[Train] epoch:292	batch id:300	 lr:0.007127 loss:1.387988
[Train] 292, loss: 1.349170, train acc: 0.992569, 
[Test] epoch:292	batch id:0	 loss:1.260975
[Test] epoch:292	batch id:10	 loss:1.414974
[Test] epoch:292	batch id:20	 loss:1.475948
[Test] epoch:292	batch id:30	 loss:1.308738
[Test] epoch:292	batch id:40	 loss:1.495140
[Test] epoch:292	batch id:50	 loss:1.459902
[Test] epoch:292	batch id:60	 loss:1.249125
[Test] epoch:292	batch id:70	 loss:1.456649
[Test] epoch:292	batch id:80	 loss:1.579575
[Test] epoch:292	batch id:90	 loss:1.506286
[Test] epoch:292	batch id:100	 loss:1.885023
[Test] epoch:292	batch id:110	 loss:1.301195
[Test] epoch:292	batch id:120	 loss:1.297213
[Test] epoch:292	batch id:130	 loss:1.341062
[Test] epoch:292	batch id:140	 loss:1.369491
[Test] epoch:292	batch id:150	 loss:1.654625
[Test] 292, loss: 1.461085, test acc: 0.921394,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:293	batch id:0	 lr:0.006914 loss:1.314837
[Train] epoch:293	batch id:10	 lr:0.006914 loss:1.333099
[Train] epoch:293	batch id:20	 lr:0.006914 loss:1.418216
[Train] epoch:293	batch id:30	 lr:0.006914 loss:1.329849
[Train] epoch:293	batch id:40	 lr:0.006914 loss:1.320218
[Train] epoch:293	batch id:50	 lr:0.006914 loss:1.316812
[Train] epoch:293	batch id:60	 lr:0.006914 loss:1.357376
[Train] epoch:293	batch id:70	 lr:0.006914 loss:1.302494
[Train] epoch:293	batch id:80	 lr:0.006914 loss:1.303363
[Train] epoch:293	batch id:90	 lr:0.006914 loss:1.400538
[Train] epoch:293	batch id:100	 lr:0.006914 loss:1.344595
[Train] epoch:293	batch id:110	 lr:0.006914 loss:1.318339
[Train] epoch:293	batch id:120	 lr:0.006914 loss:1.331060
[Train] epoch:293	batch id:130	 lr:0.006914 loss:1.348084
[Train] epoch:293	batch id:140	 lr:0.006914 loss:1.325400
[Train] epoch:293	batch id:150	 lr:0.006914 loss:1.375517
[Train] epoch:293	batch id:160	 lr:0.006914 loss:1.314784
[Train] epoch:293	batch id:170	 lr:0.006914 loss:1.335647
[Train] epoch:293	batch id:180	 lr:0.006914 loss:1.373089
[Train] epoch:293	batch id:190	 lr:0.006914 loss:1.341388
[Train] epoch:293	batch id:200	 lr:0.006914 loss:1.342712
[Train] epoch:293	batch id:210	 lr:0.006914 loss:1.360793
[Train] epoch:293	batch id:220	 lr:0.006914 loss:1.334938
[Train] epoch:293	batch id:230	 lr:0.006914 loss:1.347705
[Train] epoch:293	batch id:240	 lr:0.006914 loss:1.316800
[Train] epoch:293	batch id:250	 lr:0.006914 loss:1.338279
[Train] epoch:293	batch id:260	 lr:0.006914 loss:1.363706
[Train] epoch:293	batch id:270	 lr:0.006914 loss:1.357903
[Train] epoch:293	batch id:280	 lr:0.006914 loss:1.365120
[Train] epoch:293	batch id:290	 lr:0.006914 loss:1.340210
[Train] epoch:293	batch id:300	 lr:0.006914 loss:1.336566
[Train] 293, loss: 1.347267, train acc: 0.992773, 
[Test] epoch:293	batch id:0	 loss:1.254458
[Test] epoch:293	batch id:10	 loss:1.342533
[Test] epoch:293	batch id:20	 loss:1.460580
[Test] epoch:293	batch id:30	 loss:1.362469
[Test] epoch:293	batch id:40	 loss:1.545456
[Test] epoch:293	batch id:50	 loss:1.533503
[Test] epoch:293	batch id:60	 loss:1.251168
[Test] epoch:293	batch id:70	 loss:1.512707
[Test] epoch:293	batch id:80	 loss:1.598271
[Test] epoch:293	batch id:90	 loss:1.593867
[Test] epoch:293	batch id:100	 loss:1.922268
[Test] epoch:293	batch id:110	 loss:1.310977
[Test] epoch:293	batch id:120	 loss:1.303352
[Test] epoch:293	batch id:130	 loss:1.295689
[Test] epoch:293	batch id:140	 loss:1.346704
[Test] epoch:293	batch id:150	 loss:1.807235
[Test] 293, loss: 1.459289, test acc: 0.915721,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:294	batch id:0	 lr:0.006706 loss:1.343843
[Train] epoch:294	batch id:10	 lr:0.006706 loss:1.321823
[Train] epoch:294	batch id:20	 lr:0.006706 loss:1.382369
[Train] epoch:294	batch id:30	 lr:0.006706 loss:1.340930
[Train] epoch:294	batch id:40	 lr:0.006706 loss:1.357682
[Train] epoch:294	batch id:50	 lr:0.006706 loss:1.409282
[Train] epoch:294	batch id:60	 lr:0.006706 loss:1.349859
[Train] epoch:294	batch id:70	 lr:0.006706 loss:1.309128
[Train] epoch:294	batch id:80	 lr:0.006706 loss:1.382231
[Train] epoch:294	batch id:90	 lr:0.006706 loss:1.395561
[Train] epoch:294	batch id:100	 lr:0.006706 loss:1.318238
[Train] epoch:294	batch id:110	 lr:0.006706 loss:1.370479
[Train] epoch:294	batch id:120	 lr:0.006706 loss:1.404219
[Train] epoch:294	batch id:130	 lr:0.006706 loss:1.317808
[Train] epoch:294	batch id:140	 lr:0.006706 loss:1.314912
[Train] epoch:294	batch id:150	 lr:0.006706 loss:1.334670
[Train] epoch:294	batch id:160	 lr:0.006706 loss:1.354827
[Train] epoch:294	batch id:170	 lr:0.006706 loss:1.358219
[Train] epoch:294	batch id:180	 lr:0.006706 loss:1.361859
[Train] epoch:294	batch id:190	 lr:0.006706 loss:1.368621
[Train] epoch:294	batch id:200	 lr:0.006706 loss:1.382387
[Train] epoch:294	batch id:210	 lr:0.006706 loss:1.442978
[Train] epoch:294	batch id:220	 lr:0.006706 loss:1.310703
[Train] epoch:294	batch id:230	 lr:0.006706 loss:1.315134
[Train] epoch:294	batch id:240	 lr:0.006706 loss:1.328375
[Train] epoch:294	batch id:250	 lr:0.006706 loss:1.353726
[Train] epoch:294	batch id:260	 lr:0.006706 loss:1.342262
[Train] epoch:294	batch id:270	 lr:0.006706 loss:1.439275
[Train] epoch:294	batch id:280	 lr:0.006706 loss:1.328200
[Train] epoch:294	batch id:290	 lr:0.006706 loss:1.360314
[Train] epoch:294	batch id:300	 lr:0.006706 loss:1.331052
[Train] 294, loss: 1.347853, train acc: 0.992162, 
[Test] epoch:294	batch id:0	 loss:1.275043
[Test] epoch:294	batch id:10	 loss:1.403218
[Test] epoch:294	batch id:20	 loss:1.382096
[Test] epoch:294	batch id:30	 loss:1.326710
[Test] epoch:294	batch id:40	 loss:1.582510
[Test] epoch:294	batch id:50	 loss:1.547636
[Test] epoch:294	batch id:60	 loss:1.262892
[Test] epoch:294	batch id:70	 loss:1.388050
[Test] epoch:294	batch id:80	 loss:1.625991
[Test] epoch:294	batch id:90	 loss:1.726626
[Test] epoch:294	batch id:100	 loss:2.076473
[Test] epoch:294	batch id:110	 loss:1.376038
[Test] epoch:294	batch id:120	 loss:1.318261
[Test] epoch:294	batch id:130	 loss:1.268963
[Test] epoch:294	batch id:140	 loss:1.391379
[Test] epoch:294	batch id:150	 loss:1.858511
[Test] 294, loss: 1.492319, test acc: 0.906807,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:295	batch id:0	 lr:0.006500 loss:1.316271
[Train] epoch:295	batch id:10	 lr:0.006500 loss:1.329445
[Train] epoch:295	batch id:20	 lr:0.006500 loss:1.366949
[Train] epoch:295	batch id:30	 lr:0.006500 loss:1.329746
[Train] epoch:295	batch id:40	 lr:0.006500 loss:1.357589
[Train] epoch:295	batch id:50	 lr:0.006500 loss:1.346377
[Train] epoch:295	batch id:60	 lr:0.006500 loss:1.347164
[Train] epoch:295	batch id:70	 lr:0.006500 loss:1.362927
[Train] epoch:295	batch id:80	 lr:0.006500 loss:1.434220
[Train] epoch:295	batch id:90	 lr:0.006500 loss:1.313870
[Train] epoch:295	batch id:100	 lr:0.006500 loss:1.409885
[Train] epoch:295	batch id:110	 lr:0.006500 loss:1.307073
[Train] epoch:295	batch id:120	 lr:0.006500 loss:1.355849
[Train] epoch:295	batch id:130	 lr:0.006500 loss:1.329794
[Train] epoch:295	batch id:140	 lr:0.006500 loss:1.368137
[Train] epoch:295	batch id:150	 lr:0.006500 loss:1.319467
[Train] epoch:295	batch id:160	 lr:0.006500 loss:1.341541
[Train] epoch:295	batch id:170	 lr:0.006500 loss:1.303534
[Train] epoch:295	batch id:180	 lr:0.006500 loss:1.374502
[Train] epoch:295	batch id:190	 lr:0.006500 loss:1.349405
[Train] epoch:295	batch id:200	 lr:0.006500 loss:1.377439
[Train] epoch:295	batch id:210	 lr:0.006500 loss:1.435230
[Train] epoch:295	batch id:220	 lr:0.006500 loss:1.303999
[Train] epoch:295	batch id:230	 lr:0.006500 loss:1.370072
[Train] epoch:295	batch id:240	 lr:0.006500 loss:1.347427
[Train] epoch:295	batch id:250	 lr:0.006500 loss:1.359727
[Train] epoch:295	batch id:260	 lr:0.006500 loss:1.323717
[Train] epoch:295	batch id:270	 lr:0.006500 loss:1.314580
[Train] epoch:295	batch id:280	 lr:0.006500 loss:1.320271
[Train] epoch:295	batch id:290	 lr:0.006500 loss:1.353549
[Train] epoch:295	batch id:300	 lr:0.006500 loss:1.409132
[Train] 295, loss: 1.345411, train acc: 0.993180, 
[Test] epoch:295	batch id:0	 loss:1.274246
[Test] epoch:295	batch id:10	 loss:1.533210
[Test] epoch:295	batch id:20	 loss:1.439370
[Test] epoch:295	batch id:30	 loss:1.424461
[Test] epoch:295	batch id:40	 loss:1.459634
[Test] epoch:295	batch id:50	 loss:1.572085
[Test] epoch:295	batch id:60	 loss:1.248576
[Test] epoch:295	batch id:70	 loss:1.492815
[Test] epoch:295	batch id:80	 loss:1.667713
[Test] epoch:295	batch id:90	 loss:1.668848
[Test] epoch:295	batch id:100	 loss:1.780747
[Test] epoch:295	batch id:110	 loss:1.397212
[Test] epoch:295	batch id:120	 loss:1.338762
[Test] epoch:295	batch id:130	 loss:1.330442
[Test] epoch:295	batch id:140	 loss:1.343912
[Test] epoch:295	batch id:150	 loss:1.702210
[Test] 295, loss: 1.481528, test acc: 0.912075,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:296	batch id:0	 lr:0.006299 loss:1.356012
[Train] epoch:296	batch id:10	 lr:0.006299 loss:1.328787
[Train] epoch:296	batch id:20	 lr:0.006299 loss:1.320505
[Train] epoch:296	batch id:30	 lr:0.006299 loss:1.327435
[Train] epoch:296	batch id:40	 lr:0.006299 loss:1.305287
[Train] epoch:296	batch id:50	 lr:0.006299 loss:1.336387
[Train] epoch:296	batch id:60	 lr:0.006299 loss:1.366686
[Train] epoch:296	batch id:70	 lr:0.006299 loss:1.342964
[Train] epoch:296	batch id:80	 lr:0.006299 loss:1.337792
[Train] epoch:296	batch id:90	 lr:0.006299 loss:1.378008
[Train] epoch:296	batch id:100	 lr:0.006299 loss:1.309501
[Train] epoch:296	batch id:110	 lr:0.006299 loss:1.318870
[Train] epoch:296	batch id:120	 lr:0.006299 loss:1.334754
[Train] epoch:296	batch id:130	 lr:0.006299 loss:1.401446
[Train] epoch:296	batch id:140	 lr:0.006299 loss:1.369292
[Train] epoch:296	batch id:150	 lr:0.006299 loss:1.337776
[Train] epoch:296	batch id:160	 lr:0.006299 loss:1.312847
[Train] epoch:296	batch id:170	 lr:0.006299 loss:1.334037
[Train] epoch:296	batch id:180	 lr:0.006299 loss:1.310479
[Train] epoch:296	batch id:190	 lr:0.006299 loss:1.394744
[Train] epoch:296	batch id:200	 lr:0.006299 loss:1.327229
[Train] epoch:296	batch id:210	 lr:0.006299 loss:1.381596
[Train] epoch:296	batch id:220	 lr:0.006299 loss:1.339194
[Train] epoch:296	batch id:230	 lr:0.006299 loss:1.311990
[Train] epoch:296	batch id:240	 lr:0.006299 loss:1.334198
[Train] epoch:296	batch id:250	 lr:0.006299 loss:1.369379
[Train] epoch:296	batch id:260	 lr:0.006299 loss:1.363103
[Train] epoch:296	batch id:270	 lr:0.006299 loss:1.331959
[Train] epoch:296	batch id:280	 lr:0.006299 loss:1.469463
[Train] epoch:296	batch id:290	 lr:0.006299 loss:1.446175
[Train] epoch:296	batch id:300	 lr:0.006299 loss:1.368766
[Train] 296, loss: 1.344992, train acc: 0.991348, 
[Test] epoch:296	batch id:0	 loss:1.294533
[Test] epoch:296	batch id:10	 loss:1.376641
[Test] epoch:296	batch id:20	 loss:1.428288
[Test] epoch:296	batch id:30	 loss:1.347070
[Test] epoch:296	batch id:40	 loss:1.550931
[Test] epoch:296	batch id:50	 loss:1.542263
[Test] epoch:296	batch id:60	 loss:1.253935
[Test] epoch:296	batch id:70	 loss:1.424599
[Test] epoch:296	batch id:80	 loss:1.599904
[Test] epoch:296	batch id:90	 loss:1.613681
[Test] epoch:296	batch id:100	 loss:2.103320
[Test] epoch:296	batch id:110	 loss:1.308808
[Test] epoch:296	batch id:120	 loss:1.302728
[Test] epoch:296	batch id:130	 loss:1.349578
[Test] epoch:296	batch id:140	 loss:1.319153
[Test] epoch:296	batch id:150	 loss:1.746393
[Test] 296, loss: 1.464313, test acc: 0.920583,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:297	batch id:0	 lr:0.006100 loss:1.374378
[Train] epoch:297	batch id:10	 lr:0.006100 loss:1.331535
[Train] epoch:297	batch id:20	 lr:0.006100 loss:1.370021
[Train] epoch:297	batch id:30	 lr:0.006100 loss:1.351817
[Train] epoch:297	batch id:40	 lr:0.006100 loss:1.323591
[Train] epoch:297	batch id:50	 lr:0.006100 loss:1.302691
[Train] epoch:297	batch id:60	 lr:0.006100 loss:1.308173
[Train] epoch:297	batch id:70	 lr:0.006100 loss:1.328627
[Train] epoch:297	batch id:80	 lr:0.006100 loss:1.323920
[Train] epoch:297	batch id:90	 lr:0.006100 loss:1.332267
[Train] epoch:297	batch id:100	 lr:0.006100 loss:1.322465
[Train] epoch:297	batch id:110	 lr:0.006100 loss:1.335524
[Train] epoch:297	batch id:120	 lr:0.006100 loss:1.307345
[Train] epoch:297	batch id:130	 lr:0.006100 loss:1.345784
[Train] epoch:297	batch id:140	 lr:0.006100 loss:1.344353
[Train] epoch:297	batch id:150	 lr:0.006100 loss:1.321014
[Train] epoch:297	batch id:160	 lr:0.006100 loss:1.329518
[Train] epoch:297	batch id:170	 lr:0.006100 loss:1.377648
[Train] epoch:297	batch id:180	 lr:0.006100 loss:1.336750
[Train] epoch:297	batch id:190	 lr:0.006100 loss:1.317839
[Train] epoch:297	batch id:200	 lr:0.006100 loss:1.312739
[Train] epoch:297	batch id:210	 lr:0.006100 loss:1.439531
[Train] epoch:297	batch id:220	 lr:0.006100 loss:1.342446
[Train] epoch:297	batch id:230	 lr:0.006100 loss:1.324049
[Train] epoch:297	batch id:240	 lr:0.006100 loss:1.322427
[Train] epoch:297	batch id:250	 lr:0.006100 loss:1.340270
[Train] epoch:297	batch id:260	 lr:0.006100 loss:1.358184
[Train] epoch:297	batch id:270	 lr:0.006100 loss:1.342723
[Train] epoch:297	batch id:280	 lr:0.006100 loss:1.324479
[Train] epoch:297	batch id:290	 lr:0.006100 loss:1.338565
[Train] epoch:297	batch id:300	 lr:0.006100 loss:1.331249
[Train] 297, loss: 1.345032, train acc: 0.992773, 
[Test] epoch:297	batch id:0	 loss:1.274490
[Test] epoch:297	batch id:10	 loss:1.463019
[Test] epoch:297	batch id:20	 loss:1.438223
[Test] epoch:297	batch id:30	 loss:1.358845
[Test] epoch:297	batch id:40	 loss:1.567194
[Test] epoch:297	batch id:50	 loss:1.607837
[Test] epoch:297	batch id:60	 loss:1.262290
[Test] epoch:297	batch id:70	 loss:1.423985
[Test] epoch:297	batch id:80	 loss:1.715671
[Test] epoch:297	batch id:90	 loss:1.763798
[Test] epoch:297	batch id:100	 loss:1.896107
[Test] epoch:297	batch id:110	 loss:1.414134
[Test] epoch:297	batch id:120	 loss:1.302440
[Test] epoch:297	batch id:130	 loss:1.396103
[Test] epoch:297	batch id:140	 loss:1.356045
[Test] epoch:297	batch id:150	 loss:1.759574
[Test] 297, loss: 1.496247, test acc: 0.916937,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:298	batch id:0	 lr:0.005906 loss:1.314838
[Train] epoch:298	batch id:10	 lr:0.005906 loss:1.349942
[Train] epoch:298	batch id:20	 lr:0.005906 loss:1.371798
[Train] epoch:298	batch id:30	 lr:0.005906 loss:1.328314
[Train] epoch:298	batch id:40	 lr:0.005906 loss:1.300514
[Train] epoch:298	batch id:50	 lr:0.005906 loss:1.354814
[Train] epoch:298	batch id:60	 lr:0.005906 loss:1.321184
[Train] epoch:298	batch id:70	 lr:0.005906 loss:1.326914
[Train] epoch:298	batch id:80	 lr:0.005906 loss:1.388605
[Train] epoch:298	batch id:90	 lr:0.005906 loss:1.320868
[Train] epoch:298	batch id:100	 lr:0.005906 loss:1.357561
[Train] epoch:298	batch id:110	 lr:0.005906 loss:1.341452
[Train] epoch:298	batch id:120	 lr:0.005906 loss:1.399755
[Train] epoch:298	batch id:130	 lr:0.005906 loss:1.305264
[Train] epoch:298	batch id:140	 lr:0.005906 loss:1.305958
[Train] epoch:298	batch id:150	 lr:0.005906 loss:1.325026
[Train] epoch:298	batch id:160	 lr:0.005906 loss:1.476261
[Train] epoch:298	batch id:170	 lr:0.005906 loss:1.330735
[Train] epoch:298	batch id:180	 lr:0.005906 loss:1.335295
[Train] epoch:298	batch id:190	 lr:0.005906 loss:1.382249
[Train] epoch:298	batch id:200	 lr:0.005906 loss:1.370330
[Train] epoch:298	batch id:210	 lr:0.005906 loss:1.324649
[Train] epoch:298	batch id:220	 lr:0.005906 loss:1.316884
[Train] epoch:298	batch id:230	 lr:0.005906 loss:1.333260
[Train] epoch:298	batch id:240	 lr:0.005906 loss:1.372031
[Train] epoch:298	batch id:250	 lr:0.005906 loss:1.330229
[Train] epoch:298	batch id:260	 lr:0.005906 loss:1.329674
[Train] epoch:298	batch id:270	 lr:0.005906 loss:1.324126
[Train] epoch:298	batch id:280	 lr:0.005906 loss:1.407609
[Train] epoch:298	batch id:290	 lr:0.005906 loss:1.361508
[Train] epoch:298	batch id:300	 lr:0.005906 loss:1.320394
[Train] 298, loss: 1.341706, train acc: 0.992671, 
[Test] epoch:298	batch id:0	 loss:1.303184
[Test] epoch:298	batch id:10	 loss:1.400365
[Test] epoch:298	batch id:20	 loss:1.435439
[Test] epoch:298	batch id:30	 loss:1.352467
[Test] epoch:298	batch id:40	 loss:1.595925
[Test] epoch:298	batch id:50	 loss:1.492271
[Test] epoch:298	batch id:60	 loss:1.255829
[Test] epoch:298	batch id:70	 loss:1.389791
[Test] epoch:298	batch id:80	 loss:1.600958
[Test] epoch:298	batch id:90	 loss:1.617120
[Test] epoch:298	batch id:100	 loss:2.011676
[Test] epoch:298	batch id:110	 loss:1.289602
[Test] epoch:298	batch id:120	 loss:1.297801
[Test] epoch:298	batch id:130	 loss:1.296250
[Test] epoch:298	batch id:140	 loss:1.349758
[Test] epoch:298	batch id:150	 loss:1.881056
[Test] 298, loss: 1.476532, test acc: 0.911669,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:299	batch id:0	 lr:0.005715 loss:1.313127
[Train] epoch:299	batch id:10	 lr:0.005715 loss:1.333432
[Train] epoch:299	batch id:20	 lr:0.005715 loss:1.305964
[Train] epoch:299	batch id:30	 lr:0.005715 loss:1.317604
[Train] epoch:299	batch id:40	 lr:0.005715 loss:1.337315
[Train] epoch:299	batch id:50	 lr:0.005715 loss:1.366878
[Train] epoch:299	batch id:60	 lr:0.005715 loss:1.311066
[Train] epoch:299	batch id:70	 lr:0.005715 loss:1.393284
[Train] epoch:299	batch id:80	 lr:0.005715 loss:1.314786
[Train] epoch:299	batch id:90	 lr:0.005715 loss:1.371383
[Train] epoch:299	batch id:100	 lr:0.005715 loss:1.347332
[Train] epoch:299	batch id:110	 lr:0.005715 loss:1.361420
[Train] epoch:299	batch id:120	 lr:0.005715 loss:1.330416
[Train] epoch:299	batch id:130	 lr:0.005715 loss:1.331912
[Train] epoch:299	batch id:140	 lr:0.005715 loss:1.326738
[Train] epoch:299	batch id:150	 lr:0.005715 loss:1.313570
[Train] epoch:299	batch id:160	 lr:0.005715 loss:1.356102
[Train] epoch:299	batch id:170	 lr:0.005715 loss:1.347824
[Train] epoch:299	batch id:180	 lr:0.005715 loss:1.307697
[Train] epoch:299	batch id:190	 lr:0.005715 loss:1.343390
[Train] epoch:299	batch id:200	 lr:0.005715 loss:1.331121
[Train] epoch:299	batch id:210	 lr:0.005715 loss:1.307081
[Train] epoch:299	batch id:220	 lr:0.005715 loss:1.323535
[Train] epoch:299	batch id:230	 lr:0.005715 loss:1.327041
[Train] epoch:299	batch id:240	 lr:0.005715 loss:1.322695
[Train] epoch:299	batch id:250	 lr:0.005715 loss:1.326634
[Train] epoch:299	batch id:260	 lr:0.005715 loss:1.378752
[Train] epoch:299	batch id:270	 lr:0.005715 loss:1.429203
[Train] epoch:299	batch id:280	 lr:0.005715 loss:1.348823
[Train] epoch:299	batch id:290	 lr:0.005715 loss:1.357223
[Train] epoch:299	batch id:300	 lr:0.005715 loss:1.350677
[Train] 299, loss: 1.341853, train acc: 0.993689, 
[Test] epoch:299	batch id:0	 loss:1.263108
[Test] epoch:299	batch id:10	 loss:1.456412
[Test] epoch:299	batch id:20	 loss:1.591728
[Test] epoch:299	batch id:30	 loss:1.362084
[Test] epoch:299	batch id:40	 loss:1.536376
[Test] epoch:299	batch id:50	 loss:1.483111
[Test] epoch:299	batch id:60	 loss:1.252699
[Test] epoch:299	batch id:70	 loss:1.365874
[Test] epoch:299	batch id:80	 loss:1.611687
[Test] epoch:299	batch id:90	 loss:1.601418
[Test] epoch:299	batch id:100	 loss:1.817724
[Test] epoch:299	batch id:110	 loss:1.339880
[Test] epoch:299	batch id:120	 loss:1.294000
[Test] epoch:299	batch id:130	 loss:1.357362
[Test] epoch:299	batch id:140	 loss:1.379676
[Test] epoch:299	batch id:150	 loss:1.684012
[Test] 299, loss: 1.469520, test acc: 0.914506,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:300	batch id:0	 lr:0.005528 loss:1.307903
[Train] epoch:300	batch id:10	 lr:0.005528 loss:1.398430
[Train] epoch:300	batch id:20	 lr:0.005528 loss:1.345938
[Train] epoch:300	batch id:30	 lr:0.005528 loss:1.372609
[Train] epoch:300	batch id:40	 lr:0.005528 loss:1.364536
[Train] epoch:300	batch id:50	 lr:0.005528 loss:1.362557
[Train] epoch:300	batch id:60	 lr:0.005528 loss:1.321954
[Train] epoch:300	batch id:70	 lr:0.005528 loss:1.358838
[Train] epoch:300	batch id:80	 lr:0.005528 loss:1.337073
[Train] epoch:300	batch id:90	 lr:0.005528 loss:1.315482
[Train] epoch:300	batch id:100	 lr:0.005528 loss:1.328067
[Train] epoch:300	batch id:110	 lr:0.005528 loss:1.361681
[Train] epoch:300	batch id:120	 lr:0.005528 loss:1.393858
[Train] epoch:300	batch id:130	 lr:0.005528 loss:1.316594
[Train] epoch:300	batch id:140	 lr:0.005528 loss:1.338014
[Train] epoch:300	batch id:150	 lr:0.005528 loss:1.346041
[Train] epoch:300	batch id:160	 lr:0.005528 loss:1.331112
[Train] epoch:300	batch id:170	 lr:0.005528 loss:1.393919
[Train] epoch:300	batch id:180	 lr:0.005528 loss:1.332568
[Train] epoch:300	batch id:190	 lr:0.005528 loss:1.321961
[Train] epoch:300	batch id:200	 lr:0.005528 loss:1.335642
[Train] epoch:300	batch id:210	 lr:0.005528 loss:1.399844
[Train] epoch:300	batch id:220	 lr:0.005528 loss:1.312386
[Train] epoch:300	batch id:230	 lr:0.005528 loss:1.325958
[Train] epoch:300	batch id:240	 lr:0.005528 loss:1.325334
[Train] epoch:300	batch id:250	 lr:0.005528 loss:1.408290
[Train] epoch:300	batch id:260	 lr:0.005528 loss:1.438605
[Train] epoch:300	batch id:270	 lr:0.005528 loss:1.300341
[Train] epoch:300	batch id:280	 lr:0.005528 loss:1.363039
[Train] epoch:300	batch id:290	 lr:0.005528 loss:1.391555
[Train] epoch:300	batch id:300	 lr:0.005528 loss:1.318448
[Train] 300, loss: 1.344884, train acc: 0.992366, 
[Test] epoch:300	batch id:0	 loss:1.260160
[Test] epoch:300	batch id:10	 loss:1.418457
[Test] epoch:300	batch id:20	 loss:1.386837
[Test] epoch:300	batch id:30	 loss:1.310985
[Test] epoch:300	batch id:40	 loss:1.521253
[Test] epoch:300	batch id:50	 loss:1.468408
[Test] epoch:300	batch id:60	 loss:1.256642
[Test] epoch:300	batch id:70	 loss:1.410998
[Test] epoch:300	batch id:80	 loss:1.595635
[Test] epoch:300	batch id:90	 loss:1.582407
[Test] epoch:300	batch id:100	 loss:1.927532
[Test] epoch:300	batch id:110	 loss:1.344521
[Test] epoch:300	batch id:120	 loss:1.291519
[Test] epoch:300	batch id:130	 loss:1.308419
[Test] epoch:300	batch id:140	 loss:1.336953
[Test] epoch:300	batch id:150	 loss:1.673058
[Test] 300, loss: 1.464626, test acc: 0.918152,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:301	batch id:0	 lr:0.005344 loss:1.317363
[Train] epoch:301	batch id:10	 lr:0.005344 loss:1.336590
[Train] epoch:301	batch id:20	 lr:0.005344 loss:1.394847
[Train] epoch:301	batch id:30	 lr:0.005344 loss:1.327819
[Train] epoch:301	batch id:40	 lr:0.005344 loss:1.319545
[Train] epoch:301	batch id:50	 lr:0.005344 loss:1.365875
[Train] epoch:301	batch id:60	 lr:0.005344 loss:1.338936
[Train] epoch:301	batch id:70	 lr:0.005344 loss:1.367127
[Train] epoch:301	batch id:80	 lr:0.005344 loss:1.425030
[Train] epoch:301	batch id:90	 lr:0.005344 loss:1.308416
[Train] epoch:301	batch id:100	 lr:0.005344 loss:1.321939
[Train] epoch:301	batch id:110	 lr:0.005344 loss:1.322906
[Train] epoch:301	batch id:120	 lr:0.005344 loss:1.324970
[Train] epoch:301	batch id:130	 lr:0.005344 loss:1.309406
[Train] epoch:301	batch id:140	 lr:0.005344 loss:1.309226
[Train] epoch:301	batch id:150	 lr:0.005344 loss:1.345310
[Train] epoch:301	batch id:160	 lr:0.005344 loss:1.333346
[Train] epoch:301	batch id:170	 lr:0.005344 loss:1.398643
[Train] epoch:301	batch id:180	 lr:0.005344 loss:1.321555
[Train] epoch:301	batch id:190	 lr:0.005344 loss:1.323038
[Train] epoch:301	batch id:200	 lr:0.005344 loss:1.333500
[Train] epoch:301	batch id:210	 lr:0.005344 loss:1.409155
[Train] epoch:301	batch id:220	 lr:0.005344 loss:1.354013
[Train] epoch:301	batch id:230	 lr:0.005344 loss:1.347699
[Train] epoch:301	batch id:240	 lr:0.005344 loss:1.327224
[Train] epoch:301	batch id:250	 lr:0.005344 loss:1.378693
[Train] epoch:301	batch id:260	 lr:0.005344 loss:1.445634
[Train] epoch:301	batch id:270	 lr:0.005344 loss:1.378897
[Train] epoch:301	batch id:280	 lr:0.005344 loss:1.337483
[Train] epoch:301	batch id:290	 lr:0.005344 loss:1.319910
[Train] epoch:301	batch id:300	 lr:0.005344 loss:1.351476
[Train] 301, loss: 1.343140, train acc: 0.992671, 
[Test] epoch:301	batch id:0	 loss:1.249776
[Test] epoch:301	batch id:10	 loss:1.389277
[Test] epoch:301	batch id:20	 loss:1.493459
[Test] epoch:301	batch id:30	 loss:1.351771
[Test] epoch:301	batch id:40	 loss:1.553824
[Test] epoch:301	batch id:50	 loss:1.473632
[Test] epoch:301	batch id:60	 loss:1.250100
[Test] epoch:301	batch id:70	 loss:1.456296
[Test] epoch:301	batch id:80	 loss:1.635746
[Test] epoch:301	batch id:90	 loss:1.486044
[Test] epoch:301	batch id:100	 loss:1.889069
[Test] epoch:301	batch id:110	 loss:1.320672
[Test] epoch:301	batch id:120	 loss:1.290715
[Test] epoch:301	batch id:130	 loss:1.295209
[Test] epoch:301	batch id:140	 loss:1.322928
[Test] epoch:301	batch id:150	 loss:1.767769
[Test] 301, loss: 1.459300, test acc: 0.918152,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:302	batch id:0	 lr:0.005164 loss:1.316726
[Train] epoch:302	batch id:10	 lr:0.005164 loss:1.372335
[Train] epoch:302	batch id:20	 lr:0.005164 loss:1.348742
[Train] epoch:302	batch id:30	 lr:0.005164 loss:1.370205
[Train] epoch:302	batch id:40	 lr:0.005164 loss:1.318316
[Train] epoch:302	batch id:50	 lr:0.005164 loss:1.323274
[Train] epoch:302	batch id:60	 lr:0.005164 loss:1.349500
[Train] epoch:302	batch id:70	 lr:0.005164 loss:1.332130
[Train] epoch:302	batch id:80	 lr:0.005164 loss:1.336046
[Train] epoch:302	batch id:90	 lr:0.005164 loss:1.305429
[Train] epoch:302	batch id:100	 lr:0.005164 loss:1.339641
[Train] epoch:302	batch id:110	 lr:0.005164 loss:1.474985
[Train] epoch:302	batch id:120	 lr:0.005164 loss:1.320332
[Train] epoch:302	batch id:130	 lr:0.005164 loss:1.333364
[Train] epoch:302	batch id:140	 lr:0.005164 loss:1.375077
[Train] epoch:302	batch id:150	 lr:0.005164 loss:1.343165
[Train] epoch:302	batch id:160	 lr:0.005164 loss:1.337766
[Train] epoch:302	batch id:170	 lr:0.005164 loss:1.318815
[Train] epoch:302	batch id:180	 lr:0.005164 loss:1.305053
[Train] epoch:302	batch id:190	 lr:0.005164 loss:1.325475
[Train] epoch:302	batch id:200	 lr:0.005164 loss:1.430542
[Train] epoch:302	batch id:210	 lr:0.005164 loss:1.356506
[Train] epoch:302	batch id:220	 lr:0.005164 loss:1.365822
[Train] epoch:302	batch id:230	 lr:0.005164 loss:1.333442
[Train] epoch:302	batch id:240	 lr:0.005164 loss:1.370233
[Train] epoch:302	batch id:250	 lr:0.005164 loss:1.380505
[Train] epoch:302	batch id:260	 lr:0.005164 loss:1.321535
[Train] epoch:302	batch id:270	 lr:0.005164 loss:1.368730
[Train] epoch:302	batch id:280	 lr:0.005164 loss:1.390256
[Train] epoch:302	batch id:290	 lr:0.005164 loss:1.324221
[Train] epoch:302	batch id:300	 lr:0.005164 loss:1.299379
[Train] 302, loss: 1.341999, train acc: 0.993791, 
[Test] epoch:302	batch id:0	 loss:1.268058
[Test] epoch:302	batch id:10	 loss:1.387073
[Test] epoch:302	batch id:20	 loss:1.450310
[Test] epoch:302	batch id:30	 loss:1.431054
[Test] epoch:302	batch id:40	 loss:1.525602
[Test] epoch:302	batch id:50	 loss:1.531218
[Test] epoch:302	batch id:60	 loss:1.266044
[Test] epoch:302	batch id:70	 loss:1.471453
[Test] epoch:302	batch id:80	 loss:1.597354
[Test] epoch:302	batch id:90	 loss:1.622513
[Test] epoch:302	batch id:100	 loss:1.889275
[Test] epoch:302	batch id:110	 loss:1.316312
[Test] epoch:302	batch id:120	 loss:1.291464
[Test] epoch:302	batch id:130	 loss:1.349946
[Test] epoch:302	batch id:140	 loss:1.304846
[Test] epoch:302	batch id:150	 loss:1.728212
[Test] 302, loss: 1.463565, test acc: 0.916532,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:303	batch id:0	 lr:0.004987 loss:1.382095
[Train] epoch:303	batch id:10	 lr:0.004987 loss:1.361422
[Train] epoch:303	batch id:20	 lr:0.004987 loss:1.335711
[Train] epoch:303	batch id:30	 lr:0.004987 loss:1.306788
[Train] epoch:303	batch id:40	 lr:0.004987 loss:1.319365
[Train] epoch:303	batch id:50	 lr:0.004987 loss:1.337538
[Train] epoch:303	batch id:60	 lr:0.004987 loss:1.375828
[Train] epoch:303	batch id:70	 lr:0.004987 loss:1.356668
[Train] epoch:303	batch id:80	 lr:0.004987 loss:1.312493
[Train] epoch:303	batch id:90	 lr:0.004987 loss:1.340071
[Train] epoch:303	batch id:100	 lr:0.004987 loss:1.388603
[Train] epoch:303	batch id:110	 lr:0.004987 loss:1.344673
[Train] epoch:303	batch id:120	 lr:0.004987 loss:1.342215
[Train] epoch:303	batch id:130	 lr:0.004987 loss:1.417536
[Train] epoch:303	batch id:140	 lr:0.004987 loss:1.326391
[Train] epoch:303	batch id:150	 lr:0.004987 loss:1.337965
[Train] epoch:303	batch id:160	 lr:0.004987 loss:1.323492
[Train] epoch:303	batch id:170	 lr:0.004987 loss:1.376924
[Train] epoch:303	batch id:180	 lr:0.004987 loss:1.343587
[Train] epoch:303	batch id:190	 lr:0.004987 loss:1.308851
[Train] epoch:303	batch id:200	 lr:0.004987 loss:1.341195
[Train] epoch:303	batch id:210	 lr:0.004987 loss:1.373723
[Train] epoch:303	batch id:220	 lr:0.004987 loss:1.330847
[Train] epoch:303	batch id:230	 lr:0.004987 loss:1.313796
[Train] epoch:303	batch id:240	 lr:0.004987 loss:1.319895
[Train] epoch:303	batch id:250	 lr:0.004987 loss:1.319038
[Train] epoch:303	batch id:260	 lr:0.004987 loss:1.397056
[Train] epoch:303	batch id:270	 lr:0.004987 loss:1.331155
[Train] epoch:303	batch id:280	 lr:0.004987 loss:1.350215
[Train] epoch:303	batch id:290	 lr:0.004987 loss:1.302125
[Train] epoch:303	batch id:300	 lr:0.004987 loss:1.367474
[Train] 303, loss: 1.340250, train acc: 0.993282, 
[Test] epoch:303	batch id:0	 loss:1.257090
[Test] epoch:303	batch id:10	 loss:1.393573
[Test] epoch:303	batch id:20	 loss:1.432609
[Test] epoch:303	batch id:30	 loss:1.327374
[Test] epoch:303	batch id:40	 loss:1.509102
[Test] epoch:303	batch id:50	 loss:1.435737
[Test] epoch:303	batch id:60	 loss:1.253237
[Test] epoch:303	batch id:70	 loss:1.431364
[Test] epoch:303	batch id:80	 loss:1.635591
[Test] epoch:303	batch id:90	 loss:1.570624
[Test] epoch:303	batch id:100	 loss:1.891638
[Test] epoch:303	batch id:110	 loss:1.319634
[Test] epoch:303	batch id:120	 loss:1.331237
[Test] epoch:303	batch id:130	 loss:1.335840
[Test] epoch:303	batch id:140	 loss:1.382138
[Test] epoch:303	batch id:150	 loss:1.720775
[Test] 303, loss: 1.476814, test acc: 0.913695,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:304	batch id:0	 lr:0.004814 loss:1.327677
[Train] epoch:304	batch id:10	 lr:0.004814 loss:1.353525
[Train] epoch:304	batch id:20	 lr:0.004814 loss:1.322315
[Train] epoch:304	batch id:30	 lr:0.004814 loss:1.313201
[Train] epoch:304	batch id:40	 lr:0.004814 loss:1.303815
[Train] epoch:304	batch id:50	 lr:0.004814 loss:1.330150
[Train] epoch:304	batch id:60	 lr:0.004814 loss:1.366568
[Train] epoch:304	batch id:70	 lr:0.004814 loss:1.350013
[Train] epoch:304	batch id:80	 lr:0.004814 loss:1.434514
[Train] epoch:304	batch id:90	 lr:0.004814 loss:1.353073
[Train] epoch:304	batch id:100	 lr:0.004814 loss:1.426580
[Train] epoch:304	batch id:110	 lr:0.004814 loss:1.355511
[Train] epoch:304	batch id:120	 lr:0.004814 loss:1.369553
[Train] epoch:304	batch id:130	 lr:0.004814 loss:1.326343
[Train] epoch:304	batch id:140	 lr:0.004814 loss:1.362508
[Train] epoch:304	batch id:150	 lr:0.004814 loss:1.339424
[Train] epoch:304	batch id:160	 lr:0.004814 loss:1.365563
[Train] epoch:304	batch id:170	 lr:0.004814 loss:1.332355
[Train] epoch:304	batch id:180	 lr:0.004814 loss:1.342701
[Train] epoch:304	batch id:190	 lr:0.004814 loss:1.382027
[Train] epoch:304	batch id:200	 lr:0.004814 loss:1.307868
[Train] epoch:304	batch id:210	 lr:0.004814 loss:1.340630
[Train] epoch:304	batch id:220	 lr:0.004814 loss:1.320735
[Train] epoch:304	batch id:230	 lr:0.004814 loss:1.314452
[Train] epoch:304	batch id:240	 lr:0.004814 loss:1.300577
[Train] epoch:304	batch id:250	 lr:0.004814 loss:1.311629
[Train] epoch:304	batch id:260	 lr:0.004814 loss:1.310543
[Train] epoch:304	batch id:270	 lr:0.004814 loss:1.333477
[Train] epoch:304	batch id:280	 lr:0.004814 loss:1.336259
[Train] epoch:304	batch id:290	 lr:0.004814 loss:1.323222
[Train] epoch:304	batch id:300	 lr:0.004814 loss:1.361528
[Train] 304, loss: 1.341843, train acc: 0.994198, 
[Test] epoch:304	batch id:0	 loss:1.269114
[Test] epoch:304	batch id:10	 loss:1.387954
[Test] epoch:304	batch id:20	 loss:1.570145
[Test] epoch:304	batch id:30	 loss:1.348057
[Test] epoch:304	batch id:40	 loss:1.574874
[Test] epoch:304	batch id:50	 loss:1.504202
[Test] epoch:304	batch id:60	 loss:1.258191
[Test] epoch:304	batch id:70	 loss:1.331347
[Test] epoch:304	batch id:80	 loss:1.547676
[Test] epoch:304	batch id:90	 loss:1.449634
[Test] epoch:304	batch id:100	 loss:1.898211
[Test] epoch:304	batch id:110	 loss:1.292195
[Test] epoch:304	batch id:120	 loss:1.273530
[Test] epoch:304	batch id:130	 loss:1.297522
[Test] epoch:304	batch id:140	 loss:1.316036
[Test] epoch:304	batch id:150	 loss:1.637637
[Test] 304, loss: 1.460293, test acc: 0.916532,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:305	batch id:0	 lr:0.004645 loss:1.306593
[Train] epoch:305	batch id:10	 lr:0.004645 loss:1.336061
[Train] epoch:305	batch id:20	 lr:0.004645 loss:1.318997
[Train] epoch:305	batch id:30	 lr:0.004645 loss:1.359955
[Train] epoch:305	batch id:40	 lr:0.004645 loss:1.328096
[Train] epoch:305	batch id:50	 lr:0.004645 loss:1.329136
[Train] epoch:305	batch id:60	 lr:0.004645 loss:1.330800
[Train] epoch:305	batch id:70	 lr:0.004645 loss:1.404486
[Train] epoch:305	batch id:80	 lr:0.004645 loss:1.349848
[Train] epoch:305	batch id:90	 lr:0.004645 loss:1.360548
[Train] epoch:305	batch id:100	 lr:0.004645 loss:1.321021
[Train] epoch:305	batch id:110	 lr:0.004645 loss:1.323656
[Train] epoch:305	batch id:120	 lr:0.004645 loss:1.316014
[Train] epoch:305	batch id:130	 lr:0.004645 loss:1.337013
[Train] epoch:305	batch id:140	 lr:0.004645 loss:1.327686
[Train] epoch:305	batch id:150	 lr:0.004645 loss:1.328155
[Train] epoch:305	batch id:160	 lr:0.004645 loss:1.330643
[Train] epoch:305	batch id:170	 lr:0.004645 loss:1.361472
[Train] epoch:305	batch id:180	 lr:0.004645 loss:1.341782
[Train] epoch:305	batch id:190	 lr:0.004645 loss:1.350075
[Train] epoch:305	batch id:200	 lr:0.004645 loss:1.322196
[Train] epoch:305	batch id:210	 lr:0.004645 loss:1.328712
[Train] epoch:305	batch id:220	 lr:0.004645 loss:1.314049
[Train] epoch:305	batch id:230	 lr:0.004645 loss:1.357562
[Train] epoch:305	batch id:240	 lr:0.004645 loss:1.357692
[Train] epoch:305	batch id:250	 lr:0.004645 loss:1.325670
[Train] epoch:305	batch id:260	 lr:0.004645 loss:1.332169
[Train] epoch:305	batch id:270	 lr:0.004645 loss:1.329163
[Train] epoch:305	batch id:280	 lr:0.004645 loss:1.327753
[Train] epoch:305	batch id:290	 lr:0.004645 loss:1.320328
[Train] epoch:305	batch id:300	 lr:0.004645 loss:1.303498
[Train] 305, loss: 1.339252, train acc: 0.994198, 
[Test] epoch:305	batch id:0	 loss:1.280969
[Test] epoch:305	batch id:10	 loss:1.444865
[Test] epoch:305	batch id:20	 loss:1.391713
[Test] epoch:305	batch id:30	 loss:1.352835
[Test] epoch:305	batch id:40	 loss:1.536643
[Test] epoch:305	batch id:50	 loss:1.512544
[Test] epoch:305	batch id:60	 loss:1.264508
[Test] epoch:305	batch id:70	 loss:1.374338
[Test] epoch:305	batch id:80	 loss:1.592570
[Test] epoch:305	batch id:90	 loss:1.727083
[Test] epoch:305	batch id:100	 loss:1.929302
[Test] epoch:305	batch id:110	 loss:1.358477
[Test] epoch:305	batch id:120	 loss:1.289054
[Test] epoch:305	batch id:130	 loss:1.322701
[Test] epoch:305	batch id:140	 loss:1.370279
[Test] epoch:305	batch id:150	 loss:1.730393
[Test] 305, loss: 1.477961, test acc: 0.913695,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:306	batch id:0	 lr:0.004480 loss:1.343219
[Train] epoch:306	batch id:10	 lr:0.004480 loss:1.310688
[Train] epoch:306	batch id:20	 lr:0.004480 loss:1.342108
[Train] epoch:306	batch id:30	 lr:0.004480 loss:1.351963
[Train] epoch:306	batch id:40	 lr:0.004480 loss:1.310378
[Train] epoch:306	batch id:50	 lr:0.004480 loss:1.371876
[Train] epoch:306	batch id:60	 lr:0.004480 loss:1.331352
[Train] epoch:306	batch id:70	 lr:0.004480 loss:1.347250
[Train] epoch:306	batch id:80	 lr:0.004480 loss:1.319457
[Train] epoch:306	batch id:90	 lr:0.004480 loss:1.305978
[Train] epoch:306	batch id:100	 lr:0.004480 loss:1.396872
[Train] epoch:306	batch id:110	 lr:0.004480 loss:1.298202
[Train] epoch:306	batch id:120	 lr:0.004480 loss:1.310638
[Train] epoch:306	batch id:130	 lr:0.004480 loss:1.393036
[Train] epoch:306	batch id:140	 lr:0.004480 loss:1.330229
[Train] epoch:306	batch id:150	 lr:0.004480 loss:1.332491
[Train] epoch:306	batch id:160	 lr:0.004480 loss:1.353711
[Train] epoch:306	batch id:170	 lr:0.004480 loss:1.333372
[Train] epoch:306	batch id:180	 lr:0.004480 loss:1.357800
[Train] epoch:306	batch id:190	 lr:0.004480 loss:1.407278
[Train] epoch:306	batch id:200	 lr:0.004480 loss:1.332569
[Train] epoch:306	batch id:210	 lr:0.004480 loss:1.322841
[Train] epoch:306	batch id:220	 lr:0.004480 loss:1.316106
[Train] epoch:306	batch id:230	 lr:0.004480 loss:1.328722
[Train] epoch:306	batch id:240	 lr:0.004480 loss:1.327203
[Train] epoch:306	batch id:250	 lr:0.004480 loss:1.317793
[Train] epoch:306	batch id:260	 lr:0.004480 loss:1.360547
[Train] epoch:306	batch id:270	 lr:0.004480 loss:1.311667
[Train] epoch:306	batch id:280	 lr:0.004480 loss:1.325962
[Train] epoch:306	batch id:290	 lr:0.004480 loss:1.317479
[Train] epoch:306	batch id:300	 lr:0.004480 loss:1.346064
[Train] 306, loss: 1.340709, train acc: 0.993689, 
[Test] epoch:306	batch id:0	 loss:1.279892
[Test] epoch:306	batch id:10	 loss:1.387408
[Test] epoch:306	batch id:20	 loss:1.418807
[Test] epoch:306	batch id:30	 loss:1.370910
[Test] epoch:306	batch id:40	 loss:1.523890
[Test] epoch:306	batch id:50	 loss:1.529284
[Test] epoch:306	batch id:60	 loss:1.257902
[Test] epoch:306	batch id:70	 loss:1.461848
[Test] epoch:306	batch id:80	 loss:1.607766
[Test] epoch:306	batch id:90	 loss:1.553098
[Test] epoch:306	batch id:100	 loss:1.917240
[Test] epoch:306	batch id:110	 loss:1.327915
[Test] epoch:306	batch id:120	 loss:1.277641
[Test] epoch:306	batch id:130	 loss:1.289437
[Test] epoch:306	batch id:140	 loss:1.286455
[Test] epoch:306	batch id:150	 loss:1.715508
[Test] 306, loss: 1.456071, test acc: 0.920989,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:307	batch id:0	 lr:0.004318 loss:1.322272
[Train] epoch:307	batch id:10	 lr:0.004318 loss:1.320489
[Train] epoch:307	batch id:20	 lr:0.004318 loss:1.338013
[Train] epoch:307	batch id:30	 lr:0.004318 loss:1.372028
[Train] epoch:307	batch id:40	 lr:0.004318 loss:1.339039
[Train] epoch:307	batch id:50	 lr:0.004318 loss:1.325512
[Train] epoch:307	batch id:60	 lr:0.004318 loss:1.343647
[Train] epoch:307	batch id:70	 lr:0.004318 loss:1.330585
[Train] epoch:307	batch id:80	 lr:0.004318 loss:1.326370
[Train] epoch:307	batch id:90	 lr:0.004318 loss:1.295376
[Train] epoch:307	batch id:100	 lr:0.004318 loss:1.312934
[Train] epoch:307	batch id:110	 lr:0.004318 loss:1.316306
[Train] epoch:307	batch id:120	 lr:0.004318 loss:1.367916
[Train] epoch:307	batch id:130	 lr:0.004318 loss:1.314726
[Train] epoch:307	batch id:140	 lr:0.004318 loss:1.323762
[Train] epoch:307	batch id:150	 lr:0.004318 loss:1.338685
[Train] epoch:307	batch id:160	 lr:0.004318 loss:1.326965
[Train] epoch:307	batch id:170	 lr:0.004318 loss:1.314676
[Train] epoch:307	batch id:180	 lr:0.004318 loss:1.342869
[Train] epoch:307	batch id:190	 lr:0.004318 loss:1.320135
[Train] epoch:307	batch id:200	 lr:0.004318 loss:1.330324
[Train] epoch:307	batch id:210	 lr:0.004318 loss:1.357459
[Train] epoch:307	batch id:220	 lr:0.004318 loss:1.309710
[Train] epoch:307	batch id:230	 lr:0.004318 loss:1.383146
[Train] epoch:307	batch id:240	 lr:0.004318 loss:1.339864
[Train] epoch:307	batch id:250	 lr:0.004318 loss:1.357768
[Train] epoch:307	batch id:260	 lr:0.004318 loss:1.333014
[Train] epoch:307	batch id:270	 lr:0.004318 loss:1.407314
[Train] epoch:307	batch id:280	 lr:0.004318 loss:1.359588
[Train] epoch:307	batch id:290	 lr:0.004318 loss:1.319117
[Train] epoch:307	batch id:300	 lr:0.004318 loss:1.310921
[Train] 307, loss: 1.339803, train acc: 0.994300, 
[Test] epoch:307	batch id:0	 loss:1.265933
[Test] epoch:307	batch id:10	 loss:1.409709
[Test] epoch:307	batch id:20	 loss:1.466110
[Test] epoch:307	batch id:30	 loss:1.342510
[Test] epoch:307	batch id:40	 loss:1.661135
[Test] epoch:307	batch id:50	 loss:1.577428
[Test] epoch:307	batch id:60	 loss:1.262314
[Test] epoch:307	batch id:70	 loss:1.375822
[Test] epoch:307	batch id:80	 loss:1.571885
[Test] epoch:307	batch id:90	 loss:1.576035
[Test] epoch:307	batch id:100	 loss:1.892336
[Test] epoch:307	batch id:110	 loss:1.378205
[Test] epoch:307	batch id:120	 loss:1.288129
[Test] epoch:307	batch id:130	 loss:1.333521
[Test] epoch:307	batch id:140	 loss:1.333545
[Test] epoch:307	batch id:150	 loss:1.853094
[Test] 307, loss: 1.466899, test acc: 0.918963,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:308	batch id:0	 lr:0.004160 loss:1.331175
[Train] epoch:308	batch id:10	 lr:0.004160 loss:1.355103
[Train] epoch:308	batch id:20	 lr:0.004160 loss:1.339742
[Train] epoch:308	batch id:30	 lr:0.004160 loss:1.351453
[Train] epoch:308	batch id:40	 lr:0.004160 loss:1.368046
[Train] epoch:308	batch id:50	 lr:0.004160 loss:1.305953
[Train] epoch:308	batch id:60	 lr:0.004160 loss:1.365739
[Train] epoch:308	batch id:70	 lr:0.004160 loss:1.333171
[Train] epoch:308	batch id:80	 lr:0.004160 loss:1.327658
[Train] epoch:308	batch id:90	 lr:0.004160 loss:1.311042
[Train] epoch:308	batch id:100	 lr:0.004160 loss:1.313329
[Train] epoch:308	batch id:110	 lr:0.004160 loss:1.382012
[Train] epoch:308	batch id:120	 lr:0.004160 loss:1.306502
[Train] epoch:308	batch id:130	 lr:0.004160 loss:1.337749
[Train] epoch:308	batch id:140	 lr:0.004160 loss:1.345950
[Train] epoch:308	batch id:150	 lr:0.004160 loss:1.312426
[Train] epoch:308	batch id:160	 lr:0.004160 loss:1.332282
[Train] epoch:308	batch id:170	 lr:0.004160 loss:1.324247
[Train] epoch:308	batch id:180	 lr:0.004160 loss:1.315434
[Train] epoch:308	batch id:190	 lr:0.004160 loss:1.349695
[Train] epoch:308	batch id:200	 lr:0.004160 loss:1.332820
[Train] epoch:308	batch id:210	 lr:0.004160 loss:1.317746
[Train] epoch:308	batch id:220	 lr:0.004160 loss:1.458966
[Train] epoch:308	batch id:230	 lr:0.004160 loss:1.319079
[Train] epoch:308	batch id:240	 lr:0.004160 loss:1.390372
[Train] epoch:308	batch id:250	 lr:0.004160 loss:1.343938
[Train] epoch:308	batch id:260	 lr:0.004160 loss:1.318036
[Train] epoch:308	batch id:270	 lr:0.004160 loss:1.340141
[Train] epoch:308	batch id:280	 lr:0.004160 loss:1.335084
[Train] epoch:308	batch id:290	 lr:0.004160 loss:1.314166
[Train] epoch:308	batch id:300	 lr:0.004160 loss:1.340958
[Train] 308, loss: 1.337086, train acc: 0.995114, 
[Test] epoch:308	batch id:0	 loss:1.288881
[Test] epoch:308	batch id:10	 loss:1.505544
[Test] epoch:308	batch id:20	 loss:1.476850
[Test] epoch:308	batch id:30	 loss:1.400767
[Test] epoch:308	batch id:40	 loss:1.588949
[Test] epoch:308	batch id:50	 loss:1.524984
[Test] epoch:308	batch id:60	 loss:1.252383
[Test] epoch:308	batch id:70	 loss:1.444225
[Test] epoch:308	batch id:80	 loss:1.564654
[Test] epoch:308	batch id:90	 loss:1.598237
[Test] epoch:308	batch id:100	 loss:1.821429
[Test] epoch:308	batch id:110	 loss:1.369249
[Test] epoch:308	batch id:120	 loss:1.320887
[Test] epoch:308	batch id:130	 loss:1.321947
[Test] epoch:308	batch id:140	 loss:1.356354
[Test] epoch:308	batch id:150	 loss:1.621654
[Test] 308, loss: 1.487028, test acc: 0.910049,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:309	batch id:0	 lr:0.004006 loss:1.311647
[Train] epoch:309	batch id:10	 lr:0.004006 loss:1.407285
[Train] epoch:309	batch id:20	 lr:0.004006 loss:1.389449
[Train] epoch:309	batch id:30	 lr:0.004006 loss:1.330303
[Train] epoch:309	batch id:40	 lr:0.004006 loss:1.328772
[Train] epoch:309	batch id:50	 lr:0.004006 loss:1.316326
[Train] epoch:309	batch id:60	 lr:0.004006 loss:1.340515
[Train] epoch:309	batch id:70	 lr:0.004006 loss:1.333563
[Train] epoch:309	batch id:80	 lr:0.004006 loss:1.314173
[Train] epoch:309	batch id:90	 lr:0.004006 loss:1.330971
[Train] epoch:309	batch id:100	 lr:0.004006 loss:1.319751
[Train] epoch:309	batch id:110	 lr:0.004006 loss:1.353225
[Train] epoch:309	batch id:120	 lr:0.004006 loss:1.403099
[Train] epoch:309	batch id:130	 lr:0.004006 loss:1.313601
[Train] epoch:309	batch id:140	 lr:0.004006 loss:1.346475
[Train] epoch:309	batch id:150	 lr:0.004006 loss:1.309039
[Train] epoch:309	batch id:160	 lr:0.004006 loss:1.319198
[Train] epoch:309	batch id:170	 lr:0.004006 loss:1.394885
[Train] epoch:309	batch id:180	 lr:0.004006 loss:1.351885
[Train] epoch:309	batch id:190	 lr:0.004006 loss:1.301254
[Train] epoch:309	batch id:200	 lr:0.004006 loss:1.339134
[Train] epoch:309	batch id:210	 lr:0.004006 loss:1.399160
[Train] epoch:309	batch id:220	 lr:0.004006 loss:1.315823
[Train] epoch:309	batch id:230	 lr:0.004006 loss:1.308318
[Train] epoch:309	batch id:240	 lr:0.004006 loss:1.334410
[Train] epoch:309	batch id:250	 lr:0.004006 loss:1.315195
[Train] epoch:309	batch id:260	 lr:0.004006 loss:1.345959
[Train] epoch:309	batch id:270	 lr:0.004006 loss:1.319825
[Train] epoch:309	batch id:280	 lr:0.004006 loss:1.343900
[Train] epoch:309	batch id:290	 lr:0.004006 loss:1.318876
[Train] epoch:309	batch id:300	 lr:0.004006 loss:1.329221
[Train] 309, loss: 1.337846, train acc: 0.995419, 
[Test] epoch:309	batch id:0	 loss:1.272987
[Test] epoch:309	batch id:10	 loss:1.380360
[Test] epoch:309	batch id:20	 loss:1.398098
[Test] epoch:309	batch id:30	 loss:1.382360
[Test] epoch:309	batch id:40	 loss:1.554680
[Test] epoch:309	batch id:50	 loss:1.505823
[Test] epoch:309	batch id:60	 loss:1.246439
[Test] epoch:309	batch id:70	 loss:1.499706
[Test] epoch:309	batch id:80	 loss:1.554183
[Test] epoch:309	batch id:90	 loss:1.586600
[Test] epoch:309	batch id:100	 loss:1.871150
[Test] epoch:309	batch id:110	 loss:1.391245
[Test] epoch:309	batch id:120	 loss:1.329968
[Test] epoch:309	batch id:130	 loss:1.306898
[Test] epoch:309	batch id:140	 loss:1.402358
[Test] epoch:309	batch id:150	 loss:1.749774
[Test] 309, loss: 1.467450, test acc: 0.915721,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:310	batch id:0	 lr:0.003855 loss:1.348708
[Train] epoch:310	batch id:10	 lr:0.003855 loss:1.310250
[Train] epoch:310	batch id:20	 lr:0.003855 loss:1.331909
[Train] epoch:310	batch id:30	 lr:0.003855 loss:1.311289
[Train] epoch:310	batch id:40	 lr:0.003855 loss:1.328158
[Train] epoch:310	batch id:50	 lr:0.003855 loss:1.332987
[Train] epoch:310	batch id:60	 lr:0.003855 loss:1.316727
[Train] epoch:310	batch id:70	 lr:0.003855 loss:1.291656
[Train] epoch:310	batch id:80	 lr:0.003855 loss:1.385449
[Train] epoch:310	batch id:90	 lr:0.003855 loss:1.321835
[Train] epoch:310	batch id:100	 lr:0.003855 loss:1.334241
[Train] epoch:310	batch id:110	 lr:0.003855 loss:1.323579
[Train] epoch:310	batch id:120	 lr:0.003855 loss:1.334538
[Train] epoch:310	batch id:130	 lr:0.003855 loss:1.407344
[Train] epoch:310	batch id:140	 lr:0.003855 loss:1.321979
[Train] epoch:310	batch id:150	 lr:0.003855 loss:1.324324
[Train] epoch:310	batch id:160	 lr:0.003855 loss:1.334483
[Train] epoch:310	batch id:170	 lr:0.003855 loss:1.351659
[Train] epoch:310	batch id:180	 lr:0.003855 loss:1.363002
[Train] epoch:310	batch id:190	 lr:0.003855 loss:1.351109
[Train] epoch:310	batch id:200	 lr:0.003855 loss:1.388470
[Train] epoch:310	batch id:210	 lr:0.003855 loss:1.350830
[Train] epoch:310	batch id:220	 lr:0.003855 loss:1.375173
[Train] epoch:310	batch id:230	 lr:0.003855 loss:1.317490
[Train] epoch:310	batch id:240	 lr:0.003855 loss:1.304672
[Train] epoch:310	batch id:250	 lr:0.003855 loss:1.315624
[Train] epoch:310	batch id:260	 lr:0.003855 loss:1.305617
[Train] epoch:310	batch id:270	 lr:0.003855 loss:1.330058
[Train] epoch:310	batch id:280	 lr:0.003855 loss:1.320358
[Train] epoch:310	batch id:290	 lr:0.003855 loss:1.342092
[Train] epoch:310	batch id:300	 lr:0.003855 loss:1.372134
[Train] 310, loss: 1.336895, train acc: 0.995216, 
[Test] epoch:310	batch id:0	 loss:1.256242
[Test] epoch:310	batch id:10	 loss:1.440982
[Test] epoch:310	batch id:20	 loss:1.459535
[Test] epoch:310	batch id:30	 loss:1.367082
[Test] epoch:310	batch id:40	 loss:1.557242
[Test] epoch:310	batch id:50	 loss:1.573176
[Test] epoch:310	batch id:60	 loss:1.248058
[Test] epoch:310	batch id:70	 loss:1.454187
[Test] epoch:310	batch id:80	 loss:1.604791
[Test] epoch:310	batch id:90	 loss:1.662309
[Test] epoch:310	batch id:100	 loss:1.782499
[Test] epoch:310	batch id:110	 loss:1.398807
[Test] epoch:310	batch id:120	 loss:1.322758
[Test] epoch:310	batch id:130	 loss:1.381314
[Test] epoch:310	batch id:140	 loss:1.314835
[Test] epoch:310	batch id:150	 loss:1.814497
[Test] 310, loss: 1.479572, test acc: 0.911669,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:311	batch id:0	 lr:0.003709 loss:1.317773
[Train] epoch:311	batch id:10	 lr:0.003709 loss:1.303432
[Train] epoch:311	batch id:20	 lr:0.003709 loss:1.310825
[Train] epoch:311	batch id:30	 lr:0.003709 loss:1.316653
[Train] epoch:311	batch id:40	 lr:0.003709 loss:1.308218
[Train] epoch:311	batch id:50	 lr:0.003709 loss:1.347525
[Train] epoch:311	batch id:60	 lr:0.003709 loss:1.327981
[Train] epoch:311	batch id:70	 lr:0.003709 loss:1.337760
[Train] epoch:311	batch id:80	 lr:0.003709 loss:1.319347
[Train] epoch:311	batch id:90	 lr:0.003709 loss:1.332947
[Train] epoch:311	batch id:100	 lr:0.003709 loss:1.311439
[Train] epoch:311	batch id:110	 lr:0.003709 loss:1.314734
[Train] epoch:311	batch id:120	 lr:0.003709 loss:1.309803
[Train] epoch:311	batch id:130	 lr:0.003709 loss:1.353112
[Train] epoch:311	batch id:140	 lr:0.003709 loss:1.367789
[Train] epoch:311	batch id:150	 lr:0.003709 loss:1.333381
[Train] epoch:311	batch id:160	 lr:0.003709 loss:1.324806
[Train] epoch:311	batch id:170	 lr:0.003709 loss:1.309365
[Train] epoch:311	batch id:180	 lr:0.003709 loss:1.333722
[Train] epoch:311	batch id:190	 lr:0.003709 loss:1.327717
[Train] epoch:311	batch id:200	 lr:0.003709 loss:1.317515
[Train] epoch:311	batch id:210	 lr:0.003709 loss:1.337628
[Train] epoch:311	batch id:220	 lr:0.003709 loss:1.367259
[Train] epoch:311	batch id:230	 lr:0.003709 loss:1.313218
[Train] epoch:311	batch id:240	 lr:0.003709 loss:1.316057
[Train] epoch:311	batch id:250	 lr:0.003709 loss:1.352050
[Train] epoch:311	batch id:260	 lr:0.003709 loss:1.314184
[Train] epoch:311	batch id:270	 lr:0.003709 loss:1.367900
[Train] epoch:311	batch id:280	 lr:0.003709 loss:1.326734
[Train] epoch:311	batch id:290	 lr:0.003709 loss:1.328089
[Train] epoch:311	batch id:300	 lr:0.003709 loss:1.319721
[Train] 311, loss: 1.335390, train acc: 0.995318, 
[Test] epoch:311	batch id:0	 loss:1.282771
[Test] epoch:311	batch id:10	 loss:1.397877
[Test] epoch:311	batch id:20	 loss:1.509227
[Test] epoch:311	batch id:30	 loss:1.326232
[Test] epoch:311	batch id:40	 loss:1.636820
[Test] epoch:311	batch id:50	 loss:1.549850
[Test] epoch:311	batch id:60	 loss:1.278308
[Test] epoch:311	batch id:70	 loss:1.467697
[Test] epoch:311	batch id:80	 loss:1.587531
[Test] epoch:311	batch id:90	 loss:1.646233
[Test] epoch:311	batch id:100	 loss:1.970613
[Test] epoch:311	batch id:110	 loss:1.413780
[Test] epoch:311	batch id:120	 loss:1.345510
[Test] epoch:311	batch id:130	 loss:1.320361
[Test] epoch:311	batch id:140	 loss:1.326110
[Test] epoch:311	batch id:150	 loss:1.762398
[Test] 311, loss: 1.488782, test acc: 0.918558,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:312	batch id:0	 lr:0.003566 loss:1.329748
[Train] epoch:312	batch id:10	 lr:0.003566 loss:1.313422
[Train] epoch:312	batch id:20	 lr:0.003566 loss:1.386145
[Train] epoch:312	batch id:30	 lr:0.003566 loss:1.358031
[Train] epoch:312	batch id:40	 lr:0.003566 loss:1.343613
[Train] epoch:312	batch id:50	 lr:0.003566 loss:1.310983
[Train] epoch:312	batch id:60	 lr:0.003566 loss:1.376279
[Train] epoch:312	batch id:70	 lr:0.003566 loss:1.337194
[Train] epoch:312	batch id:80	 lr:0.003566 loss:1.381604
[Train] epoch:312	batch id:90	 lr:0.003566 loss:1.338796
[Train] epoch:312	batch id:100	 lr:0.003566 loss:1.317164
[Train] epoch:312	batch id:110	 lr:0.003566 loss:1.307831
[Train] epoch:312	batch id:120	 lr:0.003566 loss:1.310822
[Train] epoch:312	batch id:130	 lr:0.003566 loss:1.326966
[Train] epoch:312	batch id:140	 lr:0.003566 loss:1.313349
[Train] epoch:312	batch id:150	 lr:0.003566 loss:1.313402
[Train] epoch:312	batch id:160	 lr:0.003566 loss:1.316020
[Train] epoch:312	batch id:170	 lr:0.003566 loss:1.378284
[Train] epoch:312	batch id:180	 lr:0.003566 loss:1.352551
[Train] epoch:312	batch id:190	 lr:0.003566 loss:1.352062
[Train] epoch:312	batch id:200	 lr:0.003566 loss:1.323581
[Train] epoch:312	batch id:210	 lr:0.003566 loss:1.339676
[Train] epoch:312	batch id:220	 lr:0.003566 loss:1.322448
[Train] epoch:312	batch id:230	 lr:0.003566 loss:1.347812
[Train] epoch:312	batch id:240	 lr:0.003566 loss:1.473627
[Train] epoch:312	batch id:250	 lr:0.003566 loss:1.329438
[Train] epoch:312	batch id:260	 lr:0.003566 loss:1.314327
[Train] epoch:312	batch id:270	 lr:0.003566 loss:1.362987
[Train] epoch:312	batch id:280	 lr:0.003566 loss:1.333102
[Train] epoch:312	batch id:290	 lr:0.003566 loss:1.318153
[Train] epoch:312	batch id:300	 lr:0.003566 loss:1.367280
[Train] 312, loss: 1.336356, train acc: 0.995521, 
[Test] epoch:312	batch id:0	 loss:1.258642
[Test] epoch:312	batch id:10	 loss:1.472777
[Test] epoch:312	batch id:20	 loss:1.420059
[Test] epoch:312	batch id:30	 loss:1.395317
[Test] epoch:312	batch id:40	 loss:1.544826
[Test] epoch:312	batch id:50	 loss:1.514801
[Test] epoch:312	batch id:60	 loss:1.261354
[Test] epoch:312	batch id:70	 loss:1.469772
[Test] epoch:312	batch id:80	 loss:1.601436
[Test] epoch:312	batch id:90	 loss:1.591362
[Test] epoch:312	batch id:100	 loss:2.001033
[Test] epoch:312	batch id:110	 loss:1.337535
[Test] epoch:312	batch id:120	 loss:1.348404
[Test] epoch:312	batch id:130	 loss:1.284090
[Test] epoch:312	batch id:140	 loss:1.297709
[Test] epoch:312	batch id:150	 loss:1.661650
[Test] 312, loss: 1.463133, test acc: 0.924230,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:313	batch id:0	 lr:0.003426 loss:1.364269
[Train] epoch:313	batch id:10	 lr:0.003426 loss:1.302577
[Train] epoch:313	batch id:20	 lr:0.003426 loss:1.327604
[Train] epoch:313	batch id:30	 lr:0.003426 loss:1.309779
[Train] epoch:313	batch id:40	 lr:0.003426 loss:1.317642
[Train] epoch:313	batch id:50	 lr:0.003426 loss:1.304195
[Train] epoch:313	batch id:60	 lr:0.003426 loss:1.316742
[Train] epoch:313	batch id:70	 lr:0.003426 loss:1.337953
[Train] epoch:313	batch id:80	 lr:0.003426 loss:1.330469
[Train] epoch:313	batch id:90	 lr:0.003426 loss:1.345203
[Train] epoch:313	batch id:100	 lr:0.003426 loss:1.396290
[Train] epoch:313	batch id:110	 lr:0.003426 loss:1.343448
[Train] epoch:313	batch id:120	 lr:0.003426 loss:1.382765
[Train] epoch:313	batch id:130	 lr:0.003426 loss:1.319167
[Train] epoch:313	batch id:140	 lr:0.003426 loss:1.305674
[Train] epoch:313	batch id:150	 lr:0.003426 loss:1.354896
[Train] epoch:313	batch id:160	 lr:0.003426 loss:1.365248
[Train] epoch:313	batch id:170	 lr:0.003426 loss:1.347017
[Train] epoch:313	batch id:180	 lr:0.003426 loss:1.355734
[Train] epoch:313	batch id:190	 lr:0.003426 loss:1.377336
[Train] epoch:313	batch id:200	 lr:0.003426 loss:1.295634
[Train] epoch:313	batch id:210	 lr:0.003426 loss:1.331057
[Train] epoch:313	batch id:220	 lr:0.003426 loss:1.353342
[Train] epoch:313	batch id:230	 lr:0.003426 loss:1.353719
[Train] epoch:313	batch id:240	 lr:0.003426 loss:1.337761
[Train] epoch:313	batch id:250	 lr:0.003426 loss:1.306894
[Train] epoch:313	batch id:260	 lr:0.003426 loss:1.331862
[Train] epoch:313	batch id:270	 lr:0.003426 loss:1.335758
[Train] epoch:313	batch id:280	 lr:0.003426 loss:1.321647
[Train] epoch:313	batch id:290	 lr:0.003426 loss:1.332487
[Train] epoch:313	batch id:300	 lr:0.003426 loss:1.357401
[Train] 313, loss: 1.335067, train acc: 0.995725, 
[Test] epoch:313	batch id:0	 loss:1.252368
[Test] epoch:313	batch id:10	 loss:1.417368
[Test] epoch:313	batch id:20	 loss:1.500734
[Test] epoch:313	batch id:30	 loss:1.322896
[Test] epoch:313	batch id:40	 loss:1.508199
[Test] epoch:313	batch id:50	 loss:1.512158
[Test] epoch:313	batch id:60	 loss:1.243434
[Test] epoch:313	batch id:70	 loss:1.482126
[Test] epoch:313	batch id:80	 loss:1.626853
[Test] epoch:313	batch id:90	 loss:1.548254
[Test] epoch:313	batch id:100	 loss:1.821792
[Test] epoch:313	batch id:110	 loss:1.377235
[Test] epoch:313	batch id:120	 loss:1.313708
[Test] epoch:313	batch id:130	 loss:1.312372
[Test] epoch:313	batch id:140	 loss:1.313621
[Test] epoch:313	batch id:150	 loss:1.720443
[Test] 313, loss: 1.454731, test acc: 0.923015,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:314	batch id:0	 lr:0.003291 loss:1.312604
[Train] epoch:314	batch id:10	 lr:0.003291 loss:1.334262
[Train] epoch:314	batch id:20	 lr:0.003291 loss:1.353824
[Train] epoch:314	batch id:30	 lr:0.003291 loss:1.313304
[Train] epoch:314	batch id:40	 lr:0.003291 loss:1.331442
[Train] epoch:314	batch id:50	 lr:0.003291 loss:1.331249
[Train] epoch:314	batch id:60	 lr:0.003291 loss:1.353957
[Train] epoch:314	batch id:70	 lr:0.003291 loss:1.315909
[Train] epoch:314	batch id:80	 lr:0.003291 loss:1.346952
[Train] epoch:314	batch id:90	 lr:0.003291 loss:1.309515
[Train] epoch:314	batch id:100	 lr:0.003291 loss:1.316035
[Train] epoch:314	batch id:110	 lr:0.003291 loss:1.332227
[Train] epoch:314	batch id:120	 lr:0.003291 loss:1.323873
[Train] epoch:314	batch id:130	 lr:0.003291 loss:1.314309
[Train] epoch:314	batch id:140	 lr:0.003291 loss:1.320909
[Train] epoch:314	batch id:150	 lr:0.003291 loss:1.304966
[Train] epoch:314	batch id:160	 lr:0.003291 loss:1.330105
[Train] epoch:314	batch id:170	 lr:0.003291 loss:1.346178
[Train] epoch:314	batch id:180	 lr:0.003291 loss:1.321306
[Train] epoch:314	batch id:190	 lr:0.003291 loss:1.337855
[Train] epoch:314	batch id:200	 lr:0.003291 loss:1.340033
[Train] epoch:314	batch id:210	 lr:0.003291 loss:1.336302
[Train] epoch:314	batch id:220	 lr:0.003291 loss:1.362369
[Train] epoch:314	batch id:230	 lr:0.003291 loss:1.330432
[Train] epoch:314	batch id:240	 lr:0.003291 loss:1.303664
[Train] epoch:314	batch id:250	 lr:0.003291 loss:1.323310
[Train] epoch:314	batch id:260	 lr:0.003291 loss:1.332303
[Train] epoch:314	batch id:270	 lr:0.003291 loss:1.329660
[Train] epoch:314	batch id:280	 lr:0.003291 loss:1.319217
[Train] epoch:314	batch id:290	 lr:0.003291 loss:1.314232
[Train] epoch:314	batch id:300	 lr:0.003291 loss:1.433416
[Train] 314, loss: 1.335924, train acc: 0.995419, 
[Test] epoch:314	batch id:0	 loss:1.247375
[Test] epoch:314	batch id:10	 loss:1.407110
[Test] epoch:314	batch id:20	 loss:1.460581
[Test] epoch:314	batch id:30	 loss:1.384446
[Test] epoch:314	batch id:40	 loss:1.548504
[Test] epoch:314	batch id:50	 loss:1.587182
[Test] epoch:314	batch id:60	 loss:1.248400
[Test] epoch:314	batch id:70	 loss:1.455195
[Test] epoch:314	batch id:80	 loss:1.636655
[Test] epoch:314	batch id:90	 loss:1.627334
[Test] epoch:314	batch id:100	 loss:1.974926
[Test] epoch:314	batch id:110	 loss:1.398327
[Test] epoch:314	batch id:120	 loss:1.421613
[Test] epoch:314	batch id:130	 loss:1.318696
[Test] epoch:314	batch id:140	 loss:1.406699
[Test] epoch:314	batch id:150	 loss:1.729422
[Test] 314, loss: 1.481558, test acc: 0.912885,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:315	batch id:0	 lr:0.003159 loss:1.328504
[Train] epoch:315	batch id:10	 lr:0.003159 loss:1.328479
[Train] epoch:315	batch id:20	 lr:0.003159 loss:1.356480
[Train] epoch:315	batch id:30	 lr:0.003159 loss:1.386463
[Train] epoch:315	batch id:40	 lr:0.003159 loss:1.334331
[Train] epoch:315	batch id:50	 lr:0.003159 loss:1.419979
[Train] epoch:315	batch id:60	 lr:0.003159 loss:1.306696
[Train] epoch:315	batch id:70	 lr:0.003159 loss:1.375996
[Train] epoch:315	batch id:80	 lr:0.003159 loss:1.340205
[Train] epoch:315	batch id:90	 lr:0.003159 loss:1.306532
[Train] epoch:315	batch id:100	 lr:0.003159 loss:1.335943
[Train] epoch:315	batch id:110	 lr:0.003159 loss:1.343874
[Train] epoch:315	batch id:120	 lr:0.003159 loss:1.334975
[Train] epoch:315	batch id:130	 lr:0.003159 loss:1.319197
[Train] epoch:315	batch id:140	 lr:0.003159 loss:1.352963
[Train] epoch:315	batch id:150	 lr:0.003159 loss:1.363379
[Train] epoch:315	batch id:160	 lr:0.003159 loss:1.315669
[Train] epoch:315	batch id:170	 lr:0.003159 loss:1.374833
[Train] epoch:315	batch id:180	 lr:0.003159 loss:1.342376
[Train] epoch:315	batch id:190	 lr:0.003159 loss:1.343803
[Train] epoch:315	batch id:200	 lr:0.003159 loss:1.379521
[Train] epoch:315	batch id:210	 lr:0.003159 loss:1.311735
[Train] epoch:315	batch id:220	 lr:0.003159 loss:1.323405
[Train] epoch:315	batch id:230	 lr:0.003159 loss:1.330086
[Train] epoch:315	batch id:240	 lr:0.003159 loss:1.315050
[Train] epoch:315	batch id:250	 lr:0.003159 loss:1.323452
[Train] epoch:315	batch id:260	 lr:0.003159 loss:1.311772
[Train] epoch:315	batch id:270	 lr:0.003159 loss:1.388983
[Train] epoch:315	batch id:280	 lr:0.003159 loss:1.379203
[Train] epoch:315	batch id:290	 lr:0.003159 loss:1.360220
[Train] epoch:315	batch id:300	 lr:0.003159 loss:1.330231
[Train] 315, loss: 1.337335, train acc: 0.994809, 
[Test] epoch:315	batch id:0	 loss:1.255302
[Test] epoch:315	batch id:10	 loss:1.518577
[Test] epoch:315	batch id:20	 loss:1.447851
[Test] epoch:315	batch id:30	 loss:1.402074
[Test] epoch:315	batch id:40	 loss:1.696246
[Test] epoch:315	batch id:50	 loss:1.494531
[Test] epoch:315	batch id:60	 loss:1.248551
[Test] epoch:315	batch id:70	 loss:1.424224
[Test] epoch:315	batch id:80	 loss:1.620318
[Test] epoch:315	batch id:90	 loss:1.550816
[Test] epoch:315	batch id:100	 loss:2.113046
[Test] epoch:315	batch id:110	 loss:1.267794
[Test] epoch:315	batch id:120	 loss:1.315788
[Test] epoch:315	batch id:130	 loss:1.267425
[Test] epoch:315	batch id:140	 loss:1.376736
[Test] epoch:315	batch id:150	 loss:1.794452
[Test] 315, loss: 1.482451, test acc: 0.909238,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:316	batch id:0	 lr:0.003032 loss:1.306145
[Train] epoch:316	batch id:10	 lr:0.003032 loss:1.305990
[Train] epoch:316	batch id:20	 lr:0.003032 loss:1.319783
[Train] epoch:316	batch id:30	 lr:0.003032 loss:1.303185
[Train] epoch:316	batch id:40	 lr:0.003032 loss:1.337294
[Train] epoch:316	batch id:50	 lr:0.003032 loss:1.320134
[Train] epoch:316	batch id:60	 lr:0.003032 loss:1.340453
[Train] epoch:316	batch id:70	 lr:0.003032 loss:1.320084
[Train] epoch:316	batch id:80	 lr:0.003032 loss:1.333165
[Train] epoch:316	batch id:90	 lr:0.003032 loss:1.326330
[Train] epoch:316	batch id:100	 lr:0.003032 loss:1.324658
[Train] epoch:316	batch id:110	 lr:0.003032 loss:1.329613
[Train] epoch:316	batch id:120	 lr:0.003032 loss:1.326929
[Train] epoch:316	batch id:130	 lr:0.003032 loss:1.314653
[Train] epoch:316	batch id:140	 lr:0.003032 loss:1.326417
[Train] epoch:316	batch id:150	 lr:0.003032 loss:1.328769
[Train] epoch:316	batch id:160	 lr:0.003032 loss:1.391567
[Train] epoch:316	batch id:170	 lr:0.003032 loss:1.298843
[Train] epoch:316	batch id:180	 lr:0.003032 loss:1.315169
[Train] epoch:316	batch id:190	 lr:0.003032 loss:1.311546
[Train] epoch:316	batch id:200	 lr:0.003032 loss:1.339100
[Train] epoch:316	batch id:210	 lr:0.003032 loss:1.348797
[Train] epoch:316	batch id:220	 lr:0.003032 loss:1.334432
[Train] epoch:316	batch id:230	 lr:0.003032 loss:1.334307
[Train] epoch:316	batch id:240	 lr:0.003032 loss:1.454296
[Train] epoch:316	batch id:250	 lr:0.003032 loss:1.373913
[Train] epoch:316	batch id:260	 lr:0.003032 loss:1.359191
[Train] epoch:316	batch id:270	 lr:0.003032 loss:1.317288
[Train] epoch:316	batch id:280	 lr:0.003032 loss:1.329030
[Train] epoch:316	batch id:290	 lr:0.003032 loss:1.330276
[Train] epoch:316	batch id:300	 lr:0.003032 loss:1.340323
[Train] 316, loss: 1.336454, train acc: 0.995521, 
[Test] epoch:316	batch id:0	 loss:1.264926
[Test] epoch:316	batch id:10	 loss:1.480448
[Test] epoch:316	batch id:20	 loss:1.398788
[Test] epoch:316	batch id:30	 loss:1.314686
[Test] epoch:316	batch id:40	 loss:1.521004
[Test] epoch:316	batch id:50	 loss:1.495194
[Test] epoch:316	batch id:60	 loss:1.244079
[Test] epoch:316	batch id:70	 loss:1.383776
[Test] epoch:316	batch id:80	 loss:1.623065
[Test] epoch:316	batch id:90	 loss:1.558175
[Test] epoch:316	batch id:100	 loss:2.047685
[Test] epoch:316	batch id:110	 loss:1.350630
[Test] epoch:316	batch id:120	 loss:1.320740
[Test] epoch:316	batch id:130	 loss:1.294017
[Test] epoch:316	batch id:140	 loss:1.320816
[Test] epoch:316	batch id:150	 loss:1.707608
[Test] 316, loss: 1.467908, test acc: 0.916532,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:317	batch id:0	 lr:0.002908 loss:1.314797
[Train] epoch:317	batch id:10	 lr:0.002908 loss:1.304464
[Train] epoch:317	batch id:20	 lr:0.002908 loss:1.311107
[Train] epoch:317	batch id:30	 lr:0.002908 loss:1.381280
[Train] epoch:317	batch id:40	 lr:0.002908 loss:1.322870
[Train] epoch:317	batch id:50	 lr:0.002908 loss:1.330303
[Train] epoch:317	batch id:60	 lr:0.002908 loss:1.342221
[Train] epoch:317	batch id:70	 lr:0.002908 loss:1.331589
[Train] epoch:317	batch id:80	 lr:0.002908 loss:1.317513
[Train] epoch:317	batch id:90	 lr:0.002908 loss:1.334488
[Train] epoch:317	batch id:100	 lr:0.002908 loss:1.346606
[Train] epoch:317	batch id:110	 lr:0.002908 loss:1.335213
[Train] epoch:317	batch id:120	 lr:0.002908 loss:1.345297
[Train] epoch:317	batch id:130	 lr:0.002908 loss:1.318374
[Train] epoch:317	batch id:140	 lr:0.002908 loss:1.306577
[Train] epoch:317	batch id:150	 lr:0.002908 loss:1.315777
[Train] epoch:317	batch id:160	 lr:0.002908 loss:1.333253
[Train] epoch:317	batch id:170	 lr:0.002908 loss:1.301192
[Train] epoch:317	batch id:180	 lr:0.002908 loss:1.354979
[Train] epoch:317	batch id:190	 lr:0.002908 loss:1.297570
[Train] epoch:317	batch id:200	 lr:0.002908 loss:1.314813
[Train] epoch:317	batch id:210	 lr:0.002908 loss:1.316148
[Train] epoch:317	batch id:220	 lr:0.002908 loss:1.348131
[Train] epoch:317	batch id:230	 lr:0.002908 loss:1.342784
[Train] epoch:317	batch id:240	 lr:0.002908 loss:1.333285
[Train] epoch:317	batch id:250	 lr:0.002908 loss:1.319667
[Train] epoch:317	batch id:260	 lr:0.002908 loss:1.374531
[Train] epoch:317	batch id:270	 lr:0.002908 loss:1.342345
[Train] epoch:317	batch id:280	 lr:0.002908 loss:1.335359
[Train] epoch:317	batch id:290	 lr:0.002908 loss:1.351511
[Train] epoch:317	batch id:300	 lr:0.002908 loss:1.353500
[Train] 317, loss: 1.334383, train acc: 0.996030, 
[Test] epoch:317	batch id:0	 loss:1.259772
[Test] epoch:317	batch id:10	 loss:1.444144
[Test] epoch:317	batch id:20	 loss:1.389966
[Test] epoch:317	batch id:30	 loss:1.362126
[Test] epoch:317	batch id:40	 loss:1.610065
[Test] epoch:317	batch id:50	 loss:1.536974
[Test] epoch:317	batch id:60	 loss:1.249765
[Test] epoch:317	batch id:70	 loss:1.373303
[Test] epoch:317	batch id:80	 loss:1.511611
[Test] epoch:317	batch id:90	 loss:1.539449
[Test] epoch:317	batch id:100	 loss:2.003244
[Test] epoch:317	batch id:110	 loss:1.383163
[Test] epoch:317	batch id:120	 loss:1.300572
[Test] epoch:317	batch id:130	 loss:1.294923
[Test] epoch:317	batch id:140	 loss:1.356633
[Test] epoch:317	batch id:150	 loss:1.656448
[Test] 317, loss: 1.470171, test acc: 0.912075,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:318	batch id:0	 lr:0.002788 loss:1.349786
[Train] epoch:318	batch id:10	 lr:0.002788 loss:1.335917
[Train] epoch:318	batch id:20	 lr:0.002788 loss:1.359519
[Train] epoch:318	batch id:30	 lr:0.002788 loss:1.334590
[Train] epoch:318	batch id:40	 lr:0.002788 loss:1.334920
[Train] epoch:318	batch id:50	 lr:0.002788 loss:1.317239
[Train] epoch:318	batch id:60	 lr:0.002788 loss:1.380189
[Train] epoch:318	batch id:70	 lr:0.002788 loss:1.330492
[Train] epoch:318	batch id:80	 lr:0.002788 loss:1.347187
[Train] epoch:318	batch id:90	 lr:0.002788 loss:1.330777
[Train] epoch:318	batch id:100	 lr:0.002788 loss:1.354834
[Train] epoch:318	batch id:110	 lr:0.002788 loss:1.319412
[Train] epoch:318	batch id:120	 lr:0.002788 loss:1.317018
[Train] epoch:318	batch id:130	 lr:0.002788 loss:1.364832
[Train] epoch:318	batch id:140	 lr:0.002788 loss:1.361619
[Train] epoch:318	batch id:150	 lr:0.002788 loss:1.315050
[Train] epoch:318	batch id:160	 lr:0.002788 loss:1.303809
[Train] epoch:318	batch id:170	 lr:0.002788 loss:1.336623
[Train] epoch:318	batch id:180	 lr:0.002788 loss:1.330843
[Train] epoch:318	batch id:190	 lr:0.002788 loss:1.393038
[Train] epoch:318	batch id:200	 lr:0.002788 loss:1.311570
[Train] epoch:318	batch id:210	 lr:0.002788 loss:1.313668
[Train] epoch:318	batch id:220	 lr:0.002788 loss:1.368047
[Train] epoch:318	batch id:230	 lr:0.002788 loss:1.340885
[Train] epoch:318	batch id:240	 lr:0.002788 loss:1.303422
[Train] epoch:318	batch id:250	 lr:0.002788 loss:1.459089
[Train] epoch:318	batch id:260	 lr:0.002788 loss:1.322853
[Train] epoch:318	batch id:270	 lr:0.002788 loss:1.321435
[Train] epoch:318	batch id:280	 lr:0.002788 loss:1.347593
[Train] epoch:318	batch id:290	 lr:0.002788 loss:1.328400
[Train] epoch:318	batch id:300	 lr:0.002788 loss:1.326566
[Train] 318, loss: 1.336855, train acc: 0.994605, 
[Test] epoch:318	batch id:0	 loss:1.279289
[Test] epoch:318	batch id:10	 loss:1.456564
[Test] epoch:318	batch id:20	 loss:1.475151
[Test] epoch:318	batch id:30	 loss:1.339857
[Test] epoch:318	batch id:40	 loss:1.565711
[Test] epoch:318	batch id:50	 loss:1.528907
[Test] epoch:318	batch id:60	 loss:1.252737
[Test] epoch:318	batch id:70	 loss:1.463435
[Test] epoch:318	batch id:80	 loss:1.553426
[Test] epoch:318	batch id:90	 loss:1.502648
[Test] epoch:318	batch id:100	 loss:2.028131
[Test] epoch:318	batch id:110	 loss:1.334384
[Test] epoch:318	batch id:120	 loss:1.296395
[Test] epoch:318	batch id:130	 loss:1.289411
[Test] epoch:318	batch id:140	 loss:1.344265
[Test] epoch:318	batch id:150	 loss:1.617323
[Test] 318, loss: 1.464732, test acc: 0.916937,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:319	batch id:0	 lr:0.002671 loss:1.322270
[Train] epoch:319	batch id:10	 lr:0.002671 loss:1.317340
[Train] epoch:319	batch id:20	 lr:0.002671 loss:1.313553
[Train] epoch:319	batch id:30	 lr:0.002671 loss:1.371811
[Train] epoch:319	batch id:40	 lr:0.002671 loss:1.307681
[Train] epoch:319	batch id:50	 lr:0.002671 loss:1.325984
[Train] epoch:319	batch id:60	 lr:0.002671 loss:1.352878
[Train] epoch:319	batch id:70	 lr:0.002671 loss:1.321814
[Train] epoch:319	batch id:80	 lr:0.002671 loss:1.329339
[Train] epoch:319	batch id:90	 lr:0.002671 loss:1.328608
[Train] epoch:319	batch id:100	 lr:0.002671 loss:1.323602
[Train] epoch:319	batch id:110	 lr:0.002671 loss:1.306450
[Train] epoch:319	batch id:120	 lr:0.002671 loss:1.317473
[Train] epoch:319	batch id:130	 lr:0.002671 loss:1.313078
[Train] epoch:319	batch id:140	 lr:0.002671 loss:1.346793
[Train] epoch:319	batch id:150	 lr:0.002671 loss:1.364270
[Train] epoch:319	batch id:160	 lr:0.002671 loss:1.339614
[Train] epoch:319	batch id:170	 lr:0.002671 loss:1.318147
[Train] epoch:319	batch id:180	 lr:0.002671 loss:1.333517
[Train] epoch:319	batch id:190	 lr:0.002671 loss:1.322151
[Train] epoch:319	batch id:200	 lr:0.002671 loss:1.343261
[Train] epoch:319	batch id:210	 lr:0.002671 loss:1.314449
[Train] epoch:319	batch id:220	 lr:0.002671 loss:1.340933
[Train] epoch:319	batch id:230	 lr:0.002671 loss:1.320057
[Train] epoch:319	batch id:240	 lr:0.002671 loss:1.328606
[Train] epoch:319	batch id:250	 lr:0.002671 loss:1.328914
[Train] epoch:319	batch id:260	 lr:0.002671 loss:1.317456
[Train] epoch:319	batch id:270	 lr:0.002671 loss:1.327004
[Train] epoch:319	batch id:280	 lr:0.002671 loss:1.352986
[Train] epoch:319	batch id:290	 lr:0.002671 loss:1.315863
[Train] epoch:319	batch id:300	 lr:0.002671 loss:1.312557
[Train] 319, loss: 1.333342, train acc: 0.995623, 
[Test] epoch:319	batch id:0	 loss:1.254178
[Test] epoch:319	batch id:10	 loss:1.464427
[Test] epoch:319	batch id:20	 loss:1.448884
[Test] epoch:319	batch id:30	 loss:1.360396
[Test] epoch:319	batch id:40	 loss:1.519778
[Test] epoch:319	batch id:50	 loss:1.517419
[Test] epoch:319	batch id:60	 loss:1.242970
[Test] epoch:319	batch id:70	 loss:1.432809
[Test] epoch:319	batch id:80	 loss:1.624260
[Test] epoch:319	batch id:90	 loss:1.604648
[Test] epoch:319	batch id:100	 loss:1.952770
[Test] epoch:319	batch id:110	 loss:1.318269
[Test] epoch:319	batch id:120	 loss:1.376759
[Test] epoch:319	batch id:130	 loss:1.281278
[Test] epoch:319	batch id:140	 loss:1.409441
[Test] epoch:319	batch id:150	 loss:1.705297
[Test] 319, loss: 1.470329, test acc: 0.915721,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:320	batch id:0	 lr:0.002559 loss:1.327950
[Train] epoch:320	batch id:10	 lr:0.002559 loss:1.319113
[Train] epoch:320	batch id:20	 lr:0.002559 loss:1.365083
[Train] epoch:320	batch id:30	 lr:0.002559 loss:1.350956
[Train] epoch:320	batch id:40	 lr:0.002559 loss:1.315103
[Train] epoch:320	batch id:50	 lr:0.002559 loss:1.347034
[Train] epoch:320	batch id:60	 lr:0.002559 loss:1.341710
[Train] epoch:320	batch id:70	 lr:0.002559 loss:1.329545
[Train] epoch:320	batch id:80	 lr:0.002559 loss:1.304134
[Train] epoch:320	batch id:90	 lr:0.002559 loss:1.363081
[Train] epoch:320	batch id:100	 lr:0.002559 loss:1.346888
[Train] epoch:320	batch id:110	 lr:0.002559 loss:1.322773
[Train] epoch:320	batch id:120	 lr:0.002559 loss:1.320940
[Train] epoch:320	batch id:130	 lr:0.002559 loss:1.479067
[Train] epoch:320	batch id:140	 lr:0.002559 loss:1.334534
[Train] epoch:320	batch id:150	 lr:0.002559 loss:1.396792
[Train] epoch:320	batch id:160	 lr:0.002559 loss:1.348192
[Train] epoch:320	batch id:170	 lr:0.002559 loss:1.330949
[Train] epoch:320	batch id:180	 lr:0.002559 loss:1.362806
[Train] epoch:320	batch id:190	 lr:0.002559 loss:1.364583
[Train] epoch:320	batch id:200	 lr:0.002559 loss:1.329581
[Train] epoch:320	batch id:210	 lr:0.002559 loss:1.335438
[Train] epoch:320	batch id:220	 lr:0.002559 loss:1.335655
[Train] epoch:320	batch id:230	 lr:0.002559 loss:1.344252
[Train] epoch:320	batch id:240	 lr:0.002559 loss:1.317071
[Train] epoch:320	batch id:250	 lr:0.002559 loss:1.321686
[Train] epoch:320	batch id:260	 lr:0.002559 loss:1.367785
[Train] epoch:320	batch id:270	 lr:0.002559 loss:1.325833
[Train] epoch:320	batch id:280	 lr:0.002559 loss:1.334125
[Train] epoch:320	batch id:290	 lr:0.002559 loss:1.304953
[Train] epoch:320	batch id:300	 lr:0.002559 loss:1.360290
[Train] 320, loss: 1.334178, train acc: 0.995623, 
[Test] epoch:320	batch id:0	 loss:1.258954
[Test] epoch:320	batch id:10	 loss:1.344679
[Test] epoch:320	batch id:20	 loss:1.391201
[Test] epoch:320	batch id:30	 loss:1.339112
[Test] epoch:320	batch id:40	 loss:1.550856
[Test] epoch:320	batch id:50	 loss:1.631253
[Test] epoch:320	batch id:60	 loss:1.252846
[Test] epoch:320	batch id:70	 loss:1.484415
[Test] epoch:320	batch id:80	 loss:1.716863
[Test] epoch:320	batch id:90	 loss:1.651883
[Test] epoch:320	batch id:100	 loss:1.912416
[Test] epoch:320	batch id:110	 loss:1.375416
[Test] epoch:320	batch id:120	 loss:1.294222
[Test] epoch:320	batch id:130	 loss:1.312598
[Test] epoch:320	batch id:140	 loss:1.296473
[Test] epoch:320	batch id:150	 loss:1.704539
[Test] 320, loss: 1.474619, test acc: 0.916126,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:321	batch id:0	 lr:0.002450 loss:1.382098
[Train] epoch:321	batch id:10	 lr:0.002450 loss:1.351114
[Train] epoch:321	batch id:20	 lr:0.002450 loss:1.316902
[Train] epoch:321	batch id:30	 lr:0.002450 loss:1.317076
[Train] epoch:321	batch id:40	 lr:0.002450 loss:1.318315
[Train] epoch:321	batch id:50	 lr:0.002450 loss:1.308174
[Train] epoch:321	batch id:60	 lr:0.002450 loss:1.353082
[Train] epoch:321	batch id:70	 lr:0.002450 loss:1.334224
[Train] epoch:321	batch id:80	 lr:0.002450 loss:1.305181
[Train] epoch:321	batch id:90	 lr:0.002450 loss:1.317292
[Train] epoch:321	batch id:100	 lr:0.002450 loss:1.322415
[Train] epoch:321	batch id:110	 lr:0.002450 loss:1.313276
[Train] epoch:321	batch id:120	 lr:0.002450 loss:1.329005
[Train] epoch:321	batch id:130	 lr:0.002450 loss:1.308382
[Train] epoch:321	batch id:140	 lr:0.002450 loss:1.321494
[Train] epoch:321	batch id:150	 lr:0.002450 loss:1.297771
[Train] epoch:321	batch id:160	 lr:0.002450 loss:1.327761
[Train] epoch:321	batch id:170	 lr:0.002450 loss:1.384777
[Train] epoch:321	batch id:180	 lr:0.002450 loss:1.310362
[Train] epoch:321	batch id:190	 lr:0.002450 loss:1.317140
[Train] epoch:321	batch id:200	 lr:0.002450 loss:1.338556
[Train] epoch:321	batch id:210	 lr:0.002450 loss:1.310632
[Train] epoch:321	batch id:220	 lr:0.002450 loss:1.346285
[Train] epoch:321	batch id:230	 lr:0.002450 loss:1.323929
[Train] epoch:321	batch id:240	 lr:0.002450 loss:1.340107
[Train] epoch:321	batch id:250	 lr:0.002450 loss:1.342540
[Train] epoch:321	batch id:260	 lr:0.002450 loss:1.318894
[Train] epoch:321	batch id:270	 lr:0.002450 loss:1.323031
[Train] epoch:321	batch id:280	 lr:0.002450 loss:1.303866
[Train] epoch:321	batch id:290	 lr:0.002450 loss:1.354336
[Train] epoch:321	batch id:300	 lr:0.002450 loss:1.333998
[Train] 321, loss: 1.333238, train acc: 0.995318, 
[Test] epoch:321	batch id:0	 loss:1.265931
[Test] epoch:321	batch id:10	 loss:1.437940
[Test] epoch:321	batch id:20	 loss:1.385949
[Test] epoch:321	batch id:30	 loss:1.306167
[Test] epoch:321	batch id:40	 loss:1.500307
[Test] epoch:321	batch id:50	 loss:1.552077
[Test] epoch:321	batch id:60	 loss:1.247089
[Test] epoch:321	batch id:70	 loss:1.405710
[Test] epoch:321	batch id:80	 loss:1.635587
[Test] epoch:321	batch id:90	 loss:1.666191
[Test] epoch:321	batch id:100	 loss:1.910012
[Test] epoch:321	batch id:110	 loss:1.395735
[Test] epoch:321	batch id:120	 loss:1.292886
[Test] epoch:321	batch id:130	 loss:1.328427
[Test] epoch:321	batch id:140	 loss:1.416252
[Test] epoch:321	batch id:150	 loss:1.695172
[Test] 321, loss: 1.473875, test acc: 0.916532,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:322	batch id:0	 lr:0.002346 loss:1.346308
[Train] epoch:322	batch id:10	 lr:0.002346 loss:1.312331
[Train] epoch:322	batch id:20	 lr:0.002346 loss:1.322843
[Train] epoch:322	batch id:30	 lr:0.002346 loss:1.325599
[Train] epoch:322	batch id:40	 lr:0.002346 loss:1.356552
[Train] epoch:322	batch id:50	 lr:0.002346 loss:1.317644
[Train] epoch:322	batch id:60	 lr:0.002346 loss:1.311102
[Train] epoch:322	batch id:70	 lr:0.002346 loss:1.318731
[Train] epoch:322	batch id:80	 lr:0.002346 loss:1.383119
[Train] epoch:322	batch id:90	 lr:0.002346 loss:1.346931
[Train] epoch:322	batch id:100	 lr:0.002346 loss:1.349788
[Train] epoch:322	batch id:110	 lr:0.002346 loss:1.318209
[Train] epoch:322	batch id:120	 lr:0.002346 loss:1.348926
[Train] epoch:322	batch id:130	 lr:0.002346 loss:1.344334
[Train] epoch:322	batch id:140	 lr:0.002346 loss:1.306723
[Train] epoch:322	batch id:150	 lr:0.002346 loss:1.382814
[Train] epoch:322	batch id:160	 lr:0.002346 loss:1.377210
[Train] epoch:322	batch id:170	 lr:0.002346 loss:1.360125
[Train] epoch:322	batch id:180	 lr:0.002346 loss:1.373742
[Train] epoch:322	batch id:190	 lr:0.002346 loss:1.340737
[Train] epoch:322	batch id:200	 lr:0.002346 loss:1.317660
[Train] epoch:322	batch id:210	 lr:0.002346 loss:1.317217
[Train] epoch:322	batch id:220	 lr:0.002346 loss:1.314466
[Train] epoch:322	batch id:230	 lr:0.002346 loss:1.323526
[Train] epoch:322	batch id:240	 lr:0.002346 loss:1.339726
[Train] epoch:322	batch id:250	 lr:0.002346 loss:1.337837
[Train] epoch:322	batch id:260	 lr:0.002346 loss:1.300238
[Train] epoch:322	batch id:270	 lr:0.002346 loss:1.319701
[Train] epoch:322	batch id:280	 lr:0.002346 loss:1.317939
[Train] epoch:322	batch id:290	 lr:0.002346 loss:1.332881
[Train] epoch:322	batch id:300	 lr:0.002346 loss:1.317242
[Train] 322, loss: 1.336189, train acc: 0.994707, 
[Test] epoch:322	batch id:0	 loss:1.253776
[Test] epoch:322	batch id:10	 loss:1.430237
[Test] epoch:322	batch id:20	 loss:1.408824
[Test] epoch:322	batch id:30	 loss:1.337503
[Test] epoch:322	batch id:40	 loss:1.610635
[Test] epoch:322	batch id:50	 loss:1.529953
[Test] epoch:322	batch id:60	 loss:1.246023
[Test] epoch:322	batch id:70	 loss:1.451159
[Test] epoch:322	batch id:80	 loss:1.577547
[Test] epoch:322	batch id:90	 loss:1.605674
[Test] epoch:322	batch id:100	 loss:1.855473
[Test] epoch:322	batch id:110	 loss:1.346522
[Test] epoch:322	batch id:120	 loss:1.296973
[Test] epoch:322	batch id:130	 loss:1.323021
[Test] epoch:322	batch id:140	 loss:1.379830
[Test] epoch:322	batch id:150	 loss:1.665372
[Test] 322, loss: 1.470747, test acc: 0.912480,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:323	batch id:0	 lr:0.002245 loss:1.345670
[Train] epoch:323	batch id:10	 lr:0.002245 loss:1.340980
[Train] epoch:323	batch id:20	 lr:0.002245 loss:1.319168
[Train] epoch:323	batch id:30	 lr:0.002245 loss:1.340042
[Train] epoch:323	batch id:40	 lr:0.002245 loss:1.329165
[Train] epoch:323	batch id:50	 lr:0.002245 loss:1.312939
[Train] epoch:323	batch id:60	 lr:0.002245 loss:1.337341
[Train] epoch:323	batch id:70	 lr:0.002245 loss:1.323944
[Train] epoch:323	batch id:80	 lr:0.002245 loss:1.334915
[Train] epoch:323	batch id:90	 lr:0.002245 loss:1.344262
[Train] epoch:323	batch id:100	 lr:0.002245 loss:1.359962
[Train] epoch:323	batch id:110	 lr:0.002245 loss:1.372123
[Train] epoch:323	batch id:120	 lr:0.002245 loss:1.374011
[Train] epoch:323	batch id:130	 lr:0.002245 loss:1.340267
[Train] epoch:323	batch id:140	 lr:0.002245 loss:1.339761
[Train] epoch:323	batch id:150	 lr:0.002245 loss:1.372973
[Train] epoch:323	batch id:160	 lr:0.002245 loss:1.335524
[Train] epoch:323	batch id:170	 lr:0.002245 loss:1.313924
[Train] epoch:323	batch id:180	 lr:0.002245 loss:1.320304
[Train] epoch:323	batch id:190	 lr:0.002245 loss:1.378819
[Train] epoch:323	batch id:200	 lr:0.002245 loss:1.315351
[Train] epoch:323	batch id:210	 lr:0.002245 loss:1.343141
[Train] epoch:323	batch id:220	 lr:0.002245 loss:1.323783
[Train] epoch:323	batch id:230	 lr:0.002245 loss:1.343167
[Train] epoch:323	batch id:240	 lr:0.002245 loss:1.324694
[Train] epoch:323	batch id:250	 lr:0.002245 loss:1.316569
[Train] epoch:323	batch id:260	 lr:0.002245 loss:1.319279
[Train] epoch:323	batch id:270	 lr:0.002245 loss:1.336868
[Train] epoch:323	batch id:280	 lr:0.002245 loss:1.319819
[Train] epoch:323	batch id:290	 lr:0.002245 loss:1.334490
[Train] epoch:323	batch id:300	 lr:0.002245 loss:1.356486
[Train] 323, loss: 1.333148, train acc: 0.996030, 
[Test] epoch:323	batch id:0	 loss:1.256033
[Test] epoch:323	batch id:10	 loss:1.348790
[Test] epoch:323	batch id:20	 loss:1.405361
[Test] epoch:323	batch id:30	 loss:1.312935
[Test] epoch:323	batch id:40	 loss:1.664472
[Test] epoch:323	batch id:50	 loss:1.574832
[Test] epoch:323	batch id:60	 loss:1.256958
[Test] epoch:323	batch id:70	 loss:1.448938
[Test] epoch:323	batch id:80	 loss:1.591000
[Test] epoch:323	batch id:90	 loss:1.656517
[Test] epoch:323	batch id:100	 loss:1.960944
[Test] epoch:323	batch id:110	 loss:1.361849
[Test] epoch:323	batch id:120	 loss:1.308290
[Test] epoch:323	batch id:130	 loss:1.304844
[Test] epoch:323	batch id:140	 loss:1.331341
[Test] epoch:323	batch id:150	 loss:1.730955
[Test] 323, loss: 1.475587, test acc: 0.918963,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:324	batch id:0	 lr:0.002148 loss:1.305896
[Train] epoch:324	batch id:10	 lr:0.002148 loss:1.333283
[Train] epoch:324	batch id:20	 lr:0.002148 loss:1.334887
[Train] epoch:324	batch id:30	 lr:0.002148 loss:1.342347
[Train] epoch:324	batch id:40	 lr:0.002148 loss:1.348727
[Train] epoch:324	batch id:50	 lr:0.002148 loss:1.328280
[Train] epoch:324	batch id:60	 lr:0.002148 loss:1.334501
[Train] epoch:324	batch id:70	 lr:0.002148 loss:1.324579
[Train] epoch:324	batch id:80	 lr:0.002148 loss:1.319232
[Train] epoch:324	batch id:90	 lr:0.002148 loss:1.302802
[Train] epoch:324	batch id:100	 lr:0.002148 loss:1.316485
[Train] epoch:324	batch id:110	 lr:0.002148 loss:1.358831
[Train] epoch:324	batch id:120	 lr:0.002148 loss:1.312748
[Train] epoch:324	batch id:130	 lr:0.002148 loss:1.348756
[Train] epoch:324	batch id:140	 lr:0.002148 loss:1.313894
[Train] epoch:324	batch id:150	 lr:0.002148 loss:1.356376
[Train] epoch:324	batch id:160	 lr:0.002148 loss:1.309870
[Train] epoch:324	batch id:170	 lr:0.002148 loss:1.349886
[Train] epoch:324	batch id:180	 lr:0.002148 loss:1.316051
[Train] epoch:324	batch id:190	 lr:0.002148 loss:1.367906
[Train] epoch:324	batch id:200	 lr:0.002148 loss:1.340872
[Train] epoch:324	batch id:210	 lr:0.002148 loss:1.299850
[Train] epoch:324	batch id:220	 lr:0.002148 loss:1.340174
[Train] epoch:324	batch id:230	 lr:0.002148 loss:1.403876
[Train] epoch:324	batch id:240	 lr:0.002148 loss:1.361859
[Train] epoch:324	batch id:250	 lr:0.002148 loss:1.314220
[Train] epoch:324	batch id:260	 lr:0.002148 loss:1.372323
[Train] epoch:324	batch id:270	 lr:0.002148 loss:1.359499
[Train] epoch:324	batch id:280	 lr:0.002148 loss:1.315352
[Train] epoch:324	batch id:290	 lr:0.002148 loss:1.352263
[Train] epoch:324	batch id:300	 lr:0.002148 loss:1.310802
[Train] 324, loss: 1.335298, train acc: 0.994707, 
[Test] epoch:324	batch id:0	 loss:1.260828
[Test] epoch:324	batch id:10	 loss:1.424297
[Test] epoch:324	batch id:20	 loss:1.395559
[Test] epoch:324	batch id:30	 loss:1.347486
[Test] epoch:324	batch id:40	 loss:1.565375
[Test] epoch:324	batch id:50	 loss:1.462751
[Test] epoch:324	batch id:60	 loss:1.249138
[Test] epoch:324	batch id:70	 loss:1.464962
[Test] epoch:324	batch id:80	 loss:1.544622
[Test] epoch:324	batch id:90	 loss:1.647401
[Test] epoch:324	batch id:100	 loss:1.829490
[Test] epoch:324	batch id:110	 loss:1.341504
[Test] epoch:324	batch id:120	 loss:1.336651
[Test] epoch:324	batch id:130	 loss:1.309288
[Test] epoch:324	batch id:140	 loss:1.335421
[Test] epoch:324	batch id:150	 loss:1.739475
[Test] 324, loss: 1.466160, test acc: 0.917342,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:325	batch id:0	 lr:0.002055 loss:1.335721
[Train] epoch:325	batch id:10	 lr:0.002055 loss:1.356098
[Train] epoch:325	batch id:20	 lr:0.002055 loss:1.324154
[Train] epoch:325	batch id:30	 lr:0.002055 loss:1.326897
[Train] epoch:325	batch id:40	 lr:0.002055 loss:1.313263
[Train] epoch:325	batch id:50	 lr:0.002055 loss:1.334397
[Train] epoch:325	batch id:60	 lr:0.002055 loss:1.299055
[Train] epoch:325	batch id:70	 lr:0.002055 loss:1.304714
[Train] epoch:325	batch id:80	 lr:0.002055 loss:1.324851
[Train] epoch:325	batch id:90	 lr:0.002055 loss:1.327064
[Train] epoch:325	batch id:100	 lr:0.002055 loss:1.325320
[Train] epoch:325	batch id:110	 lr:0.002055 loss:1.298279
[Train] epoch:325	batch id:120	 lr:0.002055 loss:1.308704
[Train] epoch:325	batch id:130	 lr:0.002055 loss:1.311307
[Train] epoch:325	batch id:140	 lr:0.002055 loss:1.350837
[Train] epoch:325	batch id:150	 lr:0.002055 loss:1.341888
[Train] epoch:325	batch id:160	 lr:0.002055 loss:1.318096
[Train] epoch:325	batch id:170	 lr:0.002055 loss:1.316082
[Train] epoch:325	batch id:180	 lr:0.002055 loss:1.309232
[Train] epoch:325	batch id:190	 lr:0.002055 loss:1.360213
[Train] epoch:325	batch id:200	 lr:0.002055 loss:1.304658
[Train] epoch:325	batch id:210	 lr:0.002055 loss:1.350244
[Train] epoch:325	batch id:220	 lr:0.002055 loss:1.360671
[Train] epoch:325	batch id:230	 lr:0.002055 loss:1.312641
[Train] epoch:325	batch id:240	 lr:0.002055 loss:1.326566
[Train] epoch:325	batch id:250	 lr:0.002055 loss:1.318033
[Train] epoch:325	batch id:260	 lr:0.002055 loss:1.324697
[Train] epoch:325	batch id:270	 lr:0.002055 loss:1.372952
[Train] epoch:325	batch id:280	 lr:0.002055 loss:1.333659
[Train] epoch:325	batch id:290	 lr:0.002055 loss:1.298748
[Train] epoch:325	batch id:300	 lr:0.002055 loss:1.308101
[Train] 325, loss: 1.330765, train acc: 0.996539, 
[Test] epoch:325	batch id:0	 loss:1.259281
[Test] epoch:325	batch id:10	 loss:1.485549
[Test] epoch:325	batch id:20	 loss:1.383362
[Test] epoch:325	batch id:30	 loss:1.325970
[Test] epoch:325	batch id:40	 loss:1.617541
[Test] epoch:325	batch id:50	 loss:1.559195
[Test] epoch:325	batch id:60	 loss:1.250239
[Test] epoch:325	batch id:70	 loss:1.410536
[Test] epoch:325	batch id:80	 loss:1.642355
[Test] epoch:325	batch id:90	 loss:1.650937
[Test] epoch:325	batch id:100	 loss:1.844167
[Test] epoch:325	batch id:110	 loss:1.346438
[Test] epoch:325	batch id:120	 loss:1.372276
[Test] epoch:325	batch id:130	 loss:1.335458
[Test] epoch:325	batch id:140	 loss:1.421217
[Test] epoch:325	batch id:150	 loss:1.773386
[Test] 325, loss: 1.487571, test acc: 0.911669,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:326	batch id:0	 lr:0.001966 loss:1.307323
[Train] epoch:326	batch id:10	 lr:0.001966 loss:1.329099
[Train] epoch:326	batch id:20	 lr:0.001966 loss:1.334959
[Train] epoch:326	batch id:30	 lr:0.001966 loss:1.314683
[Train] epoch:326	batch id:40	 lr:0.001966 loss:1.303652
[Train] epoch:326	batch id:50	 lr:0.001966 loss:1.319677
[Train] epoch:326	batch id:60	 lr:0.001966 loss:1.362869
[Train] epoch:326	batch id:70	 lr:0.001966 loss:1.299349
[Train] epoch:326	batch id:80	 lr:0.001966 loss:1.337357
[Train] epoch:326	batch id:90	 lr:0.001966 loss:1.321987
[Train] epoch:326	batch id:100	 lr:0.001966 loss:1.349337
[Train] epoch:326	batch id:110	 lr:0.001966 loss:1.304031
[Train] epoch:326	batch id:120	 lr:0.001966 loss:1.306809
[Train] epoch:326	batch id:130	 lr:0.001966 loss:1.334002
[Train] epoch:326	batch id:140	 lr:0.001966 loss:1.357461
[Train] epoch:326	batch id:150	 lr:0.001966 loss:1.332952
[Train] epoch:326	batch id:160	 lr:0.001966 loss:1.308289
[Train] epoch:326	batch id:170	 lr:0.001966 loss:1.338529
[Train] epoch:326	batch id:180	 lr:0.001966 loss:1.367180
[Train] epoch:326	batch id:190	 lr:0.001966 loss:1.312784
[Train] epoch:326	batch id:200	 lr:0.001966 loss:1.309614
[Train] epoch:326	batch id:210	 lr:0.001966 loss:1.307733
[Train] epoch:326	batch id:220	 lr:0.001966 loss:1.302463
[Train] epoch:326	batch id:230	 lr:0.001966 loss:1.306863
[Train] epoch:326	batch id:240	 lr:0.001966 loss:1.306858
[Train] epoch:326	batch id:250	 lr:0.001966 loss:1.325800
[Train] epoch:326	batch id:260	 lr:0.001966 loss:1.345101
[Train] epoch:326	batch id:270	 lr:0.001966 loss:1.337346
[Train] epoch:326	batch id:280	 lr:0.001966 loss:1.306870
[Train] epoch:326	batch id:290	 lr:0.001966 loss:1.320756
[Train] epoch:326	batch id:300	 lr:0.001966 loss:1.323086
[Train] 326, loss: 1.330370, train acc: 0.997150, 
[Test] epoch:326	batch id:0	 loss:1.332551
[Test] epoch:326	batch id:10	 loss:1.475999
[Test] epoch:326	batch id:20	 loss:1.436303
[Test] epoch:326	batch id:30	 loss:1.339403
[Test] epoch:326	batch id:40	 loss:1.609393
[Test] epoch:326	batch id:50	 loss:1.475516
[Test] epoch:326	batch id:60	 loss:1.252559
[Test] epoch:326	batch id:70	 loss:1.351845
[Test] epoch:326	batch id:80	 loss:1.566490
[Test] epoch:326	batch id:90	 loss:1.550560
[Test] epoch:326	batch id:100	 loss:1.991928
[Test] epoch:326	batch id:110	 loss:1.304880
[Test] epoch:326	batch id:120	 loss:1.290691
[Test] epoch:326	batch id:130	 loss:1.304329
[Test] epoch:326	batch id:140	 loss:1.331690
[Test] epoch:326	batch id:150	 loss:1.765226
[Test] 326, loss: 1.467656, test acc: 0.916937,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:327	batch id:0	 lr:0.001880 loss:1.318115
[Train] epoch:327	batch id:10	 lr:0.001880 loss:1.337665
[Train] epoch:327	batch id:20	 lr:0.001880 loss:1.306503
[Train] epoch:327	batch id:30	 lr:0.001880 loss:1.324983
[Train] epoch:327	batch id:40	 lr:0.001880 loss:1.357418
[Train] epoch:327	batch id:50	 lr:0.001880 loss:1.356852
[Train] epoch:327	batch id:60	 lr:0.001880 loss:1.309231
[Train] epoch:327	batch id:70	 lr:0.001880 loss:1.305062
[Train] epoch:327	batch id:80	 lr:0.001880 loss:1.310883
[Train] epoch:327	batch id:90	 lr:0.001880 loss:1.341372
[Train] epoch:327	batch id:100	 lr:0.001880 loss:1.338301
[Train] epoch:327	batch id:110	 lr:0.001880 loss:1.328974
[Train] epoch:327	batch id:120	 lr:0.001880 loss:1.415637
[Train] epoch:327	batch id:130	 lr:0.001880 loss:1.311786
[Train] epoch:327	batch id:140	 lr:0.001880 loss:1.305213
[Train] epoch:327	batch id:150	 lr:0.001880 loss:1.299930
[Train] epoch:327	batch id:160	 lr:0.001880 loss:1.342963
[Train] epoch:327	batch id:170	 lr:0.001880 loss:1.306144
[Train] epoch:327	batch id:180	 lr:0.001880 loss:1.318782
[Train] epoch:327	batch id:190	 lr:0.001880 loss:1.320656
[Train] epoch:327	batch id:200	 lr:0.001880 loss:1.292984
[Train] epoch:327	batch id:210	 lr:0.001880 loss:1.311032
[Train] epoch:327	batch id:220	 lr:0.001880 loss:1.333124
[Train] epoch:327	batch id:230	 lr:0.001880 loss:1.333705
[Train] epoch:327	batch id:240	 lr:0.001880 loss:1.317255
[Train] epoch:327	batch id:250	 lr:0.001880 loss:1.295494
[Train] epoch:327	batch id:260	 lr:0.001880 loss:1.351362
[Train] epoch:327	batch id:270	 lr:0.001880 loss:1.319549
[Train] epoch:327	batch id:280	 lr:0.001880 loss:1.316636
[Train] epoch:327	batch id:290	 lr:0.001880 loss:1.309176
[Train] epoch:327	batch id:300	 lr:0.001880 loss:1.360639
[Train] 327, loss: 1.330873, train acc: 0.995725, 
[Test] epoch:327	batch id:0	 loss:1.267524
[Test] epoch:327	batch id:10	 loss:1.408450
[Test] epoch:327	batch id:20	 loss:1.389065
[Test] epoch:327	batch id:30	 loss:1.375230
[Test] epoch:327	batch id:40	 loss:1.551441
[Test] epoch:327	batch id:50	 loss:1.514610
[Test] epoch:327	batch id:60	 loss:1.253068
[Test] epoch:327	batch id:70	 loss:1.428976
[Test] epoch:327	batch id:80	 loss:1.560951
[Test] epoch:327	batch id:90	 loss:1.570963
[Test] epoch:327	batch id:100	 loss:1.978003
[Test] epoch:327	batch id:110	 loss:1.331346
[Test] epoch:327	batch id:120	 loss:1.340466
[Test] epoch:327	batch id:130	 loss:1.281869
[Test] epoch:327	batch id:140	 loss:1.313130
[Test] epoch:327	batch id:150	 loss:1.797413
[Test] 327, loss: 1.461193, test acc: 0.919368,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:328	batch id:0	 lr:0.001799 loss:1.329976
[Train] epoch:328	batch id:10	 lr:0.001799 loss:1.326802
[Train] epoch:328	batch id:20	 lr:0.001799 loss:1.343875
[Train] epoch:328	batch id:30	 lr:0.001799 loss:1.369897
[Train] epoch:328	batch id:40	 lr:0.001799 loss:1.374626
[Train] epoch:328	batch id:50	 lr:0.001799 loss:1.361027
[Train] epoch:328	batch id:60	 lr:0.001799 loss:1.351316
[Train] epoch:328	batch id:70	 lr:0.001799 loss:1.308283
[Train] epoch:328	batch id:80	 lr:0.001799 loss:1.340681
[Train] epoch:328	batch id:90	 lr:0.001799 loss:1.305472
[Train] epoch:328	batch id:100	 lr:0.001799 loss:1.312694
[Train] epoch:328	batch id:110	 lr:0.001799 loss:1.316387
[Train] epoch:328	batch id:120	 lr:0.001799 loss:1.333629
[Train] epoch:328	batch id:130	 lr:0.001799 loss:1.346437
[Train] epoch:328	batch id:140	 lr:0.001799 loss:1.306326
[Train] epoch:328	batch id:150	 lr:0.001799 loss:1.307911
[Train] epoch:328	batch id:160	 lr:0.001799 loss:1.365810
[Train] epoch:328	batch id:170	 lr:0.001799 loss:1.381724
[Train] epoch:328	batch id:180	 lr:0.001799 loss:1.355277
[Train] epoch:328	batch id:190	 lr:0.001799 loss:1.353291
[Train] epoch:328	batch id:200	 lr:0.001799 loss:1.305508
[Train] epoch:328	batch id:210	 lr:0.001799 loss:1.330039
[Train] epoch:328	batch id:220	 lr:0.001799 loss:1.312856
[Train] epoch:328	batch id:230	 lr:0.001799 loss:1.350879
[Train] epoch:328	batch id:240	 lr:0.001799 loss:1.333565
[Train] epoch:328	batch id:250	 lr:0.001799 loss:1.390616
[Train] epoch:328	batch id:260	 lr:0.001799 loss:1.317074
[Train] epoch:328	batch id:270	 lr:0.001799 loss:1.294997
[Train] epoch:328	batch id:280	 lr:0.001799 loss:1.349334
[Train] epoch:328	batch id:290	 lr:0.001799 loss:1.321316
[Train] epoch:328	batch id:300	 lr:0.001799 loss:1.307180
[Train] 328, loss: 1.331159, train acc: 0.996234, 
[Test] epoch:328	batch id:0	 loss:1.255514
[Test] epoch:328	batch id:10	 loss:1.460346
[Test] epoch:328	batch id:20	 loss:1.412407
[Test] epoch:328	batch id:30	 loss:1.309943
[Test] epoch:328	batch id:40	 loss:1.601823
[Test] epoch:328	batch id:50	 loss:1.594258
[Test] epoch:328	batch id:60	 loss:1.248548
[Test] epoch:328	batch id:70	 loss:1.379365
[Test] epoch:328	batch id:80	 loss:1.578733
[Test] epoch:328	batch id:90	 loss:1.602180
[Test] epoch:328	batch id:100	 loss:1.890897
[Test] epoch:328	batch id:110	 loss:1.377664
[Test] epoch:328	batch id:120	 loss:1.285002
[Test] epoch:328	batch id:130	 loss:1.381838
[Test] epoch:328	batch id:140	 loss:1.374349
[Test] epoch:328	batch id:150	 loss:1.636409
[Test] 328, loss: 1.480636, test acc: 0.912885,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:329	batch id:0	 lr:0.001722 loss:1.323554
[Train] epoch:329	batch id:10	 lr:0.001722 loss:1.321200
[Train] epoch:329	batch id:20	 lr:0.001722 loss:1.310010
[Train] epoch:329	batch id:30	 lr:0.001722 loss:1.318164
[Train] epoch:329	batch id:40	 lr:0.001722 loss:1.322489
[Train] epoch:329	batch id:50	 lr:0.001722 loss:1.361727
[Train] epoch:329	batch id:60	 lr:0.001722 loss:1.352884
[Train] epoch:329	batch id:70	 lr:0.001722 loss:1.338069
[Train] epoch:329	batch id:80	 lr:0.001722 loss:1.386496
[Train] epoch:329	batch id:90	 lr:0.001722 loss:1.326569
[Train] epoch:329	batch id:100	 lr:0.001722 loss:1.317826
[Train] epoch:329	batch id:110	 lr:0.001722 loss:1.313418
[Train] epoch:329	batch id:120	 lr:0.001722 loss:1.384034
[Train] epoch:329	batch id:130	 lr:0.001722 loss:1.352049
[Train] epoch:329	batch id:140	 lr:0.001722 loss:1.331878
[Train] epoch:329	batch id:150	 lr:0.001722 loss:1.316938
[Train] epoch:329	batch id:160	 lr:0.001722 loss:1.310751
[Train] epoch:329	batch id:170	 lr:0.001722 loss:1.303494
[Train] epoch:329	batch id:180	 lr:0.001722 loss:1.348816
[Train] epoch:329	batch id:190	 lr:0.001722 loss:1.386275
[Train] epoch:329	batch id:200	 lr:0.001722 loss:1.343763
[Train] epoch:329	batch id:210	 lr:0.001722 loss:1.330935
[Train] epoch:329	batch id:220	 lr:0.001722 loss:1.324363
[Train] epoch:329	batch id:230	 lr:0.001722 loss:1.314522
[Train] epoch:329	batch id:240	 lr:0.001722 loss:1.336619
[Train] epoch:329	batch id:250	 lr:0.001722 loss:1.318487
[Train] epoch:329	batch id:260	 lr:0.001722 loss:1.324530
[Train] epoch:329	batch id:270	 lr:0.001722 loss:1.331764
[Train] epoch:329	batch id:280	 lr:0.001722 loss:1.336570
[Train] epoch:329	batch id:290	 lr:0.001722 loss:1.324327
[Train] epoch:329	batch id:300	 lr:0.001722 loss:1.308880
[Train] 329, loss: 1.332212, train acc: 0.996743, 
[Test] epoch:329	batch id:0	 loss:1.265647
[Test] epoch:329	batch id:10	 loss:1.420049
[Test] epoch:329	batch id:20	 loss:1.398383
[Test] epoch:329	batch id:30	 loss:1.344434
[Test] epoch:329	batch id:40	 loss:1.637260
[Test] epoch:329	batch id:50	 loss:1.530578
[Test] epoch:329	batch id:60	 loss:1.242864
[Test] epoch:329	batch id:70	 loss:1.404464
[Test] epoch:329	batch id:80	 loss:1.610082
[Test] epoch:329	batch id:90	 loss:1.481838
[Test] epoch:329	batch id:100	 loss:1.952996
[Test] epoch:329	batch id:110	 loss:1.286337
[Test] epoch:329	batch id:120	 loss:1.292526
[Test] epoch:329	batch id:130	 loss:1.300133
[Test] epoch:329	batch id:140	 loss:1.325743
[Test] epoch:329	batch id:150	 loss:1.846898
[Test] 329, loss: 1.464593, test acc: 0.917747,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:330	batch id:0	 lr:0.001648 loss:1.335104
[Train] epoch:330	batch id:10	 lr:0.001648 loss:1.332837
[Train] epoch:330	batch id:20	 lr:0.001648 loss:1.301759
[Train] epoch:330	batch id:30	 lr:0.001648 loss:1.294251
[Train] epoch:330	batch id:40	 lr:0.001648 loss:1.339064
[Train] epoch:330	batch id:50	 lr:0.001648 loss:1.386378
[Train] epoch:330	batch id:60	 lr:0.001648 loss:1.327679
[Train] epoch:330	batch id:70	 lr:0.001648 loss:1.317068
[Train] epoch:330	batch id:80	 lr:0.001648 loss:1.331573
[Train] epoch:330	batch id:90	 lr:0.001648 loss:1.387166
[Train] epoch:330	batch id:100	 lr:0.001648 loss:1.312737
[Train] epoch:330	batch id:110	 lr:0.001648 loss:1.309641
[Train] epoch:330	batch id:120	 lr:0.001648 loss:1.325364
[Train] epoch:330	batch id:130	 lr:0.001648 loss:1.317968
[Train] epoch:330	batch id:140	 lr:0.001648 loss:1.366992
[Train] epoch:330	batch id:150	 lr:0.001648 loss:1.310350
[Train] epoch:330	batch id:160	 lr:0.001648 loss:1.384722
[Train] epoch:330	batch id:170	 lr:0.001648 loss:1.315629
[Train] epoch:330	batch id:180	 lr:0.001648 loss:1.311077
[Train] epoch:330	batch id:190	 lr:0.001648 loss:1.313203
[Train] epoch:330	batch id:200	 lr:0.001648 loss:1.294131
[Train] epoch:330	batch id:210	 lr:0.001648 loss:1.315247
[Train] epoch:330	batch id:220	 lr:0.001648 loss:1.310116
[Train] epoch:330	batch id:230	 lr:0.001648 loss:1.296231
[Train] epoch:330	batch id:240	 lr:0.001648 loss:1.334309
[Train] epoch:330	batch id:250	 lr:0.001648 loss:1.366530
[Train] epoch:330	batch id:260	 lr:0.001648 loss:1.333203
[Train] epoch:330	batch id:270	 lr:0.001648 loss:1.361290
[Train] epoch:330	batch id:280	 lr:0.001648 loss:1.346505
[Train] epoch:330	batch id:290	 lr:0.001648 loss:1.342164
[Train] epoch:330	batch id:300	 lr:0.001648 loss:1.327147
[Train] 330, loss: 1.329571, train acc: 0.996743, 
[Test] epoch:330	batch id:0	 loss:1.280617
[Test] epoch:330	batch id:10	 loss:1.449244
[Test] epoch:330	batch id:20	 loss:1.443892
[Test] epoch:330	batch id:30	 loss:1.338396
[Test] epoch:330	batch id:40	 loss:1.556713
[Test] epoch:330	batch id:50	 loss:1.475487
[Test] epoch:330	batch id:60	 loss:1.249943
[Test] epoch:330	batch id:70	 loss:1.447237
[Test] epoch:330	batch id:80	 loss:1.578267
[Test] epoch:330	batch id:90	 loss:1.510736
[Test] epoch:330	batch id:100	 loss:1.842417
[Test] epoch:330	batch id:110	 loss:1.323706
[Test] epoch:330	batch id:120	 loss:1.315007
[Test] epoch:330	batch id:130	 loss:1.351741
[Test] epoch:330	batch id:140	 loss:1.365836
[Test] epoch:330	batch id:150	 loss:1.830663
[Test] 330, loss: 1.484931, test acc: 0.910454,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:331	batch id:0	 lr:0.001579 loss:1.311230
[Train] epoch:331	batch id:10	 lr:0.001579 loss:1.333626
[Train] epoch:331	batch id:20	 lr:0.001579 loss:1.376512
[Train] epoch:331	batch id:30	 lr:0.001579 loss:1.301891
[Train] epoch:331	batch id:40	 lr:0.001579 loss:1.298518
[Train] epoch:331	batch id:50	 lr:0.001579 loss:1.321267
[Train] epoch:331	batch id:60	 lr:0.001579 loss:1.303667
[Train] epoch:331	batch id:70	 lr:0.001579 loss:1.309335
[Train] epoch:331	batch id:80	 lr:0.001579 loss:1.356198
[Train] epoch:331	batch id:90	 lr:0.001579 loss:1.314708
[Train] epoch:331	batch id:100	 lr:0.001579 loss:1.410738
[Train] epoch:331	batch id:110	 lr:0.001579 loss:1.357164
[Train] epoch:331	batch id:120	 lr:0.001579 loss:1.325320
[Train] epoch:331	batch id:130	 lr:0.001579 loss:1.301019
[Train] epoch:331	batch id:140	 lr:0.001579 loss:1.345073
[Train] epoch:331	batch id:150	 lr:0.001579 loss:1.314453
[Train] epoch:331	batch id:160	 lr:0.001579 loss:1.334675
[Train] epoch:331	batch id:170	 lr:0.001579 loss:1.329201
[Train] epoch:331	batch id:180	 lr:0.001579 loss:1.324225
[Train] epoch:331	batch id:190	 lr:0.001579 loss:1.299641
[Train] epoch:331	batch id:200	 lr:0.001579 loss:1.371055
[Train] epoch:331	batch id:210	 lr:0.001579 loss:1.311787
[Train] epoch:331	batch id:220	 lr:0.001579 loss:1.394982
[Train] epoch:331	batch id:230	 lr:0.001579 loss:1.322073
[Train] epoch:331	batch id:240	 lr:0.001579 loss:1.345199
[Train] epoch:331	batch id:250	 lr:0.001579 loss:1.363692
[Train] epoch:331	batch id:260	 lr:0.001579 loss:1.304969
[Train] epoch:331	batch id:270	 lr:0.001579 loss:1.357218
[Train] epoch:331	batch id:280	 lr:0.001579 loss:1.300515
[Train] epoch:331	batch id:290	 lr:0.001579 loss:1.320138
[Train] epoch:331	batch id:300	 lr:0.001579 loss:1.314481
[Train] 331, loss: 1.330919, train acc: 0.996030, 
[Test] epoch:331	batch id:0	 loss:1.251662
[Test] epoch:331	batch id:10	 loss:1.401616
[Test] epoch:331	batch id:20	 loss:1.457426
[Test] epoch:331	batch id:30	 loss:1.336970
[Test] epoch:331	batch id:40	 loss:1.528352
[Test] epoch:331	batch id:50	 loss:1.536125
[Test] epoch:331	batch id:60	 loss:1.249185
[Test] epoch:331	batch id:70	 loss:1.506153
[Test] epoch:331	batch id:80	 loss:1.623330
[Test] epoch:331	batch id:90	 loss:1.480136
[Test] epoch:331	batch id:100	 loss:1.884711
[Test] epoch:331	batch id:110	 loss:1.366453
[Test] epoch:331	batch id:120	 loss:1.288114
[Test] epoch:331	batch id:130	 loss:1.325274
[Test] epoch:331	batch id:140	 loss:1.314033
[Test] epoch:331	batch id:150	 loss:1.755167
[Test] 331, loss: 1.455826, test acc: 0.917342,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:332	batch id:0	 lr:0.001513 loss:1.340501
[Train] epoch:332	batch id:10	 lr:0.001513 loss:1.338156
[Train] epoch:332	batch id:20	 lr:0.001513 loss:1.301527
[Train] epoch:332	batch id:30	 lr:0.001513 loss:1.326012
[Train] epoch:332	batch id:40	 lr:0.001513 loss:1.324853
[Train] epoch:332	batch id:50	 lr:0.001513 loss:1.339274
[Train] epoch:332	batch id:60	 lr:0.001513 loss:1.308513
[Train] epoch:332	batch id:70	 lr:0.001513 loss:1.315532
[Train] epoch:332	batch id:80	 lr:0.001513 loss:1.305750
[Train] epoch:332	batch id:90	 lr:0.001513 loss:1.322232
[Train] epoch:332	batch id:100	 lr:0.001513 loss:1.310612
[Train] epoch:332	batch id:110	 lr:0.001513 loss:1.319847
[Train] epoch:332	batch id:120	 lr:0.001513 loss:1.300035
[Train] epoch:332	batch id:130	 lr:0.001513 loss:1.303470
[Train] epoch:332	batch id:140	 lr:0.001513 loss:1.316482
[Train] epoch:332	batch id:150	 lr:0.001513 loss:1.316096
[Train] epoch:332	batch id:160	 lr:0.001513 loss:1.334728
[Train] epoch:332	batch id:170	 lr:0.001513 loss:1.309155
[Train] epoch:332	batch id:180	 lr:0.001513 loss:1.307269
[Train] epoch:332	batch id:190	 lr:0.001513 loss:1.307570
[Train] epoch:332	batch id:200	 lr:0.001513 loss:1.371656
[Train] epoch:332	batch id:210	 lr:0.001513 loss:1.327275
[Train] epoch:332	batch id:220	 lr:0.001513 loss:1.381999
[Train] epoch:332	batch id:230	 lr:0.001513 loss:1.334985
[Train] epoch:332	batch id:240	 lr:0.001513 loss:1.329480
[Train] epoch:332	batch id:250	 lr:0.001513 loss:1.293425
[Train] epoch:332	batch id:260	 lr:0.001513 loss:1.302358
[Train] epoch:332	batch id:270	 lr:0.001513 loss:1.323772
[Train] epoch:332	batch id:280	 lr:0.001513 loss:1.314704
[Train] epoch:332	batch id:290	 lr:0.001513 loss:1.346519
[Train] epoch:332	batch id:300	 lr:0.001513 loss:1.346321
[Train] 332, loss: 1.329283, train acc: 0.997557, 
[Test] epoch:332	batch id:0	 loss:1.264365
[Test] epoch:332	batch id:10	 loss:1.473079
[Test] epoch:332	batch id:20	 loss:1.404960
[Test] epoch:332	batch id:30	 loss:1.373345
[Test] epoch:332	batch id:40	 loss:1.622720
[Test] epoch:332	batch id:50	 loss:1.541501
[Test] epoch:332	batch id:60	 loss:1.245032
[Test] epoch:332	batch id:70	 loss:1.430793
[Test] epoch:332	batch id:80	 loss:1.576832
[Test] epoch:332	batch id:90	 loss:1.585070
[Test] epoch:332	batch id:100	 loss:1.847331
[Test] epoch:332	batch id:110	 loss:1.340796
[Test] epoch:332	batch id:120	 loss:1.314102
[Test] epoch:332	batch id:130	 loss:1.298676
[Test] epoch:332	batch id:140	 loss:1.341475
[Test] epoch:332	batch id:150	 loss:1.722196
[Test] 332, loss: 1.469096, test acc: 0.911264,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:333	batch id:0	 lr:0.001452 loss:1.317933
[Train] epoch:333	batch id:10	 lr:0.001452 loss:1.327444
[Train] epoch:333	batch id:20	 lr:0.001452 loss:1.373099
[Train] epoch:333	batch id:30	 lr:0.001452 loss:1.343708
[Train] epoch:333	batch id:40	 lr:0.001452 loss:1.311294
[Train] epoch:333	batch id:50	 lr:0.001452 loss:1.307586
[Train] epoch:333	batch id:60	 lr:0.001452 loss:1.328903
[Train] epoch:333	batch id:70	 lr:0.001452 loss:1.317846
[Train] epoch:333	batch id:80	 lr:0.001452 loss:1.311676
[Train] epoch:333	batch id:90	 lr:0.001452 loss:1.308562
[Train] epoch:333	batch id:100	 lr:0.001452 loss:1.335773
[Train] epoch:333	batch id:110	 lr:0.001452 loss:1.343045
[Train] epoch:333	batch id:120	 lr:0.001452 loss:1.331397
[Train] epoch:333	batch id:130	 lr:0.001452 loss:1.321516
[Train] epoch:333	batch id:140	 lr:0.001452 loss:1.306523
[Train] epoch:333	batch id:150	 lr:0.001452 loss:1.332162
[Train] epoch:333	batch id:160	 lr:0.001452 loss:1.363823
[Train] epoch:333	batch id:170	 lr:0.001452 loss:1.319334
[Train] epoch:333	batch id:180	 lr:0.001452 loss:1.318613
[Train] epoch:333	batch id:190	 lr:0.001452 loss:1.319523
[Train] epoch:333	batch id:200	 lr:0.001452 loss:1.328434
[Train] epoch:333	batch id:210	 lr:0.001452 loss:1.342809
[Train] epoch:333	batch id:220	 lr:0.001452 loss:1.307294
[Train] epoch:333	batch id:230	 lr:0.001452 loss:1.417928
[Train] epoch:333	batch id:240	 lr:0.001452 loss:1.322581
[Train] epoch:333	batch id:250	 lr:0.001452 loss:1.409135
[Train] epoch:333	batch id:260	 lr:0.001452 loss:1.327845
[Train] epoch:333	batch id:270	 lr:0.001452 loss:1.313378
[Train] epoch:333	batch id:280	 lr:0.001452 loss:1.365052
[Train] epoch:333	batch id:290	 lr:0.001452 loss:1.315283
[Train] epoch:333	batch id:300	 lr:0.001452 loss:1.325651
[Train] 333, loss: 1.329361, train acc: 0.996336, 
[Test] epoch:333	batch id:0	 loss:1.249316
[Test] epoch:333	batch id:10	 loss:1.370463
[Test] epoch:333	batch id:20	 loss:1.487069
[Test] epoch:333	batch id:30	 loss:1.343542
[Test] epoch:333	batch id:40	 loss:1.543121
[Test] epoch:333	batch id:50	 loss:1.546406
[Test] epoch:333	batch id:60	 loss:1.254809
[Test] epoch:333	batch id:70	 loss:1.516616
[Test] epoch:333	batch id:80	 loss:1.581390
[Test] epoch:333	batch id:90	 loss:1.504297
[Test] epoch:333	batch id:100	 loss:1.895323
[Test] epoch:333	batch id:110	 loss:1.360275
[Test] epoch:333	batch id:120	 loss:1.308187
[Test] epoch:333	batch id:130	 loss:1.322291
[Test] epoch:333	batch id:140	 loss:1.353575
[Test] epoch:333	batch id:150	 loss:1.727623
[Test] 333, loss: 1.463228, test acc: 0.918558,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:334	batch id:0	 lr:0.001394 loss:1.315262
[Train] epoch:334	batch id:10	 lr:0.001394 loss:1.325534
[Train] epoch:334	batch id:20	 lr:0.001394 loss:1.396167
[Train] epoch:334	batch id:30	 lr:0.001394 loss:1.315350
[Train] epoch:334	batch id:40	 lr:0.001394 loss:1.332817
[Train] epoch:334	batch id:50	 lr:0.001394 loss:1.323335
[Train] epoch:334	batch id:60	 lr:0.001394 loss:1.317451
[Train] epoch:334	batch id:70	 lr:0.001394 loss:1.294703
[Train] epoch:334	batch id:80	 lr:0.001394 loss:1.314891
[Train] epoch:334	batch id:90	 lr:0.001394 loss:1.305466
[Train] epoch:334	batch id:100	 lr:0.001394 loss:1.379983
[Train] epoch:334	batch id:110	 lr:0.001394 loss:1.312892
[Train] epoch:334	batch id:120	 lr:0.001394 loss:1.333310
[Train] epoch:334	batch id:130	 lr:0.001394 loss:1.328572
[Train] epoch:334	batch id:140	 lr:0.001394 loss:1.327500
[Train] epoch:334	batch id:150	 lr:0.001394 loss:1.327344
[Train] epoch:334	batch id:160	 lr:0.001394 loss:1.351690
[Train] epoch:334	batch id:170	 lr:0.001394 loss:1.393446
[Train] epoch:334	batch id:180	 lr:0.001394 loss:1.344519
[Train] epoch:334	batch id:190	 lr:0.001394 loss:1.305780
[Train] epoch:334	batch id:200	 lr:0.001394 loss:1.329892
[Train] epoch:334	batch id:210	 lr:0.001394 loss:1.317102
[Train] epoch:334	batch id:220	 lr:0.001394 loss:1.360672
[Train] epoch:334	batch id:230	 lr:0.001394 loss:1.331387
[Train] epoch:334	batch id:240	 lr:0.001394 loss:1.431954
[Train] epoch:334	batch id:250	 lr:0.001394 loss:1.328868
[Train] epoch:334	batch id:260	 lr:0.001394 loss:1.318683
[Train] epoch:334	batch id:270	 lr:0.001394 loss:1.317609
[Train] epoch:334	batch id:280	 lr:0.001394 loss:1.310751
[Train] epoch:334	batch id:290	 lr:0.001394 loss:1.306510
[Train] epoch:334	batch id:300	 lr:0.001394 loss:1.335626
[Train] 334, loss: 1.331356, train acc: 0.995827, 
[Test] epoch:334	batch id:0	 loss:1.269929
[Test] epoch:334	batch id:10	 loss:1.492941
[Test] epoch:334	batch id:20	 loss:1.430847
[Test] epoch:334	batch id:30	 loss:1.332181
[Test] epoch:334	batch id:40	 loss:1.551310
[Test] epoch:334	batch id:50	 loss:1.529197
[Test] epoch:334	batch id:60	 loss:1.242258
[Test] epoch:334	batch id:70	 loss:1.412096
[Test] epoch:334	batch id:80	 loss:1.613813
[Test] epoch:334	batch id:90	 loss:1.525606
[Test] epoch:334	batch id:100	 loss:2.047898
[Test] epoch:334	batch id:110	 loss:1.307252
[Test] epoch:334	batch id:120	 loss:1.312046
[Test] epoch:334	batch id:130	 loss:1.273529
[Test] epoch:334	batch id:140	 loss:1.415148
[Test] epoch:334	batch id:150	 loss:1.828425
[Test] 334, loss: 1.474792, test acc: 0.916937,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:335	batch id:0	 lr:0.001340 loss:1.319559
[Train] epoch:335	batch id:10	 lr:0.001340 loss:1.370379
[Train] epoch:335	batch id:20	 lr:0.001340 loss:1.334487
[Train] epoch:335	batch id:30	 lr:0.001340 loss:1.383927
[Train] epoch:335	batch id:40	 lr:0.001340 loss:1.428586
[Train] epoch:335	batch id:50	 lr:0.001340 loss:1.325572
[Train] epoch:335	batch id:60	 lr:0.001340 loss:1.340059
[Train] epoch:335	batch id:70	 lr:0.001340 loss:1.300679
[Train] epoch:335	batch id:80	 lr:0.001340 loss:1.387448
[Train] epoch:335	batch id:90	 lr:0.001340 loss:1.315759
[Train] epoch:335	batch id:100	 lr:0.001340 loss:1.358243
[Train] epoch:335	batch id:110	 lr:0.001340 loss:1.348036
[Train] epoch:335	batch id:120	 lr:0.001340 loss:1.361166
[Train] epoch:335	batch id:130	 lr:0.001340 loss:1.322402
[Train] epoch:335	batch id:140	 lr:0.001340 loss:1.317862
[Train] epoch:335	batch id:150	 lr:0.001340 loss:1.321382
[Train] epoch:335	batch id:160	 lr:0.001340 loss:1.340763
[Train] epoch:335	batch id:170	 lr:0.001340 loss:1.348429
[Train] epoch:335	batch id:180	 lr:0.001340 loss:1.336084
[Train] epoch:335	batch id:190	 lr:0.001340 loss:1.367255
[Train] epoch:335	batch id:200	 lr:0.001340 loss:1.338903
[Train] epoch:335	batch id:210	 lr:0.001340 loss:1.383486
[Train] epoch:335	batch id:220	 lr:0.001340 loss:1.299276
[Train] epoch:335	batch id:230	 lr:0.001340 loss:1.333066
[Train] epoch:335	batch id:240	 lr:0.001340 loss:1.319961
[Train] epoch:335	batch id:250	 lr:0.001340 loss:1.325871
[Train] epoch:335	batch id:260	 lr:0.001340 loss:1.337894
[Train] epoch:335	batch id:270	 lr:0.001340 loss:1.294276
[Train] epoch:335	batch id:280	 lr:0.001340 loss:1.332055
[Train] epoch:335	batch id:290	 lr:0.001340 loss:1.363270
[Train] epoch:335	batch id:300	 lr:0.001340 loss:1.300391
[Train] 335, loss: 1.329907, train acc: 0.996743, 
[Test] epoch:335	batch id:0	 loss:1.253196
[Test] epoch:335	batch id:10	 loss:1.445917
[Test] epoch:335	batch id:20	 loss:1.409925
[Test] epoch:335	batch id:30	 loss:1.340876
[Test] epoch:335	batch id:40	 loss:1.510379
[Test] epoch:335	batch id:50	 loss:1.514730
[Test] epoch:335	batch id:60	 loss:1.251243
[Test] epoch:335	batch id:70	 loss:1.354232
[Test] epoch:335	batch id:80	 loss:1.625880
[Test] epoch:335	batch id:90	 loss:1.641123
[Test] epoch:335	batch id:100	 loss:1.954854
[Test] epoch:335	batch id:110	 loss:1.324513
[Test] epoch:335	batch id:120	 loss:1.285103
[Test] epoch:335	batch id:130	 loss:1.305290
[Test] epoch:335	batch id:140	 loss:1.343377
[Test] epoch:335	batch id:150	 loss:1.778934
[Test] 335, loss: 1.470246, test acc: 0.918152,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:336	batch id:0	 lr:0.001290 loss:1.402852
[Train] epoch:336	batch id:10	 lr:0.001290 loss:1.378137
[Train] epoch:336	batch id:20	 lr:0.001290 loss:1.320207
[Train] epoch:336	batch id:30	 lr:0.001290 loss:1.296089
[Train] epoch:336	batch id:40	 lr:0.001290 loss:1.360180
[Train] epoch:336	batch id:50	 lr:0.001290 loss:1.331770
[Train] epoch:336	batch id:60	 lr:0.001290 loss:1.325074
[Train] epoch:336	batch id:70	 lr:0.001290 loss:1.300858
[Train] epoch:336	batch id:80	 lr:0.001290 loss:1.311691
[Train] epoch:336	batch id:90	 lr:0.001290 loss:1.307717
[Train] epoch:336	batch id:100	 lr:0.001290 loss:1.334930
[Train] epoch:336	batch id:110	 lr:0.001290 loss:1.306895
[Train] epoch:336	batch id:120	 lr:0.001290 loss:1.307325
[Train] epoch:336	batch id:130	 lr:0.001290 loss:1.302267
[Train] epoch:336	batch id:140	 lr:0.001290 loss:1.325443
[Train] epoch:336	batch id:150	 lr:0.001290 loss:1.328082
[Train] epoch:336	batch id:160	 lr:0.001290 loss:1.318145
[Train] epoch:336	batch id:170	 lr:0.001290 loss:1.344504
[Train] epoch:336	batch id:180	 lr:0.001290 loss:1.345713
[Train] epoch:336	batch id:190	 lr:0.001290 loss:1.324723
[Train] epoch:336	batch id:200	 lr:0.001290 loss:1.307718
[Train] epoch:336	batch id:210	 lr:0.001290 loss:1.332855
[Train] epoch:336	batch id:220	 lr:0.001290 loss:1.319745
[Train] epoch:336	batch id:230	 lr:0.001290 loss:1.312561
[Train] epoch:336	batch id:240	 lr:0.001290 loss:1.355712
[Train] epoch:336	batch id:250	 lr:0.001290 loss:1.305421
[Train] epoch:336	batch id:260	 lr:0.001290 loss:1.325684
[Train] epoch:336	batch id:270	 lr:0.001290 loss:1.310216
[Train] epoch:336	batch id:280	 lr:0.001290 loss:1.305112
[Train] epoch:336	batch id:290	 lr:0.001290 loss:1.300754
[Train] epoch:336	batch id:300	 lr:0.001290 loss:1.331912
[Train] 336, loss: 1.330234, train acc: 0.996132, 
[Test] epoch:336	batch id:0	 loss:1.248950
[Test] epoch:336	batch id:10	 loss:1.419114
[Test] epoch:336	batch id:20	 loss:1.395108
[Test] epoch:336	batch id:30	 loss:1.396706
[Test] epoch:336	batch id:40	 loss:1.562902
[Test] epoch:336	batch id:50	 loss:1.544949
[Test] epoch:336	batch id:60	 loss:1.244871
[Test] epoch:336	batch id:70	 loss:1.462763
[Test] epoch:336	batch id:80	 loss:1.640470
[Test] epoch:336	batch id:90	 loss:1.616335
[Test] epoch:336	batch id:100	 loss:1.956596
[Test] epoch:336	batch id:110	 loss:1.326377
[Test] epoch:336	batch id:120	 loss:1.328543
[Test] epoch:336	batch id:130	 loss:1.276338
[Test] epoch:336	batch id:140	 loss:1.376346
[Test] epoch:336	batch id:150	 loss:1.880563
[Test] 336, loss: 1.469511, test acc: 0.914506,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:337	batch id:0	 lr:0.001244 loss:1.307272
[Train] epoch:337	batch id:10	 lr:0.001244 loss:1.343484
[Train] epoch:337	batch id:20	 lr:0.001244 loss:1.331448
[Train] epoch:337	batch id:30	 lr:0.001244 loss:1.343945
[Train] epoch:337	batch id:40	 lr:0.001244 loss:1.331774
[Train] epoch:337	batch id:50	 lr:0.001244 loss:1.307793
[Train] epoch:337	batch id:60	 lr:0.001244 loss:1.299729
[Train] epoch:337	batch id:70	 lr:0.001244 loss:1.335223
[Train] epoch:337	batch id:80	 lr:0.001244 loss:1.310704
[Train] epoch:337	batch id:90	 lr:0.001244 loss:1.327123
[Train] epoch:337	batch id:100	 lr:0.001244 loss:1.335255
[Train] epoch:337	batch id:110	 lr:0.001244 loss:1.320121
[Train] epoch:337	batch id:120	 lr:0.001244 loss:1.340969
[Train] epoch:337	batch id:130	 lr:0.001244 loss:1.331963
[Train] epoch:337	batch id:140	 lr:0.001244 loss:1.330105
[Train] epoch:337	batch id:150	 lr:0.001244 loss:1.373112
[Train] epoch:337	batch id:160	 lr:0.001244 loss:1.302015
[Train] epoch:337	batch id:170	 lr:0.001244 loss:1.342368
[Train] epoch:337	batch id:180	 lr:0.001244 loss:1.374430
[Train] epoch:337	batch id:190	 lr:0.001244 loss:1.307540
[Train] epoch:337	batch id:200	 lr:0.001244 loss:1.326378
[Train] epoch:337	batch id:210	 lr:0.001244 loss:1.351330
[Train] epoch:337	batch id:220	 lr:0.001244 loss:1.320428
[Train] epoch:337	batch id:230	 lr:0.001244 loss:1.352840
[Train] epoch:337	batch id:240	 lr:0.001244 loss:1.302278
[Train] epoch:337	batch id:250	 lr:0.001244 loss:1.326003
[Train] epoch:337	batch id:260	 lr:0.001244 loss:1.308853
[Train] epoch:337	batch id:270	 lr:0.001244 loss:1.313248
[Train] epoch:337	batch id:280	 lr:0.001244 loss:1.306022
[Train] epoch:337	batch id:290	 lr:0.001244 loss:1.306623
[Train] epoch:337	batch id:300	 lr:0.001244 loss:1.333591
[Train] 337, loss: 1.328184, train acc: 0.997048, 
[Test] epoch:337	batch id:0	 loss:1.250055
[Test] epoch:337	batch id:10	 loss:1.427336
[Test] epoch:337	batch id:20	 loss:1.334224
[Test] epoch:337	batch id:30	 loss:1.315434
[Test] epoch:337	batch id:40	 loss:1.524187
[Test] epoch:337	batch id:50	 loss:1.523146
[Test] epoch:337	batch id:60	 loss:1.248103
[Test] epoch:337	batch id:70	 loss:1.423257
[Test] epoch:337	batch id:80	 loss:1.630514
[Test] epoch:337	batch id:90	 loss:1.665286
[Test] epoch:337	batch id:100	 loss:1.918227
[Test] epoch:337	batch id:110	 loss:1.353424
[Test] epoch:337	batch id:120	 loss:1.282268
[Test] epoch:337	batch id:130	 loss:1.321976
[Test] epoch:337	batch id:140	 loss:1.312984
[Test] epoch:337	batch id:150	 loss:1.739046
[Test] 337, loss: 1.467131, test acc: 0.918152,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:338	batch id:0	 lr:0.001203 loss:1.314679
[Train] epoch:338	batch id:10	 lr:0.001203 loss:1.297714
[Train] epoch:338	batch id:20	 lr:0.001203 loss:1.323777
[Train] epoch:338	batch id:30	 lr:0.001203 loss:1.355401
[Train] epoch:338	batch id:40	 lr:0.001203 loss:1.338310
[Train] epoch:338	batch id:50	 lr:0.001203 loss:1.322266
[Train] epoch:338	batch id:60	 lr:0.001203 loss:1.315091
[Train] epoch:338	batch id:70	 lr:0.001203 loss:1.322976
[Train] epoch:338	batch id:80	 lr:0.001203 loss:1.346432
[Train] epoch:338	batch id:90	 lr:0.001203 loss:1.368781
[Train] epoch:338	batch id:100	 lr:0.001203 loss:1.309920
[Train] epoch:338	batch id:110	 lr:0.001203 loss:1.431288
[Train] epoch:338	batch id:120	 lr:0.001203 loss:1.310489
[Train] epoch:338	batch id:130	 lr:0.001203 loss:1.310352
[Train] epoch:338	batch id:140	 lr:0.001203 loss:1.339549
[Train] epoch:338	batch id:150	 lr:0.001203 loss:1.318311
[Train] epoch:338	batch id:160	 lr:0.001203 loss:1.309288
[Train] epoch:338	batch id:170	 lr:0.001203 loss:1.302823
[Train] epoch:338	batch id:180	 lr:0.001203 loss:1.334807
[Train] epoch:338	batch id:190	 lr:0.001203 loss:1.313910
[Train] epoch:338	batch id:200	 lr:0.001203 loss:1.318928
[Train] epoch:338	batch id:210	 lr:0.001203 loss:1.395405
[Train] epoch:338	batch id:220	 lr:0.001203 loss:1.346140
[Train] epoch:338	batch id:230	 lr:0.001203 loss:1.314968
[Train] epoch:338	batch id:240	 lr:0.001203 loss:1.303946
[Train] epoch:338	batch id:250	 lr:0.001203 loss:1.348929
[Train] epoch:338	batch id:260	 lr:0.001203 loss:1.313000
[Train] epoch:338	batch id:270	 lr:0.001203 loss:1.311863
[Train] epoch:338	batch id:280	 lr:0.001203 loss:1.347874
[Train] epoch:338	batch id:290	 lr:0.001203 loss:1.303481
[Train] epoch:338	batch id:300	 lr:0.001203 loss:1.308597
[Train] 338, loss: 1.331165, train acc: 0.997150, 
[Test] epoch:338	batch id:0	 loss:1.253664
[Test] epoch:338	batch id:10	 loss:1.421203
[Test] epoch:338	batch id:20	 loss:1.440673
[Test] epoch:338	batch id:30	 loss:1.320605
[Test] epoch:338	batch id:40	 loss:1.596848
[Test] epoch:338	batch id:50	 loss:1.603569
[Test] epoch:338	batch id:60	 loss:1.247928
[Test] epoch:338	batch id:70	 loss:1.557517
[Test] epoch:338	batch id:80	 loss:1.618794
[Test] epoch:338	batch id:90	 loss:1.582342
[Test] epoch:338	batch id:100	 loss:1.889097
[Test] epoch:338	batch id:110	 loss:1.349415
[Test] epoch:338	batch id:120	 loss:1.306481
[Test] epoch:338	batch id:130	 loss:1.303782
[Test] epoch:338	batch id:140	 loss:1.293200
[Test] epoch:338	batch id:150	 loss:1.793729
[Test] 338, loss: 1.470164, test acc: 0.917747,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:339	batch id:0	 lr:0.001165 loss:1.312019
[Train] epoch:339	batch id:10	 lr:0.001165 loss:1.319571
[Train] epoch:339	batch id:20	 lr:0.001165 loss:1.324850
[Train] epoch:339	batch id:30	 lr:0.001165 loss:1.325373
[Train] epoch:339	batch id:40	 lr:0.001165 loss:1.320591
[Train] epoch:339	batch id:50	 lr:0.001165 loss:1.369998
[Train] epoch:339	batch id:60	 lr:0.001165 loss:1.291741
[Train] epoch:339	batch id:70	 lr:0.001165 loss:1.336718
[Train] epoch:339	batch id:80	 lr:0.001165 loss:1.325787
[Train] epoch:339	batch id:90	 lr:0.001165 loss:1.356172
[Train] epoch:339	batch id:100	 lr:0.001165 loss:1.303522
[Train] epoch:339	batch id:110	 lr:0.001165 loss:1.347936
[Train] epoch:339	batch id:120	 lr:0.001165 loss:1.322356
[Train] epoch:339	batch id:130	 lr:0.001165 loss:1.329856
[Train] epoch:339	batch id:140	 lr:0.001165 loss:1.321730
[Train] epoch:339	batch id:150	 lr:0.001165 loss:1.310875
[Train] epoch:339	batch id:160	 lr:0.001165 loss:1.349328
[Train] epoch:339	batch id:170	 lr:0.001165 loss:1.325276
[Train] epoch:339	batch id:180	 lr:0.001165 loss:1.382186
[Train] epoch:339	batch id:190	 lr:0.001165 loss:1.316548
[Train] epoch:339	batch id:200	 lr:0.001165 loss:1.350874
[Train] epoch:339	batch id:210	 lr:0.001165 loss:1.346585
[Train] epoch:339	batch id:220	 lr:0.001165 loss:1.354179
[Train] epoch:339	batch id:230	 lr:0.001165 loss:1.355614
[Train] epoch:339	batch id:240	 lr:0.001165 loss:1.339564
[Train] epoch:339	batch id:250	 lr:0.001165 loss:1.439540
[Train] epoch:339	batch id:260	 lr:0.001165 loss:1.426046
[Train] epoch:339	batch id:270	 lr:0.001165 loss:1.326067
[Train] epoch:339	batch id:280	 lr:0.001165 loss:1.314915
[Train] epoch:339	batch id:290	 lr:0.001165 loss:1.314509
[Train] epoch:339	batch id:300	 lr:0.001165 loss:1.306087
[Train] 339, loss: 1.333906, train acc: 0.996539, 
[Test] epoch:339	batch id:0	 loss:1.253182
[Test] epoch:339	batch id:10	 loss:1.423497
[Test] epoch:339	batch id:20	 loss:1.430110
[Test] epoch:339	batch id:30	 loss:1.369599
[Test] epoch:339	batch id:40	 loss:1.688923
[Test] epoch:339	batch id:50	 loss:1.529402
[Test] epoch:339	batch id:60	 loss:1.261368
[Test] epoch:339	batch id:70	 loss:1.447278
[Test] epoch:339	batch id:80	 loss:1.597826
[Test] epoch:339	batch id:90	 loss:1.693809
[Test] epoch:339	batch id:100	 loss:2.017402
[Test] epoch:339	batch id:110	 loss:1.373629
[Test] epoch:339	batch id:120	 loss:1.293052
[Test] epoch:339	batch id:130	 loss:1.285393
[Test] epoch:339	batch id:140	 loss:1.384129
[Test] epoch:339	batch id:150	 loss:1.801922
[Test] 339, loss: 1.477612, test acc: 0.915721,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:340	batch id:0	 lr:0.001131 loss:1.318301
[Train] epoch:340	batch id:10	 lr:0.001131 loss:1.323411
[Train] epoch:340	batch id:20	 lr:0.001131 loss:1.320165
[Train] epoch:340	batch id:30	 lr:0.001131 loss:1.329444
[Train] epoch:340	batch id:40	 lr:0.001131 loss:1.309510
[Train] epoch:340	batch id:50	 lr:0.001131 loss:1.349976
[Train] epoch:340	batch id:60	 lr:0.001131 loss:1.343056
[Train] epoch:340	batch id:70	 lr:0.001131 loss:1.303071
[Train] epoch:340	batch id:80	 lr:0.001131 loss:1.338983
[Train] epoch:340	batch id:90	 lr:0.001131 loss:1.315828
[Train] epoch:340	batch id:100	 lr:0.001131 loss:1.408960
[Train] epoch:340	batch id:110	 lr:0.001131 loss:1.334678
[Train] epoch:340	batch id:120	 lr:0.001131 loss:1.377920
[Train] epoch:340	batch id:130	 lr:0.001131 loss:1.319608
[Train] epoch:340	batch id:140	 lr:0.001131 loss:1.304347
[Train] epoch:340	batch id:150	 lr:0.001131 loss:1.332712
[Train] epoch:340	batch id:160	 lr:0.001131 loss:1.353759
[Train] epoch:340	batch id:170	 lr:0.001131 loss:1.299480
[Train] epoch:340	batch id:180	 lr:0.001131 loss:1.326547
[Train] epoch:340	batch id:190	 lr:0.001131 loss:1.319521
[Train] epoch:340	batch id:200	 lr:0.001131 loss:1.320961
[Train] epoch:340	batch id:210	 lr:0.001131 loss:1.300518
[Train] epoch:340	batch id:220	 lr:0.001131 loss:1.415231
[Train] epoch:340	batch id:230	 lr:0.001131 loss:1.316187
[Train] epoch:340	batch id:240	 lr:0.001131 loss:1.332516
[Train] epoch:340	batch id:250	 lr:0.001131 loss:1.334895
[Train] epoch:340	batch id:260	 lr:0.001131 loss:1.316977
[Train] epoch:340	batch id:270	 lr:0.001131 loss:1.391242
[Train] epoch:340	batch id:280	 lr:0.001131 loss:1.308422
[Train] epoch:340	batch id:290	 lr:0.001131 loss:1.366321
[Train] epoch:340	batch id:300	 lr:0.001131 loss:1.312869
[Train] 340, loss: 1.330692, train acc: 0.997455, 
[Test] epoch:340	batch id:0	 loss:1.251043
[Test] epoch:340	batch id:10	 loss:1.437556
[Test] epoch:340	batch id:20	 loss:1.424334
[Test] epoch:340	batch id:30	 loss:1.325456
[Test] epoch:340	batch id:40	 loss:1.553498
[Test] epoch:340	batch id:50	 loss:1.514458
[Test] epoch:340	batch id:60	 loss:1.250196
[Test] epoch:340	batch id:70	 loss:1.367832
[Test] epoch:340	batch id:80	 loss:1.538015
[Test] epoch:340	batch id:90	 loss:1.614133
[Test] epoch:340	batch id:100	 loss:1.976134
[Test] epoch:340	batch id:110	 loss:1.342256
[Test] epoch:340	batch id:120	 loss:1.299446
[Test] epoch:340	batch id:130	 loss:1.290335
[Test] epoch:340	batch id:140	 loss:1.332026
[Test] epoch:340	batch id:150	 loss:1.585700
[Test] 340, loss: 1.466966, test acc: 0.914911,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:341	batch id:0	 lr:0.001101 loss:1.328877
[Train] epoch:341	batch id:10	 lr:0.001101 loss:1.312729
[Train] epoch:341	batch id:20	 lr:0.001101 loss:1.300560
[Train] epoch:341	batch id:30	 lr:0.001101 loss:1.321627
[Train] epoch:341	batch id:40	 lr:0.001101 loss:1.371790
[Train] epoch:341	batch id:50	 lr:0.001101 loss:1.318569
[Train] epoch:341	batch id:60	 lr:0.001101 loss:1.414181
[Train] epoch:341	batch id:70	 lr:0.001101 loss:1.337816
[Train] epoch:341	batch id:80	 lr:0.001101 loss:1.360195
[Train] epoch:341	batch id:90	 lr:0.001101 loss:1.332384
[Train] epoch:341	batch id:100	 lr:0.001101 loss:1.347948
[Train] epoch:341	batch id:110	 lr:0.001101 loss:1.324098
[Train] epoch:341	batch id:120	 lr:0.001101 loss:1.381916
[Train] epoch:341	batch id:130	 lr:0.001101 loss:1.347296
[Train] epoch:341	batch id:140	 lr:0.001101 loss:1.303761
[Train] epoch:341	batch id:150	 lr:0.001101 loss:1.306345
[Train] epoch:341	batch id:160	 lr:0.001101 loss:1.336304
[Train] epoch:341	batch id:170	 lr:0.001101 loss:1.332441
[Train] epoch:341	batch id:180	 lr:0.001101 loss:1.314087
[Train] epoch:341	batch id:190	 lr:0.001101 loss:1.311599
[Train] epoch:341	batch id:200	 lr:0.001101 loss:1.327707
[Train] epoch:341	batch id:210	 lr:0.001101 loss:1.317325
[Train] epoch:341	batch id:220	 lr:0.001101 loss:1.321835
[Train] epoch:341	batch id:230	 lr:0.001101 loss:1.307067
[Train] epoch:341	batch id:240	 lr:0.001101 loss:1.331072
[Train] epoch:341	batch id:250	 lr:0.001101 loss:1.299207
[Train] epoch:341	batch id:260	 lr:0.001101 loss:1.301989
[Train] epoch:341	batch id:270	 lr:0.001101 loss:1.333820
[Train] epoch:341	batch id:280	 lr:0.001101 loss:1.310405
[Train] epoch:341	batch id:290	 lr:0.001101 loss:1.308054
[Train] epoch:341	batch id:300	 lr:0.001101 loss:1.365482
[Train] 341, loss: 1.331669, train acc: 0.996234, 
[Test] epoch:341	batch id:0	 loss:1.283245
[Test] epoch:341	batch id:10	 loss:1.445190
[Test] epoch:341	batch id:20	 loss:1.507769
[Test] epoch:341	batch id:30	 loss:1.322330
[Test] epoch:341	batch id:40	 loss:1.588269
[Test] epoch:341	batch id:50	 loss:1.490006
[Test] epoch:341	batch id:60	 loss:1.259449
[Test] epoch:341	batch id:70	 loss:1.332994
[Test] epoch:341	batch id:80	 loss:1.549458
[Test] epoch:341	batch id:90	 loss:1.620160
[Test] epoch:341	batch id:100	 loss:1.981203
[Test] epoch:341	batch id:110	 loss:1.317260
[Test] epoch:341	batch id:120	 loss:1.289031
[Test] epoch:341	batch id:130	 loss:1.331494
[Test] epoch:341	batch id:140	 loss:1.338077
[Test] epoch:341	batch id:150	 loss:1.694539
[Test] 341, loss: 1.468088, test acc: 0.918558,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:342	batch id:0	 lr:0.001075 loss:1.301945
[Train] epoch:342	batch id:10	 lr:0.001075 loss:1.309373
[Train] epoch:342	batch id:20	 lr:0.001075 loss:1.305959
[Train] epoch:342	batch id:30	 lr:0.001075 loss:1.332753
[Train] epoch:342	batch id:40	 lr:0.001075 loss:1.317533
[Train] epoch:342	batch id:50	 lr:0.001075 loss:1.300836
[Train] epoch:342	batch id:60	 lr:0.001075 loss:1.312485
[Train] epoch:342	batch id:70	 lr:0.001075 loss:1.309049
[Train] epoch:342	batch id:80	 lr:0.001075 loss:1.350566
[Train] epoch:342	batch id:90	 lr:0.001075 loss:1.318200
[Train] epoch:342	batch id:100	 lr:0.001075 loss:1.314394
[Train] epoch:342	batch id:110	 lr:0.001075 loss:1.349352
[Train] epoch:342	batch id:120	 lr:0.001075 loss:1.306556
[Train] epoch:342	batch id:130	 lr:0.001075 loss:1.299490
[Train] epoch:342	batch id:140	 lr:0.001075 loss:1.348817
[Train] epoch:342	batch id:150	 lr:0.001075 loss:1.363170
[Train] epoch:342	batch id:160	 lr:0.001075 loss:1.321098
[Train] epoch:342	batch id:170	 lr:0.001075 loss:1.362595
[Train] epoch:342	batch id:180	 lr:0.001075 loss:1.345919
[Train] epoch:342	batch id:190	 lr:0.001075 loss:1.311082
[Train] epoch:342	batch id:200	 lr:0.001075 loss:1.338521
[Train] epoch:342	batch id:210	 lr:0.001075 loss:1.347599
[Train] epoch:342	batch id:220	 lr:0.001075 loss:1.309739
[Train] epoch:342	batch id:230	 lr:0.001075 loss:1.306407
[Train] epoch:342	batch id:240	 lr:0.001075 loss:1.311838
[Train] epoch:342	batch id:250	 lr:0.001075 loss:1.319453
[Train] epoch:342	batch id:260	 lr:0.001075 loss:1.332327
[Train] epoch:342	batch id:270	 lr:0.001075 loss:1.314037
[Train] epoch:342	batch id:280	 lr:0.001075 loss:1.306626
[Train] epoch:342	batch id:290	 lr:0.001075 loss:1.315496
[Train] epoch:342	batch id:300	 lr:0.001075 loss:1.331604
[Train] 342, loss: 1.328928, train acc: 0.996743, 
[Test] epoch:342	batch id:0	 loss:1.248878
[Test] epoch:342	batch id:10	 loss:1.461609
[Test] epoch:342	batch id:20	 loss:1.423182
[Test] epoch:342	batch id:30	 loss:1.392004
[Test] epoch:342	batch id:40	 loss:1.628458
[Test] epoch:342	batch id:50	 loss:1.535802
[Test] epoch:342	batch id:60	 loss:1.250218
[Test] epoch:342	batch id:70	 loss:1.419907
[Test] epoch:342	batch id:80	 loss:1.637727
[Test] epoch:342	batch id:90	 loss:1.658097
[Test] epoch:342	batch id:100	 loss:2.050842
[Test] epoch:342	batch id:110	 loss:1.299049
[Test] epoch:342	batch id:120	 loss:1.322207
[Test] epoch:342	batch id:130	 loss:1.282326
[Test] epoch:342	batch id:140	 loss:1.335180
[Test] epoch:342	batch id:150	 loss:1.693605
[Test] 342, loss: 1.471558, test acc: 0.918152,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:343	batch id:0	 lr:0.001053 loss:1.315033
[Train] epoch:343	batch id:10	 lr:0.001053 loss:1.306179
[Train] epoch:343	batch id:20	 lr:0.001053 loss:1.374166
[Train] epoch:343	batch id:30	 lr:0.001053 loss:1.306789
[Train] epoch:343	batch id:40	 lr:0.001053 loss:1.298161
[Train] epoch:343	batch id:50	 lr:0.001053 loss:1.315990
[Train] epoch:343	batch id:60	 lr:0.001053 loss:1.325334
[Train] epoch:343	batch id:70	 lr:0.001053 loss:1.320061
[Train] epoch:343	batch id:80	 lr:0.001053 loss:1.290976
[Train] epoch:343	batch id:90	 lr:0.001053 loss:1.332604
[Train] epoch:343	batch id:100	 lr:0.001053 loss:1.363953
[Train] epoch:343	batch id:110	 lr:0.001053 loss:1.333302
[Train] epoch:343	batch id:120	 lr:0.001053 loss:1.325144
[Train] epoch:343	batch id:130	 lr:0.001053 loss:1.307745
[Train] epoch:343	batch id:140	 lr:0.001053 loss:1.344543
[Train] epoch:343	batch id:150	 lr:0.001053 loss:1.308802
[Train] epoch:343	batch id:160	 lr:0.001053 loss:1.326869
[Train] epoch:343	batch id:170	 lr:0.001053 loss:1.310129
[Train] epoch:343	batch id:180	 lr:0.001053 loss:1.316106
[Train] epoch:343	batch id:190	 lr:0.001053 loss:1.366114
[Train] epoch:343	batch id:200	 lr:0.001053 loss:1.340291
[Train] epoch:343	batch id:210	 lr:0.001053 loss:1.306136
[Train] epoch:343	batch id:220	 lr:0.001053 loss:1.323981
[Train] epoch:343	batch id:230	 lr:0.001053 loss:1.306128
[Train] epoch:343	batch id:240	 lr:0.001053 loss:1.302405
[Train] epoch:343	batch id:250	 lr:0.001053 loss:1.320529
[Train] epoch:343	batch id:260	 lr:0.001053 loss:1.312302
[Train] epoch:343	batch id:270	 lr:0.001053 loss:1.312536
[Train] epoch:343	batch id:280	 lr:0.001053 loss:1.335170
[Train] epoch:343	batch id:290	 lr:0.001053 loss:1.327666
[Train] epoch:343	batch id:300	 lr:0.001053 loss:1.347053
[Train] 343, loss: 1.326158, train acc: 0.997455, 
[Test] epoch:343	batch id:0	 loss:1.280240
[Test] epoch:343	batch id:10	 loss:1.458602
[Test] epoch:343	batch id:20	 loss:1.434244
[Test] epoch:343	batch id:30	 loss:1.359914
[Test] epoch:343	batch id:40	 loss:1.637872
[Test] epoch:343	batch id:50	 loss:1.529105
[Test] epoch:343	batch id:60	 loss:1.253789
[Test] epoch:343	batch id:70	 loss:1.412819
[Test] epoch:343	batch id:80	 loss:1.623487
[Test] epoch:343	batch id:90	 loss:1.568465
[Test] epoch:343	batch id:100	 loss:2.101509
[Test] epoch:343	batch id:110	 loss:1.283223
[Test] epoch:343	batch id:120	 loss:1.290960
[Test] epoch:343	batch id:130	 loss:1.317106
[Test] epoch:343	batch id:140	 loss:1.314853
[Test] epoch:343	batch id:150	 loss:1.638729
[Test] 343, loss: 1.468757, test acc: 0.915721,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:344	batch id:0	 lr:0.001035 loss:1.313296
[Train] epoch:344	batch id:10	 lr:0.001035 loss:1.314104
[Train] epoch:344	batch id:20	 lr:0.001035 loss:1.321475
[Train] epoch:344	batch id:30	 lr:0.001035 loss:1.337485
[Train] epoch:344	batch id:40	 lr:0.001035 loss:1.313793
[Train] epoch:344	batch id:50	 lr:0.001035 loss:1.311673
[Train] epoch:344	batch id:60	 lr:0.001035 loss:1.318605
[Train] epoch:344	batch id:70	 lr:0.001035 loss:1.342597
[Train] epoch:344	batch id:80	 lr:0.001035 loss:1.352098
[Train] epoch:344	batch id:90	 lr:0.001035 loss:1.371512
[Train] epoch:344	batch id:100	 lr:0.001035 loss:1.312185
[Train] epoch:344	batch id:110	 lr:0.001035 loss:1.329977
[Train] epoch:344	batch id:120	 lr:0.001035 loss:1.332302
[Train] epoch:344	batch id:130	 lr:0.001035 loss:1.315622
[Train] epoch:344	batch id:140	 lr:0.001035 loss:1.320692
[Train] epoch:344	batch id:150	 lr:0.001035 loss:1.321570
[Train] epoch:344	batch id:160	 lr:0.001035 loss:1.337417
[Train] epoch:344	batch id:170	 lr:0.001035 loss:1.326171
[Train] epoch:344	batch id:180	 lr:0.001035 loss:1.327149
[Train] epoch:344	batch id:190	 lr:0.001035 loss:1.342459
[Train] epoch:344	batch id:200	 lr:0.001035 loss:1.329778
[Train] epoch:344	batch id:210	 lr:0.001035 loss:1.333778
[Train] epoch:344	batch id:220	 lr:0.001035 loss:1.303075
[Train] epoch:344	batch id:230	 lr:0.001035 loss:1.327350
[Train] epoch:344	batch id:240	 lr:0.001035 loss:1.321173
[Train] epoch:344	batch id:250	 lr:0.001035 loss:1.326857
[Train] epoch:344	batch id:260	 lr:0.001035 loss:1.300727
[Train] epoch:344	batch id:270	 lr:0.001035 loss:1.302449
[Train] epoch:344	batch id:280	 lr:0.001035 loss:1.364585
[Train] epoch:344	batch id:290	 lr:0.001035 loss:1.314417
[Train] epoch:344	batch id:300	 lr:0.001035 loss:1.326110
[Train] 344, loss: 1.330905, train acc: 0.996743, 
[Test] epoch:344	batch id:0	 loss:1.246373
[Test] epoch:344	batch id:10	 loss:1.393892
[Test] epoch:344	batch id:20	 loss:1.421671
[Test] epoch:344	batch id:30	 loss:1.393258
[Test] epoch:344	batch id:40	 loss:1.575569
[Test] epoch:344	batch id:50	 loss:1.559930
[Test] epoch:344	batch id:60	 loss:1.244927
[Test] epoch:344	batch id:70	 loss:1.466037
[Test] epoch:344	batch id:80	 loss:1.591026
[Test] epoch:344	batch id:90	 loss:1.629512
[Test] epoch:344	batch id:100	 loss:1.915933
[Test] epoch:344	batch id:110	 loss:1.349086
[Test] epoch:344	batch id:120	 loss:1.302977
[Test] epoch:344	batch id:130	 loss:1.332940
[Test] epoch:344	batch id:140	 loss:1.354659
[Test] epoch:344	batch id:150	 loss:1.802017
[Test] 344, loss: 1.466055, test acc: 0.917342,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:345	batch id:0	 lr:0.001020 loss:1.299333
[Train] epoch:345	batch id:10	 lr:0.001020 loss:1.332682
[Train] epoch:345	batch id:20	 lr:0.001020 loss:1.303406
[Train] epoch:345	batch id:30	 lr:0.001020 loss:1.327127
[Train] epoch:345	batch id:40	 lr:0.001020 loss:1.361826
[Train] epoch:345	batch id:50	 lr:0.001020 loss:1.323369
[Train] epoch:345	batch id:60	 lr:0.001020 loss:1.337087
[Train] epoch:345	batch id:70	 lr:0.001020 loss:1.367052
[Train] epoch:345	batch id:80	 lr:0.001020 loss:1.317783
[Train] epoch:345	batch id:90	 lr:0.001020 loss:1.323481
[Train] epoch:345	batch id:100	 lr:0.001020 loss:1.348057
[Train] epoch:345	batch id:110	 lr:0.001020 loss:1.319111
[Train] epoch:345	batch id:120	 lr:0.001020 loss:1.295981
[Train] epoch:345	batch id:130	 lr:0.001020 loss:1.302452
[Train] epoch:345	batch id:140	 lr:0.001020 loss:1.348192
[Train] epoch:345	batch id:150	 lr:0.001020 loss:1.320518
[Train] epoch:345	batch id:160	 lr:0.001020 loss:1.328107
[Train] epoch:345	batch id:170	 lr:0.001020 loss:1.329204
[Train] epoch:345	batch id:180	 lr:0.001020 loss:1.327143
[Train] epoch:345	batch id:190	 lr:0.001020 loss:1.318173
[Train] epoch:345	batch id:200	 lr:0.001020 loss:1.323163
[Train] epoch:345	batch id:210	 lr:0.001020 loss:1.308765
[Train] epoch:345	batch id:220	 lr:0.001020 loss:1.349593
[Train] epoch:345	batch id:230	 lr:0.001020 loss:1.311811
[Train] epoch:345	batch id:240	 lr:0.001020 loss:1.296907
[Train] epoch:345	batch id:250	 lr:0.001020 loss:1.301131
[Train] epoch:345	batch id:260	 lr:0.001020 loss:1.323678
[Train] epoch:345	batch id:270	 lr:0.001020 loss:1.352412
[Train] epoch:345	batch id:280	 lr:0.001020 loss:1.322045
[Train] epoch:345	batch id:290	 lr:0.001020 loss:1.370554
[Train] epoch:345	batch id:300	 lr:0.001020 loss:1.344954
[Train] 345, loss: 1.328204, train acc: 0.996539, 
[Test] epoch:345	batch id:0	 loss:1.311899
[Test] epoch:345	batch id:10	 loss:1.365716
[Test] epoch:345	batch id:20	 loss:1.376342
[Test] epoch:345	batch id:30	 loss:1.307774
[Test] epoch:345	batch id:40	 loss:1.494773
[Test] epoch:345	batch id:50	 loss:1.512044
[Test] epoch:345	batch id:60	 loss:1.248860
[Test] epoch:345	batch id:70	 loss:1.449637
[Test] epoch:345	batch id:80	 loss:1.648128
[Test] epoch:345	batch id:90	 loss:1.572256
[Test] epoch:345	batch id:100	 loss:2.023557
[Test] epoch:345	batch id:110	 loss:1.314474
[Test] epoch:345	batch id:120	 loss:1.326270
[Test] epoch:345	batch id:130	 loss:1.311107
[Test] epoch:345	batch id:140	 loss:1.382971
[Test] epoch:345	batch id:150	 loss:1.814496
[Test] 345, loss: 1.466566, test acc: 0.917747,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:346	batch id:0	 lr:0.001010 loss:1.328334
[Train] epoch:346	batch id:10	 lr:0.001010 loss:1.323923
[Train] epoch:346	batch id:20	 lr:0.001010 loss:1.298276
[Train] epoch:346	batch id:30	 lr:0.001010 loss:1.362363
[Train] epoch:346	batch id:40	 lr:0.001010 loss:1.332421
[Train] epoch:346	batch id:50	 lr:0.001010 loss:1.357110
[Train] epoch:346	batch id:60	 lr:0.001010 loss:1.360410
[Train] epoch:346	batch id:70	 lr:0.001010 loss:1.316607
[Train] epoch:346	batch id:80	 lr:0.001010 loss:1.316311
[Train] epoch:346	batch id:90	 lr:0.001010 loss:1.314938
[Train] epoch:346	batch id:100	 lr:0.001010 loss:1.379625
[Train] epoch:346	batch id:110	 lr:0.001010 loss:1.374867
[Train] epoch:346	batch id:120	 lr:0.001010 loss:1.347355
[Train] epoch:346	batch id:130	 lr:0.001010 loss:1.323537
[Train] epoch:346	batch id:140	 lr:0.001010 loss:1.313639
[Train] epoch:346	batch id:150	 lr:0.001010 loss:1.342803
[Train] epoch:346	batch id:160	 lr:0.001010 loss:1.313664
[Train] epoch:346	batch id:170	 lr:0.001010 loss:1.320275
[Train] epoch:346	batch id:180	 lr:0.001010 loss:1.312674
[Train] epoch:346	batch id:190	 lr:0.001010 loss:1.308447
[Train] epoch:346	batch id:200	 lr:0.001010 loss:1.319910
[Train] epoch:346	batch id:210	 lr:0.001010 loss:1.311510
[Train] epoch:346	batch id:220	 lr:0.001010 loss:1.359241
[Train] epoch:346	batch id:230	 lr:0.001010 loss:1.349888
[Train] epoch:346	batch id:240	 lr:0.001010 loss:1.325439
[Train] epoch:346	batch id:250	 lr:0.001010 loss:1.326214
[Train] epoch:346	batch id:260	 lr:0.001010 loss:1.344573
[Train] epoch:346	batch id:270	 lr:0.001010 loss:1.328145
[Train] epoch:346	batch id:280	 lr:0.001010 loss:1.335900
[Train] epoch:346	batch id:290	 lr:0.001010 loss:1.336566
[Train] epoch:346	batch id:300	 lr:0.001010 loss:1.320990
[Train] 346, loss: 1.329739, train acc: 0.996946, 
[Test] epoch:346	batch id:0	 loss:1.260968
[Test] epoch:346	batch id:10	 loss:1.393713
[Test] epoch:346	batch id:20	 loss:1.411203
[Test] epoch:346	batch id:30	 loss:1.316167
[Test] epoch:346	batch id:40	 loss:1.480121
[Test] epoch:346	batch id:50	 loss:1.452898
[Test] epoch:346	batch id:60	 loss:1.249768
[Test] epoch:346	batch id:70	 loss:1.446494
[Test] epoch:346	batch id:80	 loss:1.532247
[Test] epoch:346	batch id:90	 loss:1.555198
[Test] epoch:346	batch id:100	 loss:1.992903
[Test] epoch:346	batch id:110	 loss:1.280213
[Test] epoch:346	batch id:120	 loss:1.280461
[Test] epoch:346	batch id:130	 loss:1.285051
[Test] epoch:346	batch id:140	 loss:1.353515
[Test] epoch:346	batch id:150	 loss:1.821746
[Test] 346, loss: 1.450365, test acc: 0.925851,
Max Acc:0.925851
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:347	batch id:0	 lr:0.001004 loss:1.318921
[Train] epoch:347	batch id:10	 lr:0.001004 loss:1.317054
[Train] epoch:347	batch id:20	 lr:0.001004 loss:1.305540
[Train] epoch:347	batch id:30	 lr:0.001004 loss:1.309277
[Train] epoch:347	batch id:40	 lr:0.001004 loss:1.312266
[Train] epoch:347	batch id:50	 lr:0.001004 loss:1.314242
[Train] epoch:347	batch id:60	 lr:0.001004 loss:1.315399
[Train] epoch:347	batch id:70	 lr:0.001004 loss:1.326108
[Train] epoch:347	batch id:80	 lr:0.001004 loss:1.342368
[Train] epoch:347	batch id:90	 lr:0.001004 loss:1.310383
[Train] epoch:347	batch id:100	 lr:0.001004 loss:1.332649
[Train] epoch:347	batch id:110	 lr:0.001004 loss:1.313355
[Train] epoch:347	batch id:120	 lr:0.001004 loss:1.338398
[Train] epoch:347	batch id:130	 lr:0.001004 loss:1.303843
[Train] epoch:347	batch id:140	 lr:0.001004 loss:1.371427
[Train] epoch:347	batch id:150	 lr:0.001004 loss:1.381948
[Train] epoch:347	batch id:160	 lr:0.001004 loss:1.326233
[Train] epoch:347	batch id:170	 lr:0.001004 loss:1.340921
[Train] epoch:347	batch id:180	 lr:0.001004 loss:1.298554
[Train] epoch:347	batch id:190	 lr:0.001004 loss:1.339067
[Train] epoch:347	batch id:200	 lr:0.001004 loss:1.331849
[Train] epoch:347	batch id:210	 lr:0.001004 loss:1.292705
[Train] epoch:347	batch id:220	 lr:0.001004 loss:1.340885
[Train] epoch:347	batch id:230	 lr:0.001004 loss:1.321046
[Train] epoch:347	batch id:240	 lr:0.001004 loss:1.335756
[Train] epoch:347	batch id:250	 lr:0.001004 loss:1.333135
[Train] epoch:347	batch id:260	 lr:0.001004 loss:1.337865
[Train] epoch:347	batch id:270	 lr:0.001004 loss:1.317401
[Train] epoch:347	batch id:280	 lr:0.001004 loss:1.371174
[Train] epoch:347	batch id:290	 lr:0.001004 loss:1.350598
[Train] epoch:347	batch id:300	 lr:0.001004 loss:1.353128
[Train] 347, loss: 1.327090, train acc: 0.997252, 
[Test] epoch:347	batch id:0	 loss:1.248516
[Test] epoch:347	batch id:10	 loss:1.358747
[Test] epoch:347	batch id:20	 loss:1.429164
[Test] epoch:347	batch id:30	 loss:1.353691
[Test] epoch:347	batch id:40	 loss:1.565487
[Test] epoch:347	batch id:50	 loss:1.507431
[Test] epoch:347	batch id:60	 loss:1.244063
[Test] epoch:347	batch id:70	 loss:1.519058
[Test] epoch:347	batch id:80	 loss:1.569286
[Test] epoch:347	batch id:90	 loss:1.575399
[Test] epoch:347	batch id:100	 loss:1.989116
[Test] epoch:347	batch id:110	 loss:1.317148
[Test] epoch:347	batch id:120	 loss:1.354387
[Test] epoch:347	batch id:130	 loss:1.277909
[Test] epoch:347	batch id:140	 loss:1.382400
[Test] epoch:347	batch id:150	 loss:1.846013
[Test] 347, loss: 1.460229, test acc: 0.920178,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:348	batch id:0	 lr:0.001000 loss:1.311153
[Train] epoch:348	batch id:10	 lr:0.001000 loss:1.317565
[Train] epoch:348	batch id:20	 lr:0.001000 loss:1.319523
[Train] epoch:348	batch id:30	 lr:0.001000 loss:1.323831
[Train] epoch:348	batch id:40	 lr:0.001000 loss:1.324059
[Train] epoch:348	batch id:50	 lr:0.001000 loss:1.324558
[Train] epoch:348	batch id:60	 lr:0.001000 loss:1.314759
[Train] epoch:348	batch id:70	 lr:0.001000 loss:1.358989
[Train] epoch:348	batch id:80	 lr:0.001000 loss:1.331327
[Train] epoch:348	batch id:90	 lr:0.001000 loss:1.316953
[Train] epoch:348	batch id:100	 lr:0.001000 loss:1.308398
[Train] epoch:348	batch id:110	 lr:0.001000 loss:1.325079
[Train] epoch:348	batch id:120	 lr:0.001000 loss:1.309443
[Train] epoch:348	batch id:130	 lr:0.001000 loss:1.309529
[Train] epoch:348	batch id:140	 lr:0.001000 loss:1.322224
[Train] epoch:348	batch id:150	 lr:0.001000 loss:1.340379
[Train] epoch:348	batch id:160	 lr:0.001000 loss:1.347748
[Train] epoch:348	batch id:170	 lr:0.001000 loss:1.397326
[Train] epoch:348	batch id:180	 lr:0.001000 loss:1.331780
[Train] epoch:348	batch id:190	 lr:0.001000 loss:1.296965
[Train] epoch:348	batch id:200	 lr:0.001000 loss:1.337454
[Train] epoch:348	batch id:210	 lr:0.001000 loss:1.321125
[Train] epoch:348	batch id:220	 lr:0.001000 loss:1.404861
[Train] epoch:348	batch id:230	 lr:0.001000 loss:1.340051
[Train] epoch:348	batch id:240	 lr:0.001000 loss:1.309039
[Train] epoch:348	batch id:250	 lr:0.001000 loss:1.314502
[Train] epoch:348	batch id:260	 lr:0.001000 loss:1.303917
[Train] epoch:348	batch id:270	 lr:0.001000 loss:1.310662
[Train] epoch:348	batch id:280	 lr:0.001000 loss:1.347981
[Train] epoch:348	batch id:290	 lr:0.001000 loss:1.352596
[Train] epoch:348	batch id:300	 lr:0.001000 loss:1.308608
[Train] 348, loss: 1.327463, train acc: 0.997150, 
[Test] epoch:348	batch id:0	 loss:1.265189
[Test] epoch:348	batch id:10	 loss:1.418825
[Test] epoch:348	batch id:20	 loss:1.527072
[Test] epoch:348	batch id:30	 loss:1.349678
[Test] epoch:348	batch id:40	 loss:1.523902
[Test] epoch:348	batch id:50	 loss:1.480409
[Test] epoch:348	batch id:60	 loss:1.255541
[Test] epoch:348	batch id:70	 loss:1.405928
[Test] epoch:348	batch id:80	 loss:1.561811
[Test] epoch:348	batch id:90	 loss:1.680271
[Test] epoch:348	batch id:100	 loss:1.914557
[Test] epoch:348	batch id:110	 loss:1.347150
[Test] epoch:348	batch id:120	 loss:1.291349
[Test] epoch:348	batch id:130	 loss:1.342831
[Test] epoch:348	batch id:140	 loss:1.362635
[Test] epoch:348	batch id:150	 loss:1.679848
[Test] 348, loss: 1.477102, test acc: 0.919368,
/root/miniconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[Train] epoch:349	batch id:0	 lr:0.001000 loss:1.305441
[Train] epoch:349	batch id:10	 lr:0.001000 loss:1.324615
[Train] epoch:349	batch id:20	 lr:0.001000 loss:1.343674
[Train] epoch:349	batch id:30	 lr:0.001000 loss:1.360213
[Train] epoch:349	batch id:40	 lr:0.001000 loss:1.371970
[Train] epoch:349	batch id:50	 lr:0.001000 loss:1.345044
[Train] epoch:349	batch id:60	 lr:0.001000 loss:1.340783
[Train] epoch:349	batch id:70	 lr:0.001000 loss:1.323160
[Train] epoch:349	batch id:80	 lr:0.001000 loss:1.315817
[Train] epoch:349	batch id:90	 lr:0.001000 loss:1.320981
[Train] epoch:349	batch id:100	 lr:0.001000 loss:1.302138
[Train] epoch:349	batch id:110	 lr:0.001000 loss:1.373158
[Train] epoch:349	batch id:120	 lr:0.001000 loss:1.320767
[Train] epoch:349	batch id:130	 lr:0.001000 loss:1.322616
[Train] epoch:349	batch id:140	 lr:0.001000 loss:1.315780
[Train] epoch:349	batch id:150	 lr:0.001000 loss:1.324402
[Train] epoch:349	batch id:160	 lr:0.001000 loss:1.333995
[Train] epoch:349	batch id:170	 lr:0.001000 loss:1.298668
[Train] epoch:349	batch id:180	 lr:0.001000 loss:1.354198
[Train] epoch:349	batch id:190	 lr:0.001000 loss:1.343235
[Train] epoch:349	batch id:200	 lr:0.001000 loss:1.324700
[Train] epoch:349	batch id:210	 lr:0.001000 loss:1.322314
[Train] epoch:349	batch id:220	 lr:0.001000 loss:1.303128
[Train] epoch:349	batch id:230	 lr:0.001000 loss:1.315700
[Train] epoch:349	batch id:240	 lr:0.001000 loss:1.302838
[Train] epoch:349	batch id:250	 lr:0.001000 loss:1.294473
[Train] epoch:349	batch id:260	 lr:0.001000 loss:1.412366
[Train] epoch:349	batch id:270	 lr:0.001000 loss:1.329545
[Train] epoch:349	batch id:280	 lr:0.001000 loss:1.318991
[Train] epoch:349	batch id:290	 lr:0.001000 loss:1.302593
[Train] epoch:349	batch id:300	 lr:0.001000 loss:1.319194
[Train] 349, loss: 1.327507, train acc: 0.996946, 
[Test] epoch:349	batch id:0	 loss:1.261339
[Test] epoch:349	batch id:10	 loss:1.434673
[Test] epoch:349	batch id:20	 loss:1.473316
[Test] epoch:349	batch id:30	 loss:1.362402
[Test] epoch:349	batch id:40	 loss:1.653550
[Test] epoch:349	batch id:50	 loss:1.555513
[Test] epoch:349	batch id:60	 loss:1.263406
[Test] epoch:349	batch id:70	 loss:1.414384
[Test] epoch:349	batch id:80	 loss:1.609609
[Test] epoch:349	batch id:90	 loss:1.601557
[Test] epoch:349	batch id:100	 loss:2.000034
[Test] epoch:349	batch id:110	 loss:1.358443
[Test] epoch:349	batch id:120	 loss:1.299010
[Test] epoch:349	batch id:130	 loss:1.306087
[Test] epoch:349	batch id:140	 loss:1.354546
[Test] epoch:349	batch id:150	 loss:1.717606
[Test] 349, loss: 1.475146, test acc: 0.916937,
